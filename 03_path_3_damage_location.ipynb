{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nUqg545L69Bp"
      },
      "source": [
        "# Path 3:\n",
        "To classify location of damage - front, rear or behind."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sYF_pJk_6z21"
      },
      "source": [
        "# Importing all the required libraries for this project"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-CmeYk3M7Gum"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import h5py\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from IPython.display import Image, display, clear_output\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_style('whitegrid')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ofp_E_S37Vff"
      },
      "outputs": [],
      "source": [
        "from keras import optimizers\n",
        "from keras.models import Sequential, load_model, Model\n",
        "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, Activation, Dense, Dropout, Flatten\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.callbacks import ModelCheckpoint, History\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.regularizers import l2, l1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dTtuZPZQ7zg6"
      },
      "source": [
        "Using TensorFlow backend."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RFHigEAt7_-Q"
      },
      "source": [
        "This code defines a function called save_bottleneck_features(). This function's purpose is to extract and save features from images using a pre-trained deep learning model called VGG16. These extracted features are often referred to as \"bottleneck features\" and can be used for image classification tasks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lb4nqwW570g1"
      },
      "outputs": [],
      "source": [
        "def save_bottleneck_features():\n",
        "    datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    model = VGG16(include_top=False, weights='imagenet')\n",
        "\n",
        "    generator = datagen.flow_from_directory(train_data_dir, target_size=(img_width,img_height), batch_size=batch_size, class_mode=None, shuffle=False)\n",
        "    bottleneck_features_train = model.predict_generator(generator, nb_train_samples//batch_size)\n",
        "    np.save(location+'/bottleneck_features_train.npy', bottleneck_features_train)\n",
        "\n",
        "    generator = datagen.flow_from_directory(validation_data_dir, target_size=(img_width,img_height), batch_size=batch_size, class_mode=None, shuffle=False)\n",
        "    bottleneck_features_validation = model.predict_generator(generator, nb_validation_samples//batch_size)\n",
        "    np.save(location+'/bottleneck_features_validation.npy', bottleneck_features_validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OTssXceQ8JYO"
      },
      "source": [
        "This Python code defines a function called print_best_model_results. This function's purpose is to identify and display the best performance metrics achieved during the training of a machine learning model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2IuR4mzY8Bur"
      },
      "outputs": [],
      "source": [
        "def print_best_model_results(model_hist):\n",
        "    best_epoch = np.argmax(model_hist['val_acc'])\n",
        "    print('epoch:', best_epoch+1, ', val_acc:', model_hist['val_acc'][best_epoch], ', val_loss:', model_hist['val_loss'][best_epoch])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "irbGbgpE8Udh"
      },
      "source": [
        "This function is designed to visualize the performance of a machine learning model during its training process. It takes two arguments:\n",
        "\n",
        "hist: This is a dictionary-like object (likely a Keras History object) containing the training and validation metrics recorded during the model's training. It's expected to have keys like 'acc', 'val_acc', 'loss', and 'val_loss'.\n",
        "stop: This argument determines the number of epochs to display on the plots. It defaults to 50."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BUjMe3HH8McM"
      },
      "outputs": [],
      "source": [
        "def plot_metrics(hist, stop=50):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    axes[0].plot(range(stop), hist['acc'], label='Training')\n",
        "    axes[0].plot(range(stop), hist['val_acc'], label='Validation')\n",
        "    axes[0].set_title('Accuracy')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].legend(loc='lower right')\n",
        "\n",
        "    axes[1].plot(range(stop), hist['loss'], label='Training')\n",
        "    axes[1].plot(range(stop), hist['val_loss'], label='Validation')\n",
        "    axes[1].set_title('Loss')\n",
        "    axes[1].set_ylabel('Loss')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].legend(loc='upper right')\n",
        "\n",
        "    plt.tight_layout();\n",
        "\n",
        "    print(\"Best Model:\")\n",
        "    print_best_model_results(hist)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8TPkL2b_8ko6"
      },
      "source": [
        "This function is the core of a machine learning process designed to classify images into three categories (likely front, rear, or behind damage based on the initial comments). It uses a technique called transfer learning, leveraging pre-trained models to speed up the process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kobq0_Ox8aa7"
      },
      "outputs": [],
      "source": [
        "def train_categorical_model():\n",
        "    train_data = np.load(location+'/bottleneck_features_train.npy')\n",
        "    train_labels = np.array([0]*(416) + [1]*(288) + [2]*(272))\n",
        "    train_labels = to_categorical(train_labels)\n",
        "\n",
        "    validation_data = np.load(location+'/bottleneck_features_validation.npy')\n",
        "    validation_labels = np.array([0]*(73) + [1]*(53) + [2]*(50))\n",
        "    validation_labels = to_categorical(validation_labels)\n",
        "\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    checkpoint = ModelCheckpoint(top_model_weights_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True,mode='auto')\n",
        "\n",
        "    fit = model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(validation_data,validation_labels), callbacks=[checkpoint])\n",
        "\n",
        "    with open(location+'/top_history.txt', 'w') as f:\n",
        "        json.dump(fit.history, f)\n",
        "\n",
        "    return model,fit.history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO6gjyZb8uqY"
      },
      "source": [
        "The finetune_categorical_model function, This function is designed to fine-tune a pre-trained model (VGG16) for a specific image classification task, likely classifying the location of damage in images (front, rear, or behind)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3iJoQIr8q4t"
      },
      "outputs": [],
      "source": [
        "def finetune_categorical_model():\n",
        "    input_tensor = Input(shape=(256,256,3))\n",
        "    base_model = VGG16(weights='imagenet',include_top= False,input_tensor=input_tensor)\n",
        "    print(\"Model loaded.\")\n",
        "    top_model = Sequential()\n",
        "    top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "    top_model.add(Dense(256, activation='relu'))\n",
        "    top_model.add(Dropout(0.5))\n",
        "    top_model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    top_model.load_weights(top_model_weights_path)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
        "\n",
        "    for layer in model.layers[:25]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.00001, momentum=0.9), metrics=['accuracy'])\n",
        "\n",
        "    train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(train_data_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode='categorical')\n",
        "\n",
        "    validation_generator = test_datagen.flow_from_directory(validation_data_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode='categorical')\n",
        "\n",
        "    checkpoint = ModelCheckpoint(fine_tuned_model_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
        "\n",
        "    fit = model.fit_generator(train_generator, steps_per_epoch=nb_train_samples//batch_size, epochs=epochs, validation_data=validation_generator, validation_steps=nb_validation_samples//batch_size, verbose=1, callbacks=[checkpoint])\n",
        "\n",
        "    with open(location+'/ft_history.txt', 'w') as f:\n",
        "        json.dump(fit.history, f)\n",
        "\n",
        "    return model, fit.history"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0TZ6_3yy8_ky"
      },
      "source": [
        "This function is designed to assess the performance of a trained categorical model, which likely classifies images into categories such as front, rear, or behind damage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JFqHNtto86fK"
      },
      "outputs": [],
      "source": [
        "def evaluate_categorical_model(model, directory, labels):\n",
        "    datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    generator = datagen.flow_from_directory(directory, target_size=(img_height,img_width), batch_size=batch_size, class_mode='categorical', shuffle=False)\n",
        "\n",
        "    predictions = model.predict_generator(generator, len(labels))\n",
        "\n",
        "    pred_labels = [0 if i<0.5 else 1 for i in predictions]\n",
        "\n",
        "    print('')\n",
        "    print(classification_report(validation_labels, pred_labels))\n",
        "    print('')\n",
        "    cm = confusion_matrix(validation_labels, pred_labels)\n",
        "    return cm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P97E7iXd9Enw"
      },
      "source": [
        "# Image dataset details"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdIB0ch2jwfL"
      },
      "source": [
        "This code snippet is primarily focused on setting up important parameters and file paths for the image classification task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F9QPuNlF9GL-"
      },
      "outputs": [],
      "source": [
        "location = '/content/drive/MyDrive/data2'\n",
        "top_model_weights_path = location+'/top_model_weights.h5'\n",
        "fine_tuned_model_path = location+'/ft_model.h5'\n",
        "model1 = location+'/bottleneck_fc_model.h5'\n",
        "train_data_dir = location+'/training'\n",
        "validation_data_dir = location+'/validation'\n",
        "train_samples = [len(os.listdir(train_data_dir+'/'+i)) for i in sorted(os.listdir(train_data_dir))]\n",
        "nb_train_samples = 976\n",
        "validation_samples = [len(os.listdir(validation_data_dir+'/'+i)) for i in sorted(os.listdir(validation_data_dir))]\n",
        "nb_validation_samples = 176\n",
        "\n",
        "img_width, img_height = 256,256\n",
        "epochs = 50\n",
        "batch_size = 16"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PScDYlQ4kImF"
      },
      "source": [
        "This function, save_bottleneck_features, is designed to extract and save important features from your image data. These features are then used for training a more efficient classification model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSFHGbW_jxvm"
      },
      "outputs": [],
      "source": [
        "def save_bottleneck_features():\n",
        "    datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    model = VGG16(include_top=False, weights='imagenet')\n",
        "\n",
        "    generator = datagen.flow_from_directory(train_data_dir, target_size=(img_width,img_height), batch_size=batch_size, class_mode=None, shuffle=False)\n",
        "    # Use predict instead of predict_generator\n",
        "    bottleneck_features_train = model.predict(generator, steps=nb_train_samples//batch_size)\n",
        "    np.save(location+'/bottleneck_features_train.npy', bottleneck_features_train)\n",
        "\n",
        "    generator = datagen.flow_from_directory(validation_data_dir, target_size=(img_width,img_height), batch_size=batch_size, class_mode=None, shuffle=False)\n",
        "    # Use predict instead of predict_generator\n",
        "    bottleneck_features_validation = model.predict(generator, steps=nb_validation_samples//batch_size)\n",
        "    np.save(location+'/bottleneck_features_validation.npy', bottleneck_features_validation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line is defining a variable called top_model_weights_path and assigning it a value"
      ],
      "metadata": {
        "id": "boZWLyV4U1KR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "657_eL8pj-UC"
      },
      "outputs": [],
      "source": [
        "top_model_weights_path = location+'/top_model_weights.weights.h5' # Change the file extension to .weights.h5"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function, train_categorical_model, is designed to train a model to categorize images"
      ],
      "metadata": {
        "id": "L9_K0k6fU7iR"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fsGYGFmdk4uM"
      },
      "outputs": [],
      "source": [
        "def train_categorical_model():\n",
        "    train_data = np.load(location+'/bottleneck_features_train.npy')\n",
        "    train_labels = np.array([0]*(416) + [1]*(288) + [2]*(272))  # Original line\n",
        "    train_labels = to_categorical(train_labels)\n",
        "\n",
        "    validation_data = np.load(location+'/bottleneck_features_validation.npy')\n",
        "    validation_labels = np.array([0]*(73) + [1]*(53) + [2]*(50))\n",
        "    validation_labels = to_categorical(validation_labels)\n",
        "\n",
        "    # Check the shapes of train_data and train_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function, train_categorical_model, is the heart of the machine learning process for classifying images into categories (probably 'front', 'rear', and 'behind' damage)"
      ],
      "metadata": {
        "id": "8qvBkPuMVEVG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mi04_5_klHc6"
      },
      "outputs": [],
      "source": [
        "def train_categorical_model():\n",
        "    train_data = np.load(location+'/bottleneck_features_train.npy')\n",
        "    train_labels = np.array([0]*(416) + [1]*(288) + [2]*(272))  # Original line\n",
        "    train_labels = to_categorical(train_labels)\n",
        "\n",
        "    validation_data = np.load(location+'/bottleneck_features_validation.npy')\n",
        "    validation_labels = np.array([0]*(73) + [1]*(53) + [2]*(50))\n",
        "    validation_labels = to_categorical(validation_labels)\n",
        "\n",
        "    # ... (rest of your model definition and training code) ...\n",
        "\n",
        "    # Add a return statement to return the model and history\n",
        "    return model, fit.history"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function, train_categorical_model, is the core of training a machine learning model to classify images into three categories. These categories are likely \"front,\" \"rear,\" and \"behind\" damage, based on the context of the code."
      ],
      "metadata": {
        "id": "sOl6zwrJVPeh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function, train_categorical_model, is the core of building and training a machine learning model to classify images into three categories, likely \"front,\" \"rear,\" and \"behind\" damage."
      ],
      "metadata": {
        "id": "IAtbbGAkVZos"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D5GwFXiplbI_"
      },
      "outputs": [],
      "source": [
        "def train_categorical_model():\n",
        "    train_data = np.load(location+'/bottleneck_features_train.npy')\n",
        "    train_labels = np.array([0]*(416) + [1]*(288) + [2]*(272))  # Original line\n",
        "    train_labels = to_categorical(train_labels)\n",
        "\n",
        "    validation_data = np.load(location+'/bottleneck_features_validation.npy')\n",
        "    validation_labels = np.array([0]*(73) + [1]*(53) + [2]*(50))\n",
        "    validation_labels = to_categorical(validation_labels)\n",
        "\n",
        "    # Define the model\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    checkpoint = ModelCheckpoint(top_model_weights_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True,mode='auto')\n",
        "\n",
        "    # Train the model\n",
        "    fit = model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(validation_data,validation_labels), callbacks=[checkpoint])\n",
        "\n",
        "    # Return the model and history\n",
        "    return model, fit.history"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function, train_categorical_model, is the core of building and training a machine learning model for image classification, specifically designed to categorize images into three distinct classes (likely \"front,\" \"rear,\" and \"behind\" damage)."
      ],
      "metadata": {
        "id": "If7GgrvUVh0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " This function is the heart of training a machine learning model to classify images into three categories, most likely related to the location of damage in the images (front, rear, or behind)"
      ],
      "metadata": {
        "id": "8f_1pLaKVoHz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ubd_LzwRlqZf"
      },
      "outputs": [],
      "source": [
        "def train_categorical_model():\n",
        "    train_data = np.load(location+'/bottleneck_features_train.npy')\n",
        "    # The number of samples in train_labels should match the number of samples in train_data\n",
        "    # Since train_data has 1824 samples, we adjust train_labels accordingly.\n",
        "    train_labels = np.array([0]*(416*2) + [1]*(288*2) + [2]*(272*2))\n",
        "    train_labels = to_categorical(train_labels)\n",
        "\n",
        "    validation_data = np.load(location+'/bottleneck_features_validation.npy')\n",
        "    validation_labels = np.array([0]*(73) + [1]*(53) + [2]*(50))  # This should be reviewed and fixed\n",
        "    validation_labels = to_categorical(validation_labels)\n",
        "\n",
        "    # Define the model\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    checkpoint = ModelCheckpoint(top_model_weights_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True,mode='auto')\n",
        "\n",
        "    # Train the model\n",
        "    fit = model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(validation_data,validation_labels), callbacks=[checkpoint])\n",
        "\n",
        "    # Return the model and history\n",
        "    return model, fit.history"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function, train_categorical_model, is a crucial part of a machine learning workflow for image classification. It seems to be focused on preparing the training and validation data for a model that will classify images into three categories (likely front, rear, or behind damage, based on surrounding comments)."
      ],
      "metadata": {
        "id": "9oI-eNOqVv9F"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qzjJ_JDNmAdh"
      },
      "outputs": [],
      "source": [
        "def train_categorical_model():\n",
        "    train_data = np.load(location+'/bottleneck_features_train.npy')\n",
        "    # The number of samples in train_labels should match the number of samples in train_data\n",
        "    # Since train_data has shape (number_of_samples, features_dimension_1, features_dimension_2, ...),\n",
        "    # we extract the number of samples using train_data.shape[0]\n",
        "    num_samples = train_data.shape[0]\n",
        "\n",
        "    # Assuming you have 3 classes and want to distribute the labels evenly\n",
        "    # You might need to adjust this based on your actual data distribution\n",
        "    # For example, if you know the exact number of samples per class, you can use that instead\n",
        "    train_labels = np.repeat(np.arange(3), num_samples // 3)\n",
        "    # If num_samples is not divisible by 3, you might need to handle the remaining samples\n",
        "    # One option is to truncate train_labels to match num_samples\n",
        "    train_labels = train_labels[:num_samples]\n",
        "    train_labels = to_categorical(train_labels)\n",
        "\n",
        "    validation_data = np.load(location+'/bottleneck_features_validation.npy')\n",
        "    # Similar adjustments may be needed for validation_"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " This function is designed to prepare the data for training a categorical model, likely for image classification into three categories (e.g., front, rear, or behind damage). It focuses on creating and formatting the labels for the training and validation datasets"
      ],
      "metadata": {
        "id": "ZmXLlJqyV3mE"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4eKmZUfEmXjJ"
      },
      "outputs": [],
      "source": [
        "def train_categorical_model():\n",
        "    train_data = np.load(location+'/bottleneck_features_train.npy')\n",
        "    # The number of samples in train_labels should match the number of samples in train_data\n",
        "    # Since train_data has shape (number_of_samples, features_dimension_1, features_dimension_2, ...),\n",
        "    # we extract the number of samples using train_data.shape[0]\n",
        "    num_samples = train_data.shape[0]\n",
        "\n",
        "    # Assuming you have 3 classes and want to distribute the labels evenly\n",
        "    # You might need to adjust this based on your actual data distribution\n",
        "    # For example, if you know the exact number of samples per class, you can use that instead\n",
        "    train_labels = np.repeat(np.arange(3), num_samples // 3)\n",
        "    # If num_samples is not divisible by 3, you might need to handle the remaining samples\n",
        "    # One option is to truncate train_labels to match num_samples\n",
        "    train_labels = train_labels[:num_samples]\n",
        "    train_labels = to_categorical(train_labels)\n",
        "\n",
        "    validation_data = np.load(location+'/bottleneck_features_validation.npy')\n",
        "    # Similar adjustments may be needed for validation_labels\n",
        "    num_val_samples = validation_data.shape[0]\n",
        "    validation_labels = np.repeat(np.arange(3), num_val_samples // 3)\n",
        "    validation_labels = validation_labels[:num_val_samples]\n",
        "    validation_labels"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function, train_categorical_model(), is the core of the process, responsible for building, training, and returning a machine learning model.\n",
        "\n"
      ],
      "metadata": {
        "id": "SwBGWTcMWBj9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jzkrlfA0mmS1"
      },
      "outputs": [],
      "source": [
        "def train_categorical_model():\n",
        "    train_data = np.load(location+'/bottleneck_features_train.npy')\n",
        "    # The number of samples in train_labels should match the number of samples in train_data\n",
        "    # Since train_data has shape (number_of_samples, features_dimension_1, features_dimension_2, ...),\n",
        "    # we extract the number of samples using train_data.shape[0]\n",
        "    num_samples = train_data.shape[0]\n",
        "\n",
        "    # Assuming you have 3 classes and want to distribute the labels evenly\n",
        "    # You might need to adjust this based on your actual data distribution\n",
        "    # For example, if you know the exact number of samples per class, you can use that instead\n",
        "    train_labels = np.repeat(np.arange(3), num_samples // 3)\n",
        "    # If num_samples is not divisible by 3, you might need to handle the remaining samples\n",
        "    # One option is to truncate train_labels to match num_samples\n",
        "    train_labels = train_labels[:num_samples]\n",
        "    train_labels = to_categorical(train_labels)\n",
        "\n",
        "    validation_data = np.load(location+'/bottleneck_features_validation.npy')\n",
        "    # Similar adjustments may be needed for validation_labels\n",
        "    num_val_samples = validation_data.shape[0]\n",
        "    validation_labels = np.repeat(np.arange(3), num_val_samples // 3)\n",
        "    validation_labels = validation_labels[:num_val_samples]\n",
        "    validation_labels = to_categorical(validation_labels) # Convert validation_labels to categorical\n",
        "\n",
        "    # Define the model\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    checkpoint = ModelCheckpoint(top_model_weights_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True,mode='auto')\n",
        "\n",
        "    # Train the model\n",
        "    fit = model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(validation_data,validation_labels), callbacks=[checkpoint])\n",
        "\n",
        "    # Return the model and history\n",
        "    return model, fit.history"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function is the core of building and training a machine learning model to categorize images, likely into three damage categories (\"front,\" \"rear,\" and \"behind\")."
      ],
      "metadata": {
        "id": "hfqaiJr_WRBc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S3bBzBCgm55Z"
      },
      "outputs": [],
      "source": [
        "def train_categorical_model():\n",
        "    train_data = np.load(location+'/bottleneck_features_train.npy')\n",
        "    # The number of samples in train_labels should match the number of samples in train_data\n",
        "    # Since train_data has shape (number_of_samples, features_dimension_1, features_dimension_2, ...),\n",
        "    # we extract the number of samples using train_data.shape[0]\n",
        "    num_samples = train_data.shape[0]\n",
        "\n",
        "    # Assuming you have 3 classes and want to distribute the labels evenly\n",
        "    # You might need to adjust this based on your actual data distribution\n",
        "    # For example, if you know the exact number of samples per class, you can use that instead\n",
        "    # Instead of np.repeat, let's use np.tile to repeat the sequence [0, 1, 2]\n",
        "    train_labels = np.tile(np.arange(3), num_samples // 3 + 1)[:num_samples]\n",
        "    train_labels = to_categorical(train_labels)\n",
        "\n",
        "    validation_data = np.load(location+'/bottleneck_features_validation.npy')\n",
        "    # Similar adjustments may be needed for validation_labels\n",
        "    num_val_samples = validation_data.shape[0]\n",
        "    validation_labels = np.tile(np.arange(3), num_val_samples // 3 + 1)[:num_val_samples]\n",
        "    validation_labels = to_categorical(validation_labels)  # Convert validation_labels to categorical\n",
        "\n",
        "    # Define the model\n",
        "    model = Sequential()\n",
        "    model.add(Flatten(input_shape=train_data.shape[1:]))\n",
        "    model.add(Dense(256, activation='relu'))\n",
        "    model.add(Dropout(0.5))\n",
        "    model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    checkpoint = ModelCheckpoint(top_model_weights_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True, mode='auto')\n",
        "\n",
        "    # Train the model\n",
        "    fit = model.fit(train_data, train_labels, epochs=epochs, batch_size=batch_size, validation_data=(validation_data, validation_labels), callbacks=[checkpoint])\n",
        "\n",
        "    # Return the model and history\n",
        "    return model, fit.history"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line is where the actual training of the image classification model happens"
      ],
      "metadata": {
        "id": "Sl7jOYNSWX52"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ifn2FrMXnRt1",
        "outputId": "1f640359-4453-45e8-96c7-3f31583a7c63"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 158ms/step - accuracy: 0.3311 - loss: 5.6773 - val_accuracy: 0.3371 - val_loss: 1.0988\n",
            "Epoch 2/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/model_checkpoint.py:206: UserWarning: Can save best model only with val_acc available, skipping.\n",
            "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 156ms/step - accuracy: 0.3296 - loss: 1.2850 - val_accuracy: 0.3348 - val_loss: 1.0991\n",
            "Epoch 3/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 164ms/step - accuracy: 0.3505 - loss: 1.1507 - val_accuracy: 0.3438 - val_loss: 1.1060\n",
            "Epoch 4/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 157ms/step - accuracy: 0.3342 - loss: 1.1380 - val_accuracy: 0.3348 - val_loss: 1.4326\n",
            "Epoch 5/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 157ms/step - accuracy: 0.3437 - loss: 1.1456 - val_accuracy: 0.3661 - val_loss: 1.1055\n",
            "Epoch 6/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 166ms/step - accuracy: 0.3452 - loss: 1.1186 - val_accuracy: 0.3214 - val_loss: 1.1098\n",
            "Epoch 7/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 156ms/step - accuracy: 0.3617 - loss: 1.1263 - val_accuracy: 0.3415 - val_loss: 1.1249\n",
            "Epoch 8/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 159ms/step - accuracy: 0.3742 - loss: 1.1380 - val_accuracy: 0.3460 - val_loss: 1.1522\n",
            "Epoch 9/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 157ms/step - accuracy: 0.3751 - loss: 1.1370 - val_accuracy: 0.3638 - val_loss: 1.1005\n",
            "Epoch 10/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 166ms/step - accuracy: 0.3897 - loss: 1.1126 - val_accuracy: 0.3705 - val_loss: 1.2198\n",
            "Epoch 11/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 157ms/step - accuracy: 0.4074 - loss: 1.1501 - val_accuracy: 0.3371 - val_loss: 1.1335\n",
            "Epoch 12/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 155ms/step - accuracy: 0.4086 - loss: 1.0923 - val_accuracy: 0.3728 - val_loss: 1.1478\n",
            "Epoch 13/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 169ms/step - accuracy: 0.4165 - loss: 1.0284 - val_accuracy: 0.3460 - val_loss: 1.1595\n",
            "Epoch 14/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 156ms/step - accuracy: 0.4354 - loss: 1.0711 - val_accuracy: 0.3750 - val_loss: 1.1403\n",
            "Epoch 15/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 157ms/step - accuracy: 0.4756 - loss: 1.0096 - val_accuracy: 0.3170 - val_loss: 1.3385\n",
            "Epoch 16/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 166ms/step - accuracy: 0.5070 - loss: 0.9883 - val_accuracy: 0.3594 - val_loss: 1.1532\n",
            "Epoch 17/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 154ms/step - accuracy: 0.5123 - loss: 0.9592 - val_accuracy: 0.3683 - val_loss: 1.1740\n",
            "Epoch 18/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 155ms/step - accuracy: 0.5481 - loss: 0.9159 - val_accuracy: 0.3616 - val_loss: 1.2122\n",
            "Epoch 19/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 163ms/step - accuracy: 0.5643 - loss: 0.9320 - val_accuracy: 0.3080 - val_loss: 1.2039\n",
            "Epoch 20/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 156ms/step - accuracy: 0.5418 - loss: 0.8692 - val_accuracy: 0.4040 - val_loss: 1.2300\n",
            "Epoch 21/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 158ms/step - accuracy: 0.5848 - loss: 0.8460 - val_accuracy: 0.3594 - val_loss: 1.3016\n",
            "Epoch 22/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 156ms/step - accuracy: 0.5898 - loss: 0.8458 - val_accuracy: 0.3705 - val_loss: 1.3424\n",
            "Epoch 23/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 157ms/step - accuracy: 0.5956 - loss: 0.8101 - val_accuracy: 0.3594 - val_loss: 1.7886\n",
            "Epoch 24/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 165ms/step - accuracy: 0.6040 - loss: 0.7947 - val_accuracy: 0.3393 - val_loss: 1.2117\n",
            "Epoch 25/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 159ms/step - accuracy: 0.6258 - loss: 0.7853 - val_accuracy: 0.3705 - val_loss: 1.4942\n",
            "Epoch 26/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 158ms/step - accuracy: 0.6296 - loss: 0.8009 - val_accuracy: 0.3661 - val_loss: 1.3581\n",
            "Epoch 27/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 158ms/step - accuracy: 0.6218 - loss: 0.7280 - val_accuracy: 0.3705 - val_loss: 1.4449\n",
            "Epoch 28/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 161ms/step - accuracy: 0.6930 - loss: 0.6694 - val_accuracy: 0.3616 - val_loss: 1.4763\n",
            "Epoch 29/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 277ms/step - accuracy: 0.6922 - loss: 0.6291 - val_accuracy: 0.3460 - val_loss: 1.3903\n",
            "Epoch 30/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 165ms/step - accuracy: 0.6727 - loss: 0.6936 - val_accuracy: 0.3571 - val_loss: 1.4426\n",
            "Epoch 31/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 178ms/step - accuracy: 0.7136 - loss: 0.6056 - val_accuracy: 0.3549 - val_loss: 1.5591\n",
            "Epoch 32/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 170ms/step - accuracy: 0.7222 - loss: 0.5917 - val_accuracy: 0.3304 - val_loss: 2.1577\n",
            "Epoch 33/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 162ms/step - accuracy: 0.7004 - loss: 0.6704 - val_accuracy: 0.3862 - val_loss: 1.8207\n",
            "Epoch 34/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 165ms/step - accuracy: 0.7207 - loss: 0.5940 - val_accuracy: 0.3438 - val_loss: 1.5295\n",
            "Epoch 35/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 167ms/step - accuracy: 0.7364 - loss: 0.5619 - val_accuracy: 0.3482 - val_loss: 1.6998\n",
            "Epoch 36/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 169ms/step - accuracy: 0.7449 - loss: 0.5354 - val_accuracy: 0.3438 - val_loss: 1.9519\n",
            "Epoch 37/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 164ms/step - accuracy: 0.7521 - loss: 0.5651 - val_accuracy: 0.3482 - val_loss: 1.8311\n",
            "Epoch 38/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 163ms/step - accuracy: 0.7498 - loss: 0.5341 - val_accuracy: 0.3326 - val_loss: 1.9818\n",
            "Epoch 39/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 177ms/step - accuracy: 0.7704 - loss: 0.5597 - val_accuracy: 0.3616 - val_loss: 1.8844\n",
            "Epoch 40/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 168ms/step - accuracy: 0.7746 - loss: 0.5048 - val_accuracy: 0.3482 - val_loss: 2.1002\n",
            "Epoch 41/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 163ms/step - accuracy: 0.7660 - loss: 0.4891 - val_accuracy: 0.3348 - val_loss: 1.6762\n",
            "Epoch 42/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 173ms/step - accuracy: 0.7849 - loss: 0.4770 - val_accuracy: 0.3326 - val_loss: 2.0224\n",
            "Epoch 43/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 173ms/step - accuracy: 0.8097 - loss: 0.4626 - val_accuracy: 0.3795 - val_loss: 1.8347\n",
            "Epoch 44/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 163ms/step - accuracy: 0.7970 - loss: 0.4318 - val_accuracy: 0.3683 - val_loss: 2.0732\n",
            "Epoch 45/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 162ms/step - accuracy: 0.7686 - loss: 0.5262 - val_accuracy: 0.3527 - val_loss: 2.4615\n",
            "Epoch 46/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 171ms/step - accuracy: 0.8097 - loss: 0.4489 - val_accuracy: 0.3616 - val_loss: 2.1763\n",
            "Epoch 47/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 164ms/step - accuracy: 0.7861 - loss: 0.4900 - val_accuracy: 0.3348 - val_loss: 2.4997\n",
            "Epoch 48/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 165ms/step - accuracy: 0.7973 - loss: 0.4667 - val_accuracy: 0.3393 - val_loss: 2.2395\n",
            "Epoch 49/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 174ms/step - accuracy: 0.8070 - loss: 0.4415 - val_accuracy: 0.3460 - val_loss: 2.3711\n",
            "Epoch 50/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 164ms/step - accuracy: 0.8109 - loss: 0.4589 - val_accuracy: 0.3326 - val_loss: 2.2406\n"
          ]
        }
      ],
      "source": [
        "d3_model, d3_history = train_categorical_model()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function, plot_metrics, is designed to visualize the performance of a machine learning model during its training process. It creates two plots side-by-side: one for accuracy and one for loss. These plots show how the model's performance changes over time (epochs) for both the training data and the validation data."
      ],
      "metadata": {
        "id": "0ZDjpJUkWhb4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_metrics(hist, stop=50):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    # Use 'accuracy' instead of 'acc'\n",
        "    axes[0].plot(range(stop), hist['accuracy'], label='Training')\n",
        "    # Use 'val_accuracy' instead of 'val_acc'\n",
        "    axes[0].plot(range(stop), hist['val_accuracy'], label='Validation')\n",
        "    axes[0].set_title('Accuracy')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].legend(loc='lower right')\n",
        "\n",
        "    axes[1].plot(range(stop), hist['loss'], label='Training')\n",
        "    axes[1].plot(range(stop), hist['val_loss'], label='Validation')\n",
        "    axes[1].set_title('Loss')\n",
        "    axes[1].set_ylabel('Loss')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].legend(loc='upper right')\n",
        "\n",
        "    plt.tight_layout();\n",
        "\n",
        "    print(\"Best Model:\")\n",
        "    # Update print_best_model_results as well if it uses 'acc' or 'val_acc'\n",
        "    print_best_model_results(hist)"
      ],
      "metadata": {
        "id": "WjxzT11r0lU7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code defines two functions, print_best_model_results and plot_metrics, which work together to display the performance of a machine learning model during training."
      ],
      "metadata": {
        "id": "a8_LJ7ltWp0c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_best_model_results(model_hist):\n",
        "    # Replace 'val_acc' with 'val_accuracy'\n",
        "    best_epoch = np.argmax(model_hist['val_accuracy'])\n",
        "    print('epoch:', best_epoch+1, ', val_acc:', model_hist['val_accuracy'][best_epoch], ', val_loss:', model_hist['val_loss'][best_epoch])\n",
        "\n",
        "def plot_metrics(hist, stop=50):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(10,4))\n",
        "    axes = axes.flatten()\n",
        "\n",
        "    axes[0].plot(range(stop), hist['accuracy'], label='Training')\n",
        "    # Replace 'val_acc' with 'val_accuracy'\n",
        "    axes[0].plot(range(stop), hist['val_accuracy'], label='Validation')\n",
        "    axes[0].set_title('Accuracy')\n",
        "    axes[0].set_ylabel('Accuracy')\n",
        "    axes[0].set_xlabel('Epoch')\n",
        "    axes[0].legend(loc='lower right')\n",
        "\n",
        "    axes[1].plot(range(stop), hist['loss'], label='Training')\n",
        "    axes[1].plot(range(stop), hist['val_loss'], label='Validation')\n",
        "    axes[1].set_title('Loss')\n",
        "    axes[1].set_ylabel('Loss')\n",
        "    axes[1].set_xlabel('Epoch')\n",
        "    axes[1].legend(loc='upper right')\n",
        "\n",
        "    plt.tight_layout();\n",
        "\n",
        "    print(\"Best Model:\")\n",
        "    print_best_model_results(hist)"
      ],
      "metadata": {
        "id": "l5ctThou02yy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line is responsible for visualizing the performance of your trained machine learning model."
      ],
      "metadata": {
        "id": "CwA0cpHSWwDw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "plot_metrics(d3_history)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "id": "tf0gb-o31MTr",
        "outputId": "ad526810-7da6-41df-de70-130c9cf7beec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Model:\n",
            "epoch: 20 , val_acc: 0.4040178656578064 , val_loss: 1.2300159931182861\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 2 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA90AAAGGCAYAAABmGOKbAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAADZx0lEQVR4nOzdd3iTVfvA8W+SJt10Q4FSNmW1UKYgQxEVAQUZKoIT98D54njVn7hwT/QVEXEAIjKUJQoiyN6j7D1bShfdbebvj6dJWzqTpk3a3p/r6pWnT55xciht7pz73EdlsVgsCCGEEEIIIYQQwunUrm6AEEIIIYQQQghRV0nQLYQQQgghhBBCVBMJuoUQQgghhBBCiGoiQbcQQgghhBBCCFFNJOgWQgghhBBCCCGqiQTdQgghhBBCCCFENZGgWwghhBBCCCGEqCYSdAshhBBCCCGEENVEgm4hhBBCCCGEEKKaSNAthBBCCCGEEEJUEwm6hajl5syZQ1RUFGPHjnV1U4QQQghRjRYtWkRUVBRxcXGubooQwg4SdAtRyy1dupSmTZuyb98+zpw54+rmCCGEEEIIIYqQoFuIWuzcuXPs3r2bl156ieDgYJYuXerqJpUqJyfH1U0QQgghhBDCJSToFqIWW7p0KQEBAQwcOJAbb7yx1KA7IyODd955h0GDBtG5c2cGDBjA5MmTSU1NtR2Tn5/PF198wY033kh0dDT9+vXjiSee4OzZswBs3bqVqKgotm7dWuza58+fJyoqikWLFtn2vfjii8TGxnL27FkefPBBYmNjef755wHYsWMHkyZN4pprrqFz584MHDiQd955h7y8vBLtPnHiBE899RRXXXUVMTEx3HjjjXzyyScAbNmyhaioKFatWlVqn0RFRbF7924HelQIIYSo3Q4ePMgDDzxAt27diI2N5Z577mHPnj3FjjEYDEybNo0bbriB6Ohoevfuzbhx49i4caPtmKSkJF566SUGDBhA586d6devH48++ijnz5+v4VckRO3n4eoGCCEct3TpUq6//np0Oh3Dhw/n559/Zt++fcTExACQnZ3N+PHjOXHiBKNHj6Zjx46kpaWxZs0aEhMTCQ4OxmQy8fDDD7N582aGDRvG3XffTXZ2Nhs3buTo0aNERkba3S6j0cjEiRPp3r07L7zwAl5eXgCsXLmSvLw8xo0bR2BgIPv27WP27NlcvHiRzz//3Hb+4cOHGT9+PB4eHtx+++00bdqUs2fPsmbNGp555hl69+5N48aNba//yj6JjIwkNja2Cj0rhBBC1D7Hjh1j/Pjx+Pr68sADD+Dh4cEvv/zCXXfdxezZs+nSpQsA06ZNY/r06YwdO5aYmBiysrLYv38/Bw4c4OqrrwbgySef5Pjx40yYMIGmTZuSmprKxo0bSUhIICIiwpUvU4haR4JuIWqp/fv3c/LkSV599VUAunfvTnh4OEuXLrUF3TNnzuTo0aNMmzatWHD62GOPYbFYAPjtt9/YvHkzL730Evfee6/tmIceesh2jL30ej1DhgzhueeeK7b/+eeftwXgALfffjvNmzfn448/Jj4+niZNmgDw1ltvYbFYWLx4sW2f9XwAlUrFLbfcwqxZs8jMzMTf3x/A9obgkUcecajdQgghRG326aefYjAY+Pnnn2nWrBkAI0eOZMiQIXzwwQfMnj0bgLVr1zJw4EDefPPNUq+TkZHB7t27mTx5MhMnTrTtf/jhh6v/RQhRB0l6uRC11NKlSwkNDaV3796AEogOHTqUFStWYDKZAPjrr79o3759idFg6/HWY4KCgpgwYUKZxzhi3LhxJfYVDbhzcnJITU0lNjYWi8XCwYMHASVw3r59O6NHjy4WcF/ZnhEjRqDX61m5cqVt34oVKzAajdxyyy0Ot1sIIYSojUwmExs3bmTw4MG2gBugYcOGDB8+nJ07d5KVlQVAgwYNOHbsGKdPny71Wl5eXmi1WrZt20Z6enpNNF+IOk2CbiFqIZPJxPLly+nduzfnz5/nzJkznDlzhpiYGJKTk9m8eTMAZ8+epW3btuVe6+zZs7Rs2RIPD+clvnh4eBAeHl5if3x8PC+++CK9evUiNjaWPn362IJ96xuBc+fOAdCuXbty79G6dWuio6OLzWNfunQpXbt2pXnz5s56KUIIIUStkJqaSm5uLi1btizxXOvWrTGbzSQkJAAwadIkMjMzufHGG7n55pt57733OHz4sO14nU7H888/z7///svVV1/N+PHjmTFjBklJSTX2eoSoSyS9XIhaaMuWLSQlJbF8+XKWL19e4vmlS5fSr18/p92vrBFvs9lc6n6dTodaXfwzPZPJxH333Ud6ejoPPPAArVq1wsfHh8TERF588cUyr1WekSNH8vbbb3Px4kX0ej179uzhtddes/s6QgghRH3Ss2dPVq1axd9//83GjRtZsGABP/zwA1OmTGHs2LEA3HvvvQwaNIjVq1ezYcMGPvvsM7755ht++OEHOnbs6OJXIETtIkG3ELXQ0qVLCQkJKTXAXLVqFatWrWLKlClERkZy7Nixcq8VGRnJ3r17MRgMaLXaUo9p0KABAJmZmcX2X7hwodJtPnr0KKdPn+a9995j5MiRtv1FK6UCtpS4o0ePVnjNoUOH8u6777Js2TLy8vLQarXcdNNNlW6TEEIIUVcEBwfj7e3NqVOnSjx38uRJ1Go1jRs3tu0LDAxk9OjRjB49muzsbCZMmMAXX3xhC7pBeY9w//33c//993P69GlGjhzJd999x4cfflgjr0mIukLSy4WoZfLy8vjrr7+45pprGDJkSImv8ePHk52dzZo1a7jhhhs4fPhwqUtrWYuk3XDDDaSlpTFnzpwyj2natCkajYbt27cXe/7nn3+udLutI99Fi7NZLBZ+/PHHYscFBwfTs2dPFi5cSHx8fKntKXps//79WbJkiW10Pzg4uNJtEkIIIeoKjUbD1Vdfzd9//11sWa/k5GSWLVtG9+7d8fPzAyAtLa3Yub6+vkRGRqLX6wHIzc0lPz+/2DGRkZH4+vrajhFCVJ6MdAtRy6xZs4bs7GwGDRpU6vNdu3YlODiYJUuW8NFHH/Hnn3/y1FNPMXr0aDp16kR6ejpr1qxhypQptG/fnpEjR/Lbb78xdepU9u3bR/fu3cnNzWXz5s2MGzeOwYMH4+/vz5AhQ5g9ezYqlYpmzZqxdu1aUlJSKt3uVq1aERkZyXvvvUdiYiJ+fn78+eefZGRklDj2lVdeYdy4cdx6663cfvvtREREcOHCBdauXcvvv/9e7NiRI0cyadIkAJ566ik7elIIIYSonRYuXMj69etL7H/yySfZtGkTd955J3feeScajYZffvkFvV7Pf/7zH9txw4YNo1evXnTq1InAwEDi4uL4888/bXVWTp8+zb333suQIUNo06YNGo2G1atXk5yczLBhw2rsdQpRV0jQLUQts2TJEjw9PW3raF5JrVZzzTXXsHTpUvR6PXPmzOGLL75g1apVLF68mJCQEPr06UOjRo0A5ZPxGTNm8L///Y9ly5bx119/ERgYSLdu3YiKirJd95VXXsFoNDJv3jx0Oh1Dhgxh8uTJDB8+vFLt1mq1fP3117z11ltMnz4dT09Prr/+esaPH8+IESOKHdu+fXvmz5/PZ599xs8//0x+fj5NmjQpNXX82muvJSAgALPZzHXXXVfZbhRCCCFqrbIyzUaNGsWcOXP46KOPmD59OhaLhZiYGD744APbGt0Ad911F2vWrGHjxo3o9XqaNGnC008/bVseLDw8nGHDhrF582aWLFmCRqOhVatWfPrpp9x444018hqFqEtUFkcX4hVCCDdgNBrp378/1157Le+8846rmyOEEEIIIUQxMqdbCFGrrV69mtTU1GLF2YQQQgghhHAXkl4uhKiV9u7dy5EjR/jqq6/o2LEjvXr1cnWThBBCCCGEKEGCbiFErfTzzz+zZMkS2rdvz7vvvuvq5gghhBBCCFEqmdMthBBCCCGEEEJUE5nTLYQQQgghhBBCVBMJuoUQQgghhBBCiGpS7+Z0m81mjEYjarUalUrl6uYIIYQQJVgsFsxmMx4eHqjV9ffzcfmbLYQQwp1V9u91vQu6jUYjcXFxrm6GEEIIUaHo6Gh0Op2rm+Ey8jdbCCFEbVDR3+t6F3RbP4GIjo5Go9FU+Xomk4m4uDinXa8+kb5znPSd46TvHCd95zh7+856fH0e5Qbn/s2Wn1/HSd85TvrOcdJ3jpO+qxp7+q+yf6/rXdBtTU/TaDRO/SF09vXqE+k7x0nfOU76znHSd46zt+/qe0p1dfzNlp9fx0nfOU76znHSd46Tvqsae/qvor/X9fsjdCGEEEIIIYQQohpJ0C2EEEIIIYQQQlQTCbqFEEIIIYQQQohqUu/mdAshhBBCCCFEdTGZTBgMBpfeHyAvL0/mdDugaP95eXk5pQ8l6BZCCCGEEEKIKrJYLFy8eJHLly+7vB0eHh6cOXOm3hfkdMSV/RcYGEh4eHiV+lKCbiGEEEIIIYSoImvA3bBhQ3x8fFwW8FosFnJzc/H29pag2wHW/vPy8iI3N5dLly4B0LhxY4evKUG3EEIIIYQQQlSByWSyBdwhISEubYvFYsFsNuPl5SVBtwOs/eft7Y2Pjw8Aly5domHDhg6nmkshNSGEEEIIIYSoAuscbmuQJuoO679pVebpS9AthBBCCCGEEE4gI8t1jzP+TSXoFkIIIYQQQgghqokE3UIIIUQ5zGYL7608zJj/bWLd0SRXN0c46H/rTvD2+jQMJrOrmyKEEHXeoEGD+P777yt9/NatW4mKiiIjI6P6GuVCUkhNCCGEKIPRZGbygn0s2n0BgHu+28ZNncN5dXhHmgR6u7h1wh7ztp/nfFo+hxIyiW0e7OrmCCGEW4iKiir3+SeeeIInn3zS7usuWLAAb+/K/52MjY1lw4YN+Pv7232v2kCCbiGEEKIU+UYTk37ezZ8HEtGoVQzpHM7K/Rf5Y/9F1h1NYtJ1bbn/6pboPCRprDbwKvh3ytYbXdwSIYRwHxs2bLBtr1ixgs8//5yVK1fa9hUtDGexWDCZTHh4VBxCBgfb9+GmTqcjLCzMrnNqE3mnIIQQQlwhR2/kgR928OeBRHQear6e0J0v7+zG0if60aN5EDl6E+/+cZihn69n84kUVzdXVIKPTnmTmKs3ubglQgjhPsLCwmxf/v7+qFQq2/cnT56kW7durFu3jlGjRhEdHc3OnTs5e/Ysjz76KH379iU2NpbRo0ezadOmYte9Mr08KiqKX3/9lccff5wuXbpwww038Pfff9uevzK9fNGiRfTo0YP169dz0003ERsby8SJE21rZgMYjUbeeustevToQe/evfnggw944YUXeOyxx6q30xwgQbcQQghRRHqugbtnbmP9sWR8dBpm3duT6zs2AqBjkwbMf7gPH4yJIcRXx/FLWYybsYWn5u3mUkaei1suyuOtU9ZWzZagWwhRgywWCzl6Y41+WSwWp76Gjz76iOeee44VK1YQFRVFTk4OAwcO5Pvvv2fx4sX079+fRx55hPj4+HKvM23aNG666SaWLFnCgAEDeP7557l8+XKZx+fl5fHdd9/x/vvvM3v2bBISEnjvvfdsz8+YMYOlS5cydepU5s6dS1ZWFqtXr3bWy3Yql6eXz5kzh5kzZ5KUlET79u159dVXiYmJKfP477//np9//pmEhASCgoK48cYbee655/D09KzBVgshhKiLUrLyufu7bRyIz6CBlwez7utF9+ZBxY5Rq1WM7dGMGzqG88Ffh5mz9Sy/74ln04kU1j5/Db6eLv/TKkrhWxB0y0i3EKKmWCwWxny9mZ1n0mr0vj2aB/H9XWXHU/aaNGkSV199te37wMBA2rdvb/v+6aefZvXq1axZs4YJEyaUeZ1bb72V4cOHA/Dss8/y008/sW/fPgYMGFDq8QaDgSlTphAZGQnA+PHj+eqrr2zPz549m4ceeojrr78egNdee41///3X8RdajVz6zmDFihVMnTqVKVOm0KVLF3744QcmTpzIypUrCQkJKXH80qVL+eijj3jnnXeIjY3l9OnTvPjii6hUKl566SUXvAIhhBB1xcX0PCbM3MrxS1mE+Or4cWIvOjUJKPP4AB8tb42M5rYezXhr2SEy8414aGR9Vnfl42kd6ZY53UKImlMX/ipER0cX+z47O5tp06axdu1akpKSMJlM5OXlVTjSXbRom4+PD35+fqSmppZ5vLe3ty3gBmjYsCEpKcqUrszMTJKTk4sN1mo0Gjp16oTZ7H6rVLg06J41axa33XYbo0ePBmDKlCmsXbuWhQsX8tBDD5U4fvfu3XTr1o2bb74ZgIiICIYPH87evXtrtN1CCCHcy6nkbL759yS3dGlCn9YlP7StyOnkbO76bivnUnNpHODF7Ad60zrMr1LnxkQEMv+RPnbfU9QsH63ylicnX0a6hRA1Q6VS8esjfcg11OzvHS8PNbm5uU673pVVyN977z02bdrECy+8QGRkJF5eXkyaNAmDwVDudbRabbHvVSpVuQHylQXbVCqV01Pna4rLgm69Xs+BAwd4+OGHbfvUajV9+/Zl9+7dpZ4TGxvLkiVL2LdvHzExMZw7d45169YxYsQIu+9vMjnnh996HWddrz6RvnOc9J3jpO8c5659ZzZbeHrebvaeT+fnbWe5rUcELw2JooG3tsJz841mvl1/iq/WnSDPYCYy2IfZ9/ekaZC3U1+nvX3nbn1cF1hHumv6za8Qon5TqVS2Qo41pboD0927d3Prrbfa0rqzs7O5cOFCtd7zSv7+/oSGhhIXF0fPnj0B5W/nwYMHi6W+uwuXBd1paWmYTKYSaeQhISGcPHmy1HNuvvlm0tLSuPPOO7FYLBiNRu644w4eeeQRu+8fFxfnULtr6nr1ifSd46TvHCd95zh367v1Z3PZez4dDzUYzTB/x3lW7Y/nwW4N6N3Uq8zz9ibm8+2uDOKzlCCsU5iWZ3r7knTmCElnqqet7tZ39Ym3VtLLhRDCGZo3b86qVasYNGgQKpWKTz/91CUp3RMmTGD69OlERkbSqlUrZs+eTXp6OiqV+yX116pqL1u3bmX69On83//9HzExMZw9e5a3336bL7/8kscff9yua0VHR6PRaKrcJpPJRFxcnNOuV59I3zlO+s5x0neOq+6+y9Eb8dZq7PpjmWcw8cRf6wGYNKgtvVoG8dLi/ZxKzuH9TZe5sVMjXh/egYYNCoPvhPQ83llxmBX7lcI2oX46Xr6pPbd0aVxtf6jt7Tvr8cJ5fD2lkJoQQjjDiy++yMsvv8wdd9xBUFAQDz74INnZ2TXejgcffJDk5GReeOEFNBoNt912G/369XPL93cuC7qDgoLQaDS2yfBWKSkphIaGlnrOZ599xi233MLYsWMBbCXrX3vtNR599FHU6sqvgKbRaJz6D+Ls69Un0neOk75znPSd45zZd5cy8lgRl8DSfQnsPJPG6G4RfDg2ptLB7/f/niIhPY8mAV48NLA1XloNfzw1gGlrjvP1uhP8eSCRTSdS+O/QDozqFsGsjaf47O9j5OhNqFVwd58WPHtDOxp4VZyK7gy19edu+vTp/PXXX5w8eRIvLy9iY2N5/vnnadWqVZnnLFq0qESRU51O57IPE6zpnbJkmBBClG7UqFGMGjXK9n3v3r05cuRIieMiIiL48ccfi+0bP358se/XrFlT7PvSrrNjx44y73VlWwAGDx5c7BgPDw9effVVXn31VQDMZjM33XQTN910U5mv0VVcFnTrdDo6derE5s2bGTx4MKB01ObNm8ssNZ+Xl1cisLa+eamtk+qFEKK+Sc3W88f+BJbujWfrqVSK/vpeuOs8XSMDueuq5hVe51JmHl/9cxyAyUPa41WQPuyl1fD8jVEMjW7Mi4v2se98Oi8uiuPtFYfIzFNSi7tFBvLmyM7lVicXhbZt28b48eOJjo7GZDLx8ccfM3HiRJYvX46Pj0+Z5/n5+bFy5Urb965M+fMp+PmQQmpCCFE3XLhwgY0bN9KzZ0/0ej1z5szhwoULtqLb7sSl6eX33XcfL7zwAp07dyYmJoYffviB3Nxc26cakydPplGjRjz33HMAXHvttcyaNYuOHTva0ss/++wzrr322lo5ciCEEPVFeo6BPw9eZOleZT1rk7kw0o6NDGR4TBMycg189vcx3lx6kK4RgURHlB8Qf7LqKNl6E10iArilS5MSz3ds0oBFj/bl+02n+fCvI2TmGQn21fHiTe0Z0y0Ctdr95ny5q5kzZxb7/t1336VPnz4cOHDAVsCmNCqVirCwsOpuXqUUFlKTOd1CCFEXqNVqFi1axHvvvYfFYqFdu3bMmjWL1q1bu7ppJbg06B46dCipqal8/vnnJCUl0aFDB7799ltbenlCQkKxke1HH33UNlk/MTGR4OBgrr32Wp555hlXvQQhhBBlyMwzsPpQIsv2JvDvsSQMpsJAu3PTBtwc04RhMY2JCFJGSi0WCwcTMlh1MJHH5+5i2aR+ZaZ9H0rI4Jft5wB4ZXjHMgNoD42aB/q34sZO4fx7LIlh0Y0J9NE5+ZXWP5mZmQAEBJT/wUhOTg7XXnstZrOZjh078uyzz9K2bduaaGIJ1pHubBnpFkKIOqFx48bMmzfP1c2oFJcXUpswYUKZ6eQ//fRTse89PDx44okneOKJJ2qiaUIIIeyUozfy96FLLNsXzz9HktAbC6uZtg/3Z3hMY4bHNKFFqG+Jc1UqFR+O6cKwL9ZzNjWHyb/u438TupVISbZYLLy9/BBmCwyNDqdni+AK29Us2IfxvStOWRcVM5vNvPPOO3Tr1o127dqVeVzLli155513iIqKIjMzk++++4477riD5cuXEx4ebtc9nbGEmpeH8nOUozfJkmx2ctflAmsD6TvH1ba+M5lMWCwW25crWe/v6nY4xGKCpKPgoYNg14xYX9l/1n9Tk6nk34/K/ny6POgWQghR+6Vl63l7xSGW70sotg5yqzBfbo5pws1dGtOmoX+F1wnw0fLlnd0Y8/UmVh64yKyNp7m/X8tix6w9ksSG48noNGpeGOJ+a3HWdVOmTOHYsWPMnTu33ONiY2OJjY0t9v3QoUOZN28eTz/9tF33dEbxtfg0AwDp2bns2bOnyterj6SivuOk7xxXm/rOw8OD3NxclyyfVZrc3FxXN8FuGmMOXqZ8MOWTk5WBRe26cNXaf/n5+RgMBg4fPuzwtSToFkIIUSW7zqbxxJxdxKfnARAZ7GMb0e7Q2N/u4lldmgXy36EdeH3pQab+cYjYyEBiI4MAMJjMvLX8IAD3Xt2C5iElR8xF9XnjjTdYu3Yts2fPtnu0WqvV0qFDB86ePWv3fZ2xTJ3PxQxYvQmDRU3Xrl2rdK36RpZadJz0neNqW9/l5eVx5swZvL298fLyqviEamSxWMjNzcXb29st16wuV1aWbdNbYwLvBjXehCv7T61Wo9VqadOmTYl/28ou8SlBtxBCCIdYLBZmbjjFu38cxmi20DLUl/fHxNCjeVCV/8jf07cF206nsiLuIk/M3c3ySf0I9NExb9tZTiRlE+Sj5fFr2zjplYiKWCwW3nzzTVatWsVPP/1Es2bN7L6GyWTi6NGjDBw40O5znbHUmr+3Mpc/12CqFW/g3VFtXfLOHUjfOa629J1Go0GlUtm+3IE7taXSDIWj8yp9NvhUPIWsuhT991SpVFX6WZSgWwghhN3Scw1MXrCXPw8kAjA8pjFTR0Xj76T1rlUqFe+OjuFAfAZnUnJ4/te9fHRbVz5ZfQyAZ65vR4B3zaytLZSU8mXLlvHVV1/h6+tLUlISAP7+/rZP/a9ccWTatGl07dqV5s2bk5GRwcyZM4mPj2fs2LEueQ0+OuWNksFkQW80o/NQV3CGEEIIh5hNYLGAxoFQ05BTuK3PKvu4WkaCbiGEEHaJu5DOk/P2cC41F51GzavDOzDhquZO/zS9gZcyv3vU/zax+tAlxvxvE6nZelqH+TKuV6RT7yXK9/PPPwNw1113Fds/depU2zKfV644kpGRwauvvkpSUhIBAQF06tSJefPm0aaNazIUvLWFoxO5epME3UIIUV1SjoHJAA07gD1zss0mMOUXfm/MA5PRseDdzdT+VyCEEKJGWCwW/jiezY+LtqA3WWgW7M1Xd3avcD3tqujcNIDXhnfkld/2c+yS8on3f4d1QKuRgKkmHTlypMJjrlxx5OWXX+bll1+uribZTeehxkMNRjNk640E+EimhBBCOMNdd91F+/bt+e9//wsmI4NGT+TuMUO596FI8Cr9PUJUVBRffvklgwcPLtxpTS1Xa0GlVgJwQzZoyn6fUep13JAE3UIIIcg3mpi25jg7z6SVeUxGroH98cr6zDd0bMQHY7vUSIr3+N6RbDuVypK98fRrE8q1UQ2r/Z6ibvLSqMgyW8jRG13dFCGEcAuPPPIIBoOBmTNnlnhux44djB8/nt9//5327Su5WogxjwVfv4O3lyfos8sMuktlDbq13qDRQk4+5GeBVwBffPEFq1ev5vfffy92yoYNGwgIqL4P/51Fgm4hhKjnEjPyeGT2TnafvVzhsRoVvHRTeyb2b1VjxVlUKhXvj4nhmqgwro1qWPuKwgi34eWhIstgIUdfO9b9FUKI6jZmzBiefPJJLl68WGJVioULF9K5c+fKB9wAxjyCAwsqjuuz7WuMsWA+t9YbPDwhJ6XCed1hYWH23cNFJOgWQoh6bPvpVB6dvYvkrHwaeHnwnxujCPDRlXqs2WxGk36BoVe3qPHA10urYVS3iBq9p6h7PD2Un9vsfAm6hRAC4JprriE4OJhFixbx2GOP2fZnZ2ezcuVKHnroIZ599lm2b99ORkYGkZGRPPzwwwwfPrz0CxrzGHTHE0p6+djhYLFw+swZ/vvf/7Jv3z6aNWumpKFf4YMPPmD1yhVcTEomNDSUm4cP5/ExA9CSy6KFC5g2bRqgpJNDYU2RK9PLjxw5wttvv82ePXvw9vbmhhtu4MUXX8TXV1li9MUXXyQjI4Pu3bsza9YsDAYDQ4cO5eWXX0arrb7sPQm6hRCiHrJYLMzecoYpSw9iNFtoH+7P9Lu6l7vutclkYs+exBpspRDO5VUQdOcaJL1cCFFDLJbiFblrgod35Q/18GDEiBEsXryYRx991Pah+sqVKzGbzdxyyy2sXLmSBx98ED8/P9auXcvkyZOJjIwkJiam5AWNeYXbFjNmfQ5PPvkkISEh/Prrr2RmZvLOO++UOM3X14epLzxCw9BAjqbAq6+/gS+5PHjHMIYOHsix4/ezfv16Zs2aBSirZ1wpJyeHiRMnEhsby4IFC0hJSeGVV17hzTff5N1337Udt3XrVsLCwvjhhx84e/YszzzzDB06dOC2226rdL/ZS4JuIYSoZ/IMJl75bT8Ldp4HlOW+3h8Tg49O/iSIus2roGK5jHQLIWqExQLf3QjnttbsfZtdBbcvrPTho0ePZubMmWzbto3evXsDsGjRIm644QaaNm3KxIkTbcfeddddbNiwgT/++KOMoNtafVz5fbtp/VpOnjzJt99+S6NGjQB45plnePDBB4ud9tgD90HyEVBpiIiN5tTZ8yxfspgH7xiGl9qIj48PGo2m3HTyZcuWodfree+99/Dx8QHgtdde45FHHuH5558nNDQUgICAAF577TU0Gg2tW7dm4MCBbN68WYJuIYQQznHhci6P/LSTuAvpqFXw0k0deKB/S5knLeoF60i3FFITQtQc9//72rp1a2JjY1m4cCG9e/fmzJkz7Nixgx9//BGTycTXX3/NypUrSUxMxGAwoNfr8fLyKnkhswlMemVbq4y2nzh+jPDwcFvADRAbG1vi1BXLlvLj7NmcS7hETp4eo9GIn68SOJNfubnhJ06cICoqyhZwA3Tr1g2z2cypU6dsQXebNm3QaAqXkQwLC+Po0aOVuoejJOgWQoh6YtPxZJ74eTep2XqCfLRMu7MbV7cJdXWzhKgxhUG3jHQLIWqASgX3r3RNenlurl2njBkzhrfeeovXXnuNRYsWERkZSa9evZgxYwY//vgjL7/8MlFRUXh7e/POO+9gMBhKXsQ6yq1SgUdBUG4Nwsuxe/dunn/ldZ68dyz9+l+Df+NWLF++nFmzvlMOMGQrWQNO4uFRPARWqVRYnHj9Uu9ZrVcXQgjhcmazha/WHufjVUcxW6Bz0wZ8PaE7EUE+FZ8sRB3iqZGgWwhRw1Qq0JVdL6VaOBBA3nTTTbz99tssW7aM3377jXHjxqFSqdi1axfXXXcdI0aMAJSiqqdPn6Z169YlL2Kbz60CD6Uoa+uIhly8eJFLly7RsKGy5OeePXuKnbZ7926aNGrIoxNuhcDm4BNMfHy8ch2VBiwmtBrl3uVp3bo1ixcvJicnxzbavWvXLtRqNS1btrS7T5xJ7dK7CyGEqFYpWfnc+/12PvxLCbjHdI9gwSN9JeAW9ZKklwshROl8fX0ZOnQoH3/8MUlJSdx6660ANG/enE2bNrFr1y5OnDjBa6+9RnJycukXsQbdKhWo1KDR0bd7NC2aR/Liiy9y+PBhduzYwSeffFLstObNm5OQmMTyNZs4m5DMjz/+yOrVq5UnCz6waNowmPPnz3Po0CFSU1PR60uOoN98883odDpefPFFjh49ypYtW3jzzTcZMWKELbXcVSToFkKIOmrH6VSGfb6Bf48m4aVV88GYGD4c2wUvrabik4Wog7xkyTAhhCjTmDFjSE9Pp1+/frY52I8++igdO3Zk4sSJ3HXXXYSGhtqW5yqh6Eg3gNYXtVrNtPffIC8vjzFjxvDf//6XZ555pthp1w24mnvGDOWNz2YxYszt7N69m0cffVR5UucHwI39etC/f3/uvvtu+vTpw7Jly0rc3tvbm5nffsPl5ETGjBnDU089RZ8+fXj11Ver3DdVJenlQghRx1gsFmasP8l7K49gMltoFebL/8Z3Jyq85PIaQtQnMtIthBBli42N5ciRI8X2BQYG8tVXX5V73k8//aRsXDoIwJqVS8GrAWRdgrw0WjYOZu7cucXOKXYfQw6THxnP5CcfhLAo2+57770X9NmQCTr0fP7ZZ8ooelnXAaKaBPLjhy8qaemNOoG6+EBD0aXDrEpbN9zZJOgWQohaYvOJFH7fc4GG/p60DPOlVagfrcJ88ffS2o65nKPn+V/3svrQJQBGdG3CO7dG4+spv+6FkEJqQghRTSxmMBakfFuLqFnnshtylHnmZa2UYiwo+qYtZeqb1htQg8WkjKRry1mD3GyE7ILUd4sJclLAr6HdL6U6yLswIYSoBVbuv8gTc3dhNJcsjhLm70mrUF9ahfny79FkLlzOReeh5v9u7sidvSJlOTAhCnhK0C2EENXDqAcsBXO5CwYDtN6ASgmGTXrw8Cz9XH1ukeOvoFKDzgf0WcpXeUF3drISbKNS2pJ1CXxDlWu4mATdQgjh5pbvS2DSvN2YzBaujQojPMCbk0lZnEzOJikz3/a19VQqAM1DfPjyzm50bhrg4pYL4V68Jb1cCCGqh3U+t4dX4Yi2Sq0EyYYcJU28tKDbYiky0l1GQK3zKwi6s8E3rPRjzGbITlK2AyIg8yKYDZCTqgTeLiZBtxBCuLEle+N55pc9mMwWRsU25YOxXdCoC0euM/IMnErK5mRyFqeSsvHQqLn36hY0KJJyLoRQSHq5EEJUk6JBd1E6XyXoNuQAwSXPMxmUkXBQ1hcvjTVNPT+r7PvnpijX0ejAJ1hJd8+4oIx2+4SUndpeQyToFkIIN7V493mem7/XttTXe6NjigXcAA28tHRpFkiXZoGuaaQQtYh1ne7sfBnpFkIIp7IF3VeMZlvnaeuzSz/PUDDK7eEF6jLSwK1Bt9kAxvyS97AUpJID+DZURth9QpTRblM+5F0G7yC7Xo6zuT7BXQghRAkLdp7n2YKA+46ezXi/lIBbCGEfLw/lbU+ujHQLIaqJ2Wx2dRNco7yRblCC69L6xpijPJZWRM1KrSk/eM9NU+aMqz2UUW7rOX4FqeiZiUpg7iBn/JvKSLcQQriZX7af5cVFcVgsML53JG+O6IxaAm4hqsy2TrcE3UIIJ9PpdKjVauLj4wkLC0On07mskKnFYiE/Px+1Wl0zbbBYIDcPsIBRBXl5xZ8zacBihKzLSlG0orKywGgBs0fx80rwAmO2cg11kWtYLJCWACYL+AaC3gAYlOfU/mBMVAL7jGTwrNzSqdb+U6lUGAwGkpKSUKvV6HS6Sp1fGgm6hRDCjczZeob/Lt4PwD19mvP6LZ2k+rgQTmINumWkWwjhbGq1mpYtW5KQkEB8fLxL22KxWDAYDGi12pp5D2E2QkYioIIsr5Lzp7MvKyPdqcaSgW9GvHK+nwo8Msq+hyFXKZSmuQz++SX3q9TQwBNUV8z7zs2F/Ay4mAH+jSr1cq7sPx8fHyIjI1GXlf5eCRJ0CyFEDdEbzWw+mcKljDzScvSk5RhIy9YX2z52Sfljcf/VLXl1eAcJuIVwImvQrTeZ0RvN6Dxklp0Qwnl0Oh2RkZEYjUZMJtd9uGcymTh8+DBt2rRBo9FU/w1Pb4KNz0FQKxg/v+TzO9bCli+hzfUwZGrh/tzLsHyssv3gP+WPROdehpl3K9v3rwKfIGWUe8FESNwHsfdA1/4lz8tOgh9GgFkPt06Hpt0rfDlF+0+n0+Hh4VHl92MSdAshRA3YfTaNFxfGcSQxs8JjHx7Qihdvai8BtxBOZl2nG5TRbgm6hRDOplKp0Gq1aLWuW0XEGvB7eXnVTNCddgiyzkGzWPDyKvl8007K86dWgdcnhfsvFJwX1BICylgKzMorHLx9IekwJO6ADjfD6Q1wYjloPKHX3aXf26sZtLsGdsyETR/CXYsrfDnV0X8SdAshRDXKzjfy4V9H+H7TaSwWCPLREhMRSJCPlkAfHcG+OoJ8tAT56gjy0dE00JsWob6ubrYQdZJWrUKrUWEwWcgxGAlAltYTQogqSzqiPIZFlf58k1gl/Tv9nFJR3D9c2X9xn/LYOKZy94nsowTdZzYrQff6j5X9sePLTx2/ehLs/B5OrIH43Up7apgE3UIIUU3WHU3i5UVxXLisLIcxKrYprwzvSLCv44U4hBBV463VYDAZyc6Xed1CCOEUyUeVx9Aygm5PfwjrAJcOwPkd0GG4sj+hIOgOr2TQ3bwv7JwFZzdB/B448bcSzPedVP55QS0geizsm6cE6rf/VLn7OZHkVQkhhJOlZet59pc93PPdNi5czqVpoDff39eTj2/vKgG3EC7m46mMN0gxNSGEcBLbSHe7so+J6KE8nt9euM820t2lcvdp3ld5TNgH/7ytbHceDcEtKz633zPK46Glhe2tQRJ0CyGEk1gsFpbsjWfwx+tYtPsCKhXcd3UL/npmANdENXR184QQgK9OmZ+XrTe6uCVCCFEHZCdDbiqggpC2ZR8X0VN5PL9DeczPguRjynZlR7oDIiAgEiwmOPaXss8aTFekYXtoPxywwIZPK3eOE0nQLYQQTrDtVCpjv97MpJ93k5KtJ6qRP4se7cv/3dwJX0+ZySOEu/DWKkF3jgTdQghRddZR48BmJdfgLsoadMfvApMREg8AFvALr/RSXgA071O43W4INOpU+XP7P6s87vsF0s5U/jwnkKBbCCGq4EB8OvfO2sZt0zez40wanh5qnhncjqVP9iM2MsjVzRNCXMHX0xp0S3q5EEJUWXJB0F3WfG6r0Hbg2QAMOZB0yP4ialaRRYLufs/ad27T7tDqGmWkfOf39p1bRTL8IoQQDjidnM1Hq46ydG88ABq1ijt6NmPSdW1p1KCUJSuEEG7BW6u89cmRQmpCCFF1SQVF1MqqXG6lVkPTbnByrTKvO2Gvsr+yqeVW7YaAdxC06A+Rve1uLoOnwIL7lA8BapAE3UIIYYfEjDw++/sY87efw2i2AHBLlyY8e307WepLiFrAx1PSy4UQwmlsI92VCGIjehYE3Tsgcb+yz96R7gaN4T8n7TunqCZdYdJux893kATdQghRxOUcPSeTs0m4nEdCei7x1sf0PBIu55KUlY9FibW5NiqM52+MolOTANc2WghRaYWF1GSkWwghqqyyI91QOK/7zCZIP69s2zvSDcqoeS0jQbcQQgDpOQY++/sYP24+bRvBLkuP5kH858YoercKqaHWCSGcxVsnI91CCOEU+VmQURA8V2aku2nBsmFpp5RHzwBlDe16QIJuIUS9ZjCZmbv1LJ+sPsrlHAMAjQO8aBLoTeMAL5oWPDYO9KZJgDdNAr0I8fN0cauFEI7y1RXM6ZaRbiGEqJrkglFu3zDwCa74eN8QCG4FqQXp4Y1jQKWqvva5EQm6hRD11j9HLvHWsoOcSMoGoG1DP14Z3pGB7cJc3DIhRHWxLRkmhdSEEKJqrEF3RZXLi2raozDodiS1vJaSoFsIUe+cTTfw6fc7WH8sGYBgXx3PXN+OcT2b4aGpffOEhBCVZyukZpCgWwghqsS6RneYHZXAI3pC3Hxl294iarWYBN1CiHrDYrHw7sojzNyQgtkCWo2K+65uyePXtiHAW+vq5gkhaoAtvTxf5nQLIUSVODLSHdGjcFtGuoUQou7ZfCKFGeuV4h03dGzEy0M7yDJfQtQz3rbq5RJ0CyFElTgy0h0eDWHtQaWu8bWyXUmCbiFEvbFkbzwA17X05n/jY9FoNC5ukRCiplmXDMuVQmpC1G4J++D4Kug7CTSSrVbjjPrCudn2jHRrtPDIxoLt+hOK1p9XKoSo1/KNJv7YfxGA/pFeLm6NEMJVvGWdbiHqhr/+C6f+VZac6jza1a2pf1JPgsUEOn9o0MS+c+tRsG3lFhWD5syZw6BBg4iOjmbs2LHs27evzGPvuusuoqKiSnw99NBDNdhiIURt8+/RZNJzDTTy96RjmM7VzRFCuIh1TreMdAtRy6WdUR7j97i0GfVWckFqeWjberPsV1W4/GOGFStWMHXqVKZMmUKXLl344YcfmDhxIitXriQkJKTE8V988QUGg8H2/eXLlxkxYgRDhgypyWYLIWoZa2r5sJhwNKp8F7dGCOEqPjKnW4jaz2KBTCV7jcT9rm1LfZVUUEQtzI7U8nrM5SPds2bN4rbbbmP06NG0adOGKVOm4OXlxcKFC0s9PjAwkLCwMNvXxo0b8fLykqBbCFGmHL2R1QcTAbg5xs4UKCFEnWINumWdbiFqsdw0MBV8gH5Rgm6XsI10159iaFXh0pFuvV7PgQMHePjhh2371Go1ffv2Zffu3ZW6xsKFCxk2bBg+Pj523dtkcs4fW+t1nHW9+kT6znHSd/b5c38CuQYTkcE+dAz3ZX+y9J0j5OfOcfb2nfRx9bEG3XqTGYPJjFbj8vEHIYS9MuILt7MvQWYi+DdyXXvqI1vlchnprgyXBt1paWmYTKYSaeQhISGcPHmywvP37dvH0aNHefvtt+2+d1xcnN3n1OT16hPpO8dJ31XOnA1pAPRqpGL/fuUTcek7x0nfOU76zvW8dYVvfXL0JgK8JegWotbJTCj+fWKcBN01yWyG5GPKtj2Vy+sxl8/prooFCxbQrl07YmLsX1g9OjraKcsFmUwm4uLinHa9+kT6znHSd5V3OUfP3kX/APDADbG0CvGWvnOQ/Nw5zt6+sx4vnE+nUeGhVmE0W8jVmwjwlqWGhKh1rgy6L+6HNoNd0xZXsFgg+SgEt3ZNJfD0c2DMBY1OqR4vKuTSoDsoKAiNRkNKSkqx/SkpKYSGhpZ7bk5ODsuXL2fSpEkO3Vuj0Tj1TaOzr1efSN85TvquYqsOJWEwWWgf7k/7xgG2tF3pO8dJ3zlO+s71VCoVPjoNGXlGKaYmRG2VceVIdz2b173rR1g6CQa/Dv2eqfn7JxcUUXNV0F8LuTSnSqfT0alTJzZv3mzbZzab2bx5M7GxseWeu3LlSvR6Pbfcckt1N1MIUYtZq5bf0lUKqAkhFD4FKeZSTE2IWso60t2km/JY34qp7ZuvPJ7e6Jr72+ZzSxG1ynL5RKb77ruP+fPns3jxYk6cOMHrr79Obm4uo0aNAmDy5Ml89NFHJc5bsGABgwcPJigoqKabLISoJS5l5LH5pJJJI1XLhRBWPp4FFcxlpFuI2skadLe9QXlMPgqGPNe1pyblpcO5Lcp26gnXtMFWuVzmc1eWy/MBhg4dSmpqKp9//jlJSUl06NCBb7/91pZenpCQgFpd/LOBkydPsnPnTr777jtXNFkIUUss25eAxQLdIgNpFmzfCgdCiLrLtmyYXka6haiVrNXLm8SCdzDkpkLSIeX7uu7kWjAXfGCYdgZMBtDUcG0KWaPbbi4PugEmTJjAhAkTSn3up59+KrGvVatWHDlypLqbJYSo5Wyp5V1klFsIUciWXi5BtxC1U+ZF5bFBYwjvDKf+VVLM60PQfeyvwm2LSQm8Q9vU3P2Neri4T9lu2LHm7lvLuTy9XAghqsPZlBz2nLuMWgXDJLVcCFGEb8FItxRSE6IWMhkgO0nZ9m8C4QWrGNWHYmoWCxxbrWyrC8ZOU47XbBsu7ARDDviEQlj7mr13LSZBtxCiTlq6Txnl7ts6lDB/Txe3RgjhTgoLqUnQLUStk5UIWECtBZ8QaNRZ2X+xHiyzeDEOsi6C1qdwPntNB92n1imPLfuDWkLJypKeEkLUSUv2SGq5EKJ0tjndBkkvF6LWsS4X5h+uBH3h1qB7vzISXJdZU8tbDoSGHZTtmi6mdtIadA+s2fvWchJ0CyHcyvJ9Cby+5AD7L6Q7fI3DFzM4kpiJTqPmxs7hTmydEKIusAXdsmSYELWPtXK5f2PlMTRKGfXOT4f0c65rV004tkp5bHs9hBTM467JkW59Npzfrmy3kqDbHhJ0CyHcRnqOgWfn7+H7TacZ/sUGxn+7hbVHLmGx85Nr6yj3wKgwArxruKKnEHXQ9OnTGT16NLGxsfTp04fHHnuMkydPVnjeH3/8wZAhQ4iOjubmm29m3bp1NdDaivl4SiE1IWqtzCIj3QAeusIq2nV5ve7cNDi/Tdluez0Et1a2Uyr+XVxCXgac3mB/ZsDZzWA2QEAkBLW0/771mATdQgi3sWj3efKNZhp4eaBRq9h4PIV7Z21nyKfrWbDzPHqjucJrWCwW23xuSS0Xwjm2bdvG+PHjmT9/PrNmzcJoNDJx4kRycnLKPGfXrl0899xzjBkzht9++43rrruOxx9/nKNHj9Zgy0vnq5N1uoWotazLhTUo8jc+PFp5rMvzuk+sAYtZKV4WGFk40p1xHvRl/y4u1V+vwPfDYO/P9p1nSy0fACqVfefWcxJ0CyHcgsVi4edtZwH4z41R/Dv5Wib2a4mvTsORxEye/3Uv/d9fw//WniAtW1/mdXafu8y51Fx8dBoGd2hUU80Xok6bOXMmo0aNom3btrRv3553332X+Ph4Dhw4UOY5P/74I/379+eBBx6gdevWPP3003Ts2JHZs2fXYMtL511QSC1bRrqFqH2sy4VZ08uhsJhaYh0OuoumlgP4BINXgLKddsq+a53ZpDzaG3Rbi6hJarnd3GKdbiGE2HkmjaOJWXhrNYyIbUoDLy2vDu/IpOva8vO2s8zaeIrEjHzeW3mY91YeJszfk1ahvrQK86VVqB+twnxpGerLb7svAHB9x0Z4F4xmCSGcKzMzE4CAgIAyj9mzZw/33ntvsX39+vVj9erV1dm0SrGOdOfKSLcQtU9mwUh30aC7aDG1ushshuMFvzutVctVKmW0+8JOZV53o06Vu5Y+p7D42ukNkJ0MvqEVn5eTCgkF63O3HGBf+4UE3UII9zC3YJT75i6NaeBVOA87wFvLIwNbc//VLVmyN55v15/k8MVMkjLzScrMZ+up1FKvJ6nlQlQPs9nMO++8Q7du3WjXrl2ZxyUnJxMaWvyNXEhICMnJyXbf02Sq+oi09RomkwlPDyUtMivf6JRr13VF+07YR/rOcWX1nTojARVg8msE1ufCOqIBSDuFKecyePrXZFOrX/xuNNlJWHR+mJv2tL1uVVAr1Bd2Yk4+jqVIP5X7c5d4EI2lYLqexYz54O9Yut1bcRtO/osGC5bQKMw+YYV9XwfZ8/+2sv+3JegWQrhceo6B5fuUwijjekWWeozOQ82Y7hGM6R5Beq6B08nZnEzO4mRStvKVnM2p5CzyDGaaBXvTv21YTb4EIeqNKVOmcOzYMebOnVtj94yLc17KaFxcHInxeQCkXM5kz549Trt2XefMf4f6RvrOcVf2Xdf0C2iAQ+fTyL+8x7Y/2isUXV4yxzb+TnZw55ptpAO0uUkYPQOxqCsu+Nr46E80AS4Hd+Vk3MHC/QZfmgCpx7ZxxndPifNK+7kLObuSFkW+z9o6h2PqrhW2oVncIhoCSX4dOFdPfm868/+tBN1CCJdbuEspoNahcQO6Ngus8PgAby1dmgXS5YpjzWYLFzPyCPTRovOQkhVCONsbb7zB2rVrmT17NuHh5S/HFxoaWmJUOyUlpcTod2VER0ej0VRtuojJZCIuLo7o6GhyG1yGjdvBw5OuXbtW6br1QdG+q+q/Q30jfee4UvsuPxPNUqVoWIee1xYb0VYfioXjq2jXIB+Lu/+/PrcF9Q93Qot+mMcvBFX571nUu/4DQECPMcV+Z6k8TsCR7wnhMkFF9pf3c6dK/AUAS9shqI6txD9lD13bNQOfkPLbsEkJ9kN6jiakfddyj63t7Pl/az22IhJ0CyFcqmgBtTt7NUNVhWqYarWKJoHezmqaEKKAxWLhzTffZNWqVfz00080a9aswnO6du3Kli1bis3r3rRpk0NBrkajcVrAotFo8PPSAcqSYRIIVZ4z/x3qG+k7xxXru5wk5VHnj8YnsPiB4dFwfBXqSwfA3ft66//AYoJT69DsmQ097iv72OwUuLALAHW7G4u/trC2AKhST5T681Xqz90lJXhWdbwFMi+guhiH5ugK6H5v2W3IiIeUY6BSo2k1wP3710mc+f9WhoKEEC6140waxy4VFlATQrifKVOmsGTJEj766CN8fX1JSkoiKSmJvLw82zGTJ0/mo48+sn1/9913s379er777jtOnDjBF198wf79+5kwYYIrXkIxsmSYELWUbbmwxiWfqy3F1DLi4fCKwu9X/R9kJJR9/Im/AYtSob3BFfVqrGt1ZydBXnrF97ZYILFg1YlGnaDjSGX74O/ln3fqX+WxcVfwDqz4PqIECbqFEC7189bSC6gJIdzHzz//TGZmJnfddRf9+vWzfa1YUfjGMSEhgaSkJNv33bp148MPP+SXX35hxIgR/Pnnn3z55ZflFl+rKdaVDWTJMCFqmdKWC7MKj1EeEw+AuQb+b1/YCcnH7D9v5w/KKHez3tCkG+Snwx//Kfv4Y38pj9alworyagC+DZXtlBMV3zvzIuSmKunsYe2h063K/pPrlOrkZSm6PrdwiKSXCyFc5nKOnmVxyqe7d/Zu7uLWCCHKcuTIkQqP+emnn0rsu+mmm7jpppuqo0lV4luwTrfeaMZoMuOhkTEIIWqF0pYLswpuBR7eYMyF1JMQ2rb62pF+HmbeCFofeGqPsmZ2ZZgMsOsHZbvXQxAWBdMHwqGlcGgZdBhe/HizCY7/rWxblwq7UkgbyL6kvOam3cq/v3WUO6QtaL0gpDU0ilbWNz+8DLrdXfIci0XW53YC+SsjhHCZRbsuoC8ooNYlouz1foUQwpl8PAvn6OUYZLRbiFrDmoZdWnq5WgONOirbF6u5Wvypf8FsUEapN35W+fOOrIDMBPANgw43K/PQr35KeW7F8yVTxC/sUkamPQMgolfp1wxppTymHK/4/okFqfdF1/TuNEJ5PPBb6eeknICMC6DRQbOrKr6HKJUE3UIIl7BYLLa1ue/sHVmlAmpCCGEPnUaNRq38zsmVFHMhao/MgqC7tJFuUOY9Q2FwWV1Obyzc3vYNZF2q3HnbZyqPsXeBh6eyPXCyMkqfmQCrXy9+vDW1vPW1oCkjQTmkjfJYqaC7yHxuq44FKeanykgxt45yR/QCnU/F9xClkqBbCOES20+ncdxaQK1rk4pPEEIIJ1GpVPhY53XnSzE1IWqNioLu8GjlsbqLqZ3ZoDx6BoAhBzZ8UvE5yccKAlhV8WrlWm+4uWC0fMd3cGZz4XPHVymPZaWWQ2ExtcrM6bYF3UXWMQ9to3xvNioj8VeS1HKnkKBbCOES1mXCbunSRAqoCSFqnI+tgrmMdAtRa1gLqV1ZxdvKFnRXY3p5+gVIO60UIxvxhbJv+0xlf3l2fKc8thsCgZHFn2s5QBn9Blg6CYz5yuh5/G5lX5vBZV/XNtJ9Qpl/XRajHpIL6nMUHekG6FhGirnZDKfWF7RRgu6qkKBbCFHj0rL1LC8ooDaud2QFRwshhPNZi6lJ0C1ELWE2FxnpDi/9GGswmRlffjXuqjhTkFreuAt0uAWaXw2mfFj/Ydnn6HNgzxxlu+fE0o+54U2lEnnyUVj/ERxfXXgf/0ZlXzu4pfKYnw45KWUfl3xUGc32DICAiOLPWZcOO7kWctMK9yfGKXPKdX4VF2kT5ZKgWwhR4xbtVgqodZQCakIIF7EWU8uWtbqFqB1yUpSgERX4lRGEevpDUAtlu7pGu08XpJY3vxpUKrj2v8r3u35URsBLs3+hUiQtsDm0vq70Y7yDYOj7yvb6j2HbDGW7vNRyUNLTA5op2+XN6y46n/vKOjph7aBhR6U4XNE1xK1LhTXvCxrJSqwKCbqFEDXKYrEwd+sZQBnllgJqQghX8NEqI91SSE2IWsK6XJhvWPkBYHUXU7MG3S36FTxeDa0HKR8IrPug9HO2f6s89rgf1OWEXx1HQrublOA3fpeyr6KgG5RCbFD+vO7SKpdfeW+Ag78V7jv1r/IoqeVVJkG3EKJGbT+dxomkbLy1GkZKATUhhIvYRrqlkJoQtUN5y4UVFR6jPFbHSHfmRUg9Aaggsk/h/mtfUR73zoXkK0abL+yEhD2g8Syct10WlQqGfaikc4My+t20e8XtqkwF89IqlxfVaaTyeOIfyL2szAE/s0nZJ0XUqkyCbiFEjbBYLGw4lsyUpcov/Vu6NMFfCqgJIVxECqkJUcvY5nNX8IF9eMFId3VUMLeOcod3Bu/Awv0R3ZURaosZ1k4tfo51mbBOI8E3pOJ7BEQo87tBGX1Wayo+J6SggnlqeSPdpVQuLyosCsI6KKPsR/5QPiwwZINPCDQsI1AXlVbGgm9CCOEcBpOZ5fsS+ObfkxxMyADAS6vmvn4tXNswIUS95iOF1ISoXSoqomZlDSqTDiujtR4657XBWkSteb+Sz137Mhz9Q5m/3f85aNRRKea2f6HyfM8HKn+fHvcrc8YDm1fu+KIVzEuTnQxZBZXfG3Yo+zqdRsLaQ0qKeZNYZV+L/uWnxItKkR4UQlSLzDwDM/49yYD3/+HpX/ZwMCEDb62Ge/u24K+nB9I+vIGrmyiEqMd8bSPdkl4uRK1gDbrLWi7MKjBSqdBtNigVu53pdEHQ3eLqks81jimYF22Bte8o+/bMBWMeNIqGiJ723SssCrRelTvWulZ36kmlyvuVrKPcQS3B06/s61iXDjuxBg4vU7YltdwpZKRbCOFUadl6vl53grlbz5JZMFcy1M+Te/s2Z8JVzQn0ceInzkII4SBvGekWonbJqORIt0qlzFs+u0mZ1x1eRjq1vbKSCte5bl5K0A1wzUtw8Hc4tBQu7Cpcm7vnxJIVw50pqDmoNGDIUT6c8Luijyqaz23VsAOERimv0zonXoqoOYUE3UIIp8nMM3DHN1s4kpgJQOswXx4a0IoRXZvipa3EnCQhhKghMtItRC1T2TndAOHRStDtzArm1tTyhp3AJ7j0Yxq2h5jbYN8vMP8eSD8Lng0geqzz2lEajVZZKi31hPJ1ZdB9qYL53EV1Ggnr3lO2A5oVVkYXVSLp5UIIpzCazDwxdzdHEjNp6O/JzHt6sOqZgdzeM1ICbiGE2/HWWauXy0i3ELVCZed0Q5Fiak6sYH6mnNTyoga+oIw6p59Vvu9yR/kp3c5iLaZWWgXzyo50Q+HSYQAtB1TvCH09IkG3EKLKLBYLU5YeZN3RJLy1Gmbe05PrOjRCrZZf1EII9+TrKenlQtQaxnzISVG2K5rTDcXX6rZYnNMG63zuslLLrUJaQ+z4wu97THTO/StSVjE1swkuHVK2KxN0W1PMAVpd67z21XOSXi6EqLJZG0/z05YzqFTw6R1diY4IcHWThBCiXD6SXi5E7ZFZUHlb46msXV2Rhh1ApVYC9cyEygXq5clJLUzRrijoBmW0+/gaaN5HSTmvCdY08CuD7tSTSjE3rY9SSK0iKhWMmQmn1kPnUc5vZz0lQbcQokpWH0zkzeUHAXj5pg7c2KkSaV9CCOFismSYELVI0dTyyqQ7a70htJ2ybFjC3qoH3dbU8tAo8Aur+PiACHj2QNXuaS/bSPcV6eXWee0NO1Z+6a/waOVLOI2klwshHLb/QjqT5u3GYoFxvSJ5oH8lPkEVQgg3IIXUhKhFMuKVR3uC56bdlccLO6t+//KWCnMX1jndaafBXOT3mj3zuUW1kaBbCOGQi+l5PPDDDnL0Jvq3DeWNEZ1QSbENIUQtIYXUhKhFrOnllSmiZmUNus/vqPr9z2xQHlv0q/q1qkuDCCX93myAy+cK90vQ7RYk6BZC2C0738jEH7ZzMSOPtg39mHZnN7Qa+XUihKg9rIXUcg0SdAs3dng5bPzc1a1wvcyCke7KLBdmZRvp3gVms+P3zk2DiwUp2s3dOOhWqwvndacWmddtTS+XoNul5F2yEMIu+UYTT83bw4H4DEJ8dXx3b08CvLWubpYQQtjFxzbSLenlwk1ZLPDbY7DqVUg64urWuJZ1pLtB48qf06gTeHhBfnrxINReZ7cAFmXOtH8jx69TEwpSzFXW15uXAZcLli5r2NFFjRIghdSEEFdIzzEwZ9sZLmXkk5qtJy1Hz+UcA6nZei7n6MkuKDqk81Az454eNAv2cXGLhRDCftZCavlGMyazBY0scSjcTXYy5F1Wti+fhbAolzbHpTKshdTsCLo1WmjcFc5tUVLMQ9s6du/TBanllala7mrWed2pJyG8NyQphW5p0BR8gl3XLiFBtxCiuJcW72NF3MVyj2ng5cF7o2PoFlmJZTuEEMINWUe6QSmm5u8lGTvCzaSdKty2FhKrrzIdCLpBSTE/t0UpptZ1nGP3tlYud+f53FYFFcxVKcchHFSJBUG3pJa7nATdQgibf45cYkXcRTRqFQ/0b0mYnydBPjqCfLXKo4+OIF8d/p4eqGVUSAhRi3l6qFGrwGxRlg2ToFu4ndSThdv1Oei2WIovGWaPCOu8bgeLqeVlKEuOQe0Y6Q4uMtINhWuLS9DtcnYH3YMGDWLUqFGMGjWKJk2quOadEMJt5BlM/N/vyi/n+/q24KWbOri4RUIIUX1UKhW+Og8y842yVrdwT8WC7guua4er5WeAIUfZtne97aY9lMeL+8GQB1ov+84/txUsZghqAQFN7TvXFaxrdaefQ2XSo7JVLu/sujYJwIFCanfffTerVq1i8ODB3HfffSxfvhy9Xl8dbRNC1KAv/znO2dQcGgd48fT17VzdHCGEqHY+nlJMTbix1CLp5daR3vrI+tq9AkHrbd+5gZHgE6oso3Uxzv572+Zz14LUcgC/hqDzQ2Ux45lzAS5Jerm7sDvovvfee/n999/59ddfad26NW+++Sb9+vXjjTfe4MCBA9XRRiFENTt+KYuv1ymVLv/v5o74ecrMEyFE3WctpibLhgm3JOnlCkfncwOoVBBRMNrtSIr56VqwPndRKpWtmFqDpJ2o9Fmg0RWOgAuXcXjJsE6dOvHKK6+wfv16Hn/8cX799VfGjBnDiBEjWLBgARaLxZntFEJUE4vFwqu/7cdgsnBtVBg3drJzvpQQQtRSsmyYcGsSdAOgcmS5sKKsKebn7Qy687Mgfrey3aIWzOe2KgiwAxPWK9+HRSmV3IVLORx0GwwGVqxYwaOPPsp7771H586deeutt7jhhhv45JNPeP755yt1nTlz5jBo0CCio6MZO3Ys+/btK/f4jIwMpkyZQr9+/ejcuTM33ngj69atc/RlCFHv/b4nns0nU/D0UPPGiM6oVFIgTQhRP1iDbpnTLdxO7mXITS38Pu8y6HNc1RrXqspIN0DTbsrjhZ32nXduK1hMEBCppKnXFgXF1PxS9yvfy3xut2B3DumBAwdYtGgRy5YtQ61WM3LkSF566SVat25tO+b6669nzJgxFV5rxYoVTJ06lSlTptClSxd++OEHJk6cyMqVKwkJCSlxvF6v57777iMkJITPPvuMRo0aER8fT4MGDex9GUIIlDW531quzPeZdF1bWXNbCFGvWNPLJegWbse6XJhvQ6WImD5LCT5DWpd/Xl1U5aC7oIJ52inITgHfkjFGqWxLhdWiUW4oXDaMgqxjmc/tFuwOuseMGUPfvn15/fXXGTx4MFptyXSFiIgIhg0bVuG1Zs2axW233cbo0aMBmDJlCmvXrmXhwoU89NBDJY5fuHAh6enpzJs3z3bfiIgIe1+CEKLAB38dJjlLT+swXx7s38rVzRFCiBrl62kd6Zb0cuFmrKnlwa0gJwVSjikVzOth0F3l9HLvQAhpq/ThhZ3Q7obKnXe6IOiuDUuFFXXlz4gE3W7B7qB79erVNG1afsl8Hx8fpk6dWu4xer2eAwcO8PDDD9v2qdVq+vbty+7du0s9Z82aNXTt2pU33niDv//+m+DgYIYPH86DDz6IRqOx96UIUa/tOXeZOVvPAvDWyGh0Hg7PNhFCiFrJWysj3cJNWSuXB7cCD8+CoLuezuuu6kg3KKPd9gTdeemF6ei1baQ7+IpBFEkvdwt2B90pKSkkJyfTpUuXYvv37t2LWq0mOjq6UtdJS0vDZDKVSCMPCQnh5MmTpZ5z7tw5tmzZws0338w333zD2bNnmTJlCkajkSeeeMKu12EyOecPrPU6zrpefSJ957iq9p3RZOa/i+KwWGBk1yb0ahFYb/4d5OfOcdJ3jrO376SPa4ZtpFsKqQl3UzTotqYJS9Dt+DUiesC+eZWvYH7wd2WZsbD2ENTS8fu6gk8wFu9gVLmpWHzDUPk1dHWLBA4E3W+88QYPPPBAiaA7MTGRGTNm8OuvvzqtcVeyWCyEhITw5ptvotFo6Ny5M4mJicycOdPuoDsuzoG1+mrwevWJ9J3jHO275ceyOZCQia9WxS3NDOzZs8e5DasF5OfOcdJ3jpO+cy/e1urlMtIt3I0tvbwlGHOV7foYdFtMkHVJ2a7qSDcoo9cWi7K0Vnn2zlMeu9xR8bHuKKQ1nE+Fhh1d3RJRwO6g+8SJE3TqVHJuQIcOHTh+/HilrxMUFIRGoyElJaXY/pSUFEJDQ0s9JywsDA8Pj2Kp5K1atSIpKQm9Xo9Op6v0/aOjo52Skm4ymYiLi3Pa9eoT6TvHVaXvEtLzmL9EWUbixaEdGNirFlXkdAL5uXOc9J3j7O076/GievlKITXhrqyF1IJbKpXLoXDEtx7R5qehsphApYaqjNg26gwaT8hNUz7QKG9ufNqZgiJqKoi+zfF7upAlpA2q89uxNOpELfzIoE6yO+jW6XQkJyfTrFmzYvuTkpLw8Kj85XQ6HZ06dWLz5s0MHjwYALPZzObNm5kwYUKp53Tr1o1ly5ZhNptRq5X5p6dPnyYsLMyugBtAo9E49U2js69Xn0jfOc7evkvL1nP/DzvIyjfRtVkg43u3QK2un7+O5efOcdJ3jpO+cy+FS4ZJerlwI/rswgA7uBVkJirbGRdc1yYX0eYVDM75NQJ1FX53euigcQyc366MdpcXdO+brzy2HAAB5dexcleWXo+SmpxIQPf7Xd0UUcDuyklXX301H3/8MZmZmbZ9GRkZfPLJJ/Tt29eua913333Mnz+fxYsXc+LECV5//XVyc3MZNWoUAJMnT+ajjz6yHT9u3DguX77M22+/zalTp1i7di3Tp09n/Pjx9r4MIeqdzDwD98zaxtHELBo18OSLcbH1NuAWQgiQJcOEm0o7rTx6BYJ3EDRoonxfD9PLtXnJykZVUsutmvZQHs+XM6/bYlHmfgN0GVf1e7pKeGdOdXulZFE14TJ2j3S/8MILjB8/nmuvvZYOHToAcPjwYUJCQnj//fftutbQoUNJTU3l888/JykpiQ4dOvDtt9/a0ssTEhJsI9oAjRs3ZubMmUydOpVbbrmFRo0acffdd/Pggw/a+zKEqFdy9SYm/rCDfefTCfbVMXtib1mTWwhR78mSYcItFSuiRmHQnXUJTAbQlFyut66yBd3WPqiKovO6y3JhJ6QcB60PdLi56vcUooDdQXejRo1YsmQJS5cu5fDhw3h5eTF69GiGDRtW6prdFZkwYUKZ6eQ//fRTiX2xsbHMnz/f7vsIUV/pjWYemb2TbadS8ff04Mf7e9G2kb+rmyWEEC7nrS0opJYvI93CjRRdoxvAJxTUWqWaduZFCGxW9rl1jM420h1e9YtFFATdF/eBMV9Ziu1Ke39WHjvcDJ5+Vb+nEAXsDrpBWYf79ttvd3ZbhBBOZjSZefqX3aw7moSXVs139/Wkc9MAVzdLCCHcgq+n8jYoV9LLhTspWrkcQK2GBo3h8lllrnc9Crqdml4e1BK8gyE3FS7uLwzCrYx62L9Q2Y6ROEc4l0NBN8Dx48eJj4/HYDAU23/ddddVuVFCiKozmy28uCiOFXEX0WnUfHNXD3q2CHZ1s4QQwm342JYMk/Ry4UbSrkgvB/BvogTd9ayYmq2QmjOCbpVKSTE/vkpJI78y6D72l1Ld3C8cWl1T9fsJUYTdQfe5c+d4/PHHOXr0KCqVCovFAoCqYA27Q4cOObeFQgi7WSwW3lh2kAU7z6NRq/h8XCwD2oW5ullCCOFWrIXUZKRbuJUr08uh3hZTs6WXN3BC0A0Q0aMg6N4BPFT8OWsBtZixVauULkQp7K5e/vbbbxMREcGmTZvw8vJi+fLlzJ49m86dO5c6B1sIUfM+XnWU7zedBuCDMTEM6eyEuVBCiFopISGBixcv2r7ft28fb7/9Nr/88osLW+UeZKRbuB1jPqSfV7aDWhbur6dBd+FItxMKqUHZFcxzUuHISmW7NlctF27L7qB79+7dTJo0ieDgYNRqNSqVih49evDss8/y1ltvVUcbhRCVlJln4Nlf9vDFmuMAvDmiE6O6Rbi4VUIIV3ruuefYsmULAElJSdx3333ExcXxySefMG3aNBe3zrWsQXeewYzJbHFxa4RASSG3mEHrC34NC/fXx6DbkIuHoWCJYmcUUgNo2k15TD2hBNpWBxYrheoaRUOjTs65lxBF2B10m81mfH19AQgKCuLSpUsANG3alFOnTjm3dUKISttz7jLDPt/Aot0XUKvglWEduKtPC1c3SwjhYseOHSMmJgaAP/74g7Zt2zJv3jw+/PBDFi9e7OLWuZa1kBpArkFSzIUbKLpcWMHUTaAw6M5MqPk2uUqmkqFj0fqAl5OKwPoEF6btx+8q3L/Xujb3Hc65jxBXsHtOd9u2bTly5AjNmjWjS5cufPvtt2i1WubPn0+zZvWnmqIQ7sJktvD1uhN8suooRrOFpoHefHpHVymaJoQAwGg0otPpANi0aRODBg0CoFWrViQlJbmyaS7n6aFGrQKzBXLyjfh5OlxfVgjnuLJyuZU1vbo+FVKzfsDgH178A4iqatpD6efzO6HNYEg5Aee3gUoN0WOcdx8hirB7pPvRRx/FbDYDMGnSJM6fP8/48eNZt24d//3vf53eQCFE2RLS8xj/7RY++PMIRrOFYTGNWfFUfwm4hRA2bdq0Yd68eezYsYNNmzYxYMAAAC5dukRgYKBrG+diKpXKVkwtR4qpCXdQVtBtG+m+CAXvw+s6lS3odlIRNaumBVXLL+xUHvcV1LdoPch5aexCXMHuj3T79+9v227evDkrV67k8uXLBAQE2CqYCyGq35bzeXyzbCPpuQZ8dBqm3NKJMd0j5P+hEKKY559/nieeeIKZM2cycuRI2rdvD8CaNWtsaef1mY9OQ1a+UYqpCfdQ2nJhUBAMqsCkh5wU8KsHK5IUBN0W/8Y49Z1NREExtQs7lA8wrKnlMZJaLqqPXUG3wWCgS5cu/Pbbb7Rr1862v75/Ui5ETUrN1vPuH4eYv+MyADERAXx2RywtQ31d2zAhhFvq3bs3W7ZsISsri4CAwnmRt912G97e3i5smXvw9fSAzHwZ6RbuobTlwgA0WqWwWlaikmJej4Jup48+h0eDWqt8eBE3Hy6fAZ0ftB/m3PsIUYRdQbdWq6Vx48a29HIhRM3Jyjcyc/0pZqw/SVa+ERXw8ICWPHtDe3Qeds8UEULUE3l5eVgsFlvAfeHCBVatWkXr1q2LZa/VV95apYK5BN3C5cwmSDujbAe1LPl8gyZK0J2ZAHStyZbVPIsFlbXQWYOmzr22h6cSeMfvglX/p+zrOAJ0Ps69jxBF2P1O/ZFHHuHjjz/m8uXL1dAcIcSV8o0mvttwioHv/8Mnq4+SlW+kU+MGvD4wiP/cGCUBtxCiXI899hi//fYbABkZGdx2223MmjWLxx9/nLlz57q2cW7A17Mg6M6X9HLhYunnlWWrNJ6lB5r1qZja8b9RnduKWa3FElUNI9DWFPMspUK6VC0X1c3uOd1z5szhzJkz9O/fnyZNmuDjU/xTofq+/IgQzmIyW1i06zyfrj7Ghcu5ALQM9eXZ69sxpGND9u3b6+IWCiFqgwMHDvDSSy8B8OeffxISEsJvv/3Gn3/+yeeff86dd97p4ha6lrcUUhPuwppaHtQC1KV8oG5bq7uOLxtmNsPq1wG41GIkYQERzr9H0x7AN8p2gwho3s/59xCiCLuD7sGDB1dHO4QQRfxz5BLvLD/EsUtZADRq4MnTg9sxpnsEWo0ak0neHAohKicvLw9fX6Xmw4YNG7jhhhtQq9V07dqV+Ph4F7fO9Xx11vRyGekWLlZW5XKrBgVVvDPq+P/b/QshMQ6Lpz8X295Jtcxet1YwB4i5rfQPOYRwIruD7ieeeKI62iGEKLD/QjoTv9+O2QIB3loev7Y1d/dpgVfBvEMhhLBHZGQkq1ev5vrrr2fDhg3ce++9AKSkpODn51fp62zfvp2ZM2eyf/9+kpKS+PLLL8v9IH7r1q3cfffdJfZv2LCBsDD3KQJlXTIsW0a6hauVVbncyppyXpfTy416+OctACx9JmHSBVRwgoNCWisj3FkXocu46rmHEEXYHXQLIarXF2uOYbbAtVFhfHpHLAHeWlc3SQhRiz3++OM8//zzTJ06lauuuorY2FgANm7cSIcOHSp9nZycHKKiohg9erRdH8CvXLmyWHAfEhJS+cbXAB+dFFITbiK1gqDbul51Zh1OL9/1A6SdBr9GWHo/AgePVc99VCq4ZwnkXYawdhUeLkRV2R10t2/fvtx1gA8dOlSlBglRnx2+mMGfBxJRqeDloR0k4BZCVNmQIUPo3r07SUlJtjW6Afr06WPXlLGBAwcycOBAu+8fEhJCgwYN7D6vpvhIITXhLmxzustKL7eOdNfR9PL8LFj3nrI9cDLoqnkp1JDW1Xt9IYqwO+ieNm1ase+NRiOHDh1i8eLFPPnkk05rmBD10RdrjgMwtHNj2jbyd3FrhBB1RVhYGGFhYVy8qFTqDQ8PJyYmpkbuPXLkSPR6PW3btuWJJ56ge/fuFZ90BWfUsbBe48preResAJGdb5R6GWUoq+9ExSrddxYL6tRTqABTYHMo7XjfhmgA9FmYctLAsxIfZuVngEqtrEPt5lSbpqHOTsIS1BJzlwnyc1cF0ndVY0//VbaPnVJIbciQIbRp04YVK1YwduxYey8phACOX8pkRZySMvbEoDYubo0Qoq4wm8189dVXzJo1i5ycHAB8fX257777ePTRR1FXUwGhsLAwpkyZQufOndHr9fz666/cfffdzJ8/n06dOtl1rbi4OKe168prpSVnA3AhMYk9e/Y47T51kTP/HeqbivpOm5dMjDEXi0rNntOpcDaj1OO6aP3wMGRxeNsa8vxblHtNtSGbzv/cjVHrz6EB32DR6BxtfrXT5KcTvfEzAE61HE9a3AHbc/Jz5zjpu6pxZv85bU53165dee2115x1OSHqnS//OYHFAjd0bESHxu6biimEqF0++eQTFixYwHPPPUe3bt0A2LlzJ9OmTUOv1/PMM89Uy31btWpFq1aFc1O7devGuXPn+P777/nggw/sulZ0dDQaTdWKSZpMJuLi4kpc65D+HOw9gM63AV27dq3SPeqqsvpOVKzSfXdmk/IYGEnXbj3KPEy9pRkkHaJ90wBo1bX8m5/4G01+Gtr8NLpqjmHpOt7+F1BDVH/9F7UxB0t4DM2HPk1zlVp+7qpA+q5q7Ok/67EVcUrQnZeXx48//kjDhg2dcTkh6p1Tydn8vkepRvrkoLYubo0Qoi5ZvHgxb731Ftddd51tX/v27WnUqBFTpkyptqC7NNHR0ezatcvu8zQajdPeOF55LT8vpXZGnsEsb04r4Mx/h/qmwr5LPwOAKrhV+cc1aAJJh9BkXYSK/i0u7LRtqrd8CbETnLc0lj4bts+EvT9Dz4nQ8wHHr3X5LOyYCYBq8OtoPIrXs5GfO8dJ31WNM/vP7qC7Z8+exQqpWSwWsrOz8fLysvuTayGE4qt/jtsqlkdHVNPyGEKIeik9Pb3YiLNVq1atSE9Pr9G2HD582K2WC4PC6uXZsk63cCXbGt1lVC63atBEeaxMMbXz2wu3kw7D8VXQ7kbH2melz4Ed38HGTyE7Sdm3YjI0iobI3o5d85+pYNJDi/7QelDV2ieEm7I76H7ppZeKBd0qlYrg4GC6dOlCQIAEC0LY61xqDot3F4xyXyej3EII52rfvj1z5szhlVdeKbZ/zpw5REVFVfo62dnZnD171vb9+fPnOXToEAEBATRp0oSPPvqIxMRE3n//fQC+//57IiIiaNu2Lfn5+fz6669s2bKF7777zjkvzEms63TnypJhwpUqqlxuVdkK5mYzXNihbLccAKf+hU1fOB50G3Jhxywl2M5KVPYFNofASDi9HhY+AI+sB+9A+66beFAZLQcYPEVZykuIOsjuoHvUqFHV0Q4h6q2v1p7AaLbQv20o3SKDXN0cIUQd85///IeHH36YTZs22eYs79mzh4SEBGbMmFHp6+zfv5+7777b9v3UqVMBuPXWW3n33XdJSkoiIaFw/WCDwcB7771HYmIi3t7etGvXjlmzZnHVVVc554U5iXXJMBnpFi5V0RrdVg0K1uquKOhOOQZ56eDhDbd8AV90V4LjC7ugabfKt8uQp6ydvf5jyFJWPyAgEgY8D13vVILx6f2VtbWXPQNjvrMvcF7zJmCBDrdAhP0rGwhRW9gddC9cuBAfHx9uuummYvv/+OMP8vLyuPXWW53WOCHquvjLuSzYeQ6QudxCiOrRq1cvVq5cydy5czl5UhlNu/7667n99tv53//+R48eZRdtKqp3794cOXKkzOfffffdYt8/+OCDPPjgg443vIb4Fox05+TLSLdwEYvFjqC7YKQ7s4Kg25pa3rQbBLWAzmNg3zzY9DmM/b5y7Uo9CT/cAunK+xQCmkH/56DrePAoqISu0cLomfDdjXBgEbS5Tpk7Xhmn1sORFcqSZoNerdw5QtRSdldT+OabbwgKKjkaFxISwtdff+2URglRX0xfdwKDycJVrYLp1TLY1c0RQtRRjRo14plnnuGLL77giy++4JlnniEjI4MFCxa4umkuZ53TnVPV9PJLhyo3z1aIK+WkQn46oFIC5PL4V3Kk2xp0RxR8qNb3SeXx4O+FAX55TAZYMFEJuBs0hWEfw5O7oMd9hQG3VUQPuPZlZXvFZEg+VvH1D6+Aubcr27ETIKxdxecIUYvZHXTHx8cTERFRYn+TJk2KpZUJIcp3KSOPn7crnx5PklFuIYRwCWvQnWswYTZbHLvI5XMwfSD8JFPwhAOs87kbNAGtV/nHWgup5aQoqd9lOWcNunsqj+GdofV1YDHDlq8qbtM/70D8LvAKgIl/KRXKrwy2i7r6aaUQmiEbFtwPxvzSj7NYYOPnMO9O5diWA+GGtypujxC1nN1Bd0hISKnpZYcPHyYwMNAZbRKiXpj+70n0RjPdmwfRp3WIq5sjhBD1krWQGiiBt0PObQVTPiQdgqxLTmqZqDfSKplaDuAdpMzTBsgsY7ArPxMuHVS2rUE3wNWTlMfds5XR9bKcWg8bPlG2b/4cAkoOtpWg1sCob8A7GC7ug7/fKHmMUQ9LnoRVrwIW6HE/TFioBPZC1HF2B93Dhg3j7bffZsuWLZhMJkwmE5s3b+add95h2LBh1dFGIWqdGf+e5PG5u5i25hhrDieSmJGHxVI4gpKclc+crcqanJOua1tsRQAhhBA1x0urttV9criY2sV9pW8LURm25cIqqFwOSpGyioqpXdgFWJSCZ/7hhftbDoTwaDDkwPZvSz83JxUWP6ycHzsBOo2s5ItAGYUfWTCKvnkaHFtd/Lo/3Qq7f1LmcA95T0lZ12hLv5YQdYzdhdSeeuopLly4wL333ouHh3K62WxmxIgRPPPMM05voBC1zbqjSby94hAAy/cVfgod4qujY5MGdGzSgPOpueQZzHSJCGBA21BXNVUIUYc98cQT5T6fkZFRQy1xbyqVCl+dB1n5RqWYmr8DF7kYV7idsA/aDHZa+0Q9UNnlwqwaNFXOKWuk+/w25bFZz+L7VSro+xQsegC2TlfmeWu9C5+3WGDZ05BxAYJbK4GxvaJugp4PwvYZ8Nsj8OgmpYr63NuUNuv8YewsaHu9/dcWohazO+jW6XR8+umnnD59mkOHDuHl5UW7du1o2rRpdbRPiFolV2/ild+UN1/XRoUR4K3lYEIGxy9lkZKtZ/2xZNYfS7Yd/+QgGeUWQlQPf//yo0d/f3/5213AW6dRgm5HiqlZLEqgbSUj3cJela1cbmUrpnah9OfPF6zPHdGz5HOdRsLfU5QCaXvnKYXRrHb/pBRaU3vA6Bng6Ve59lzphjfhzEYlxf3nOyDluBJ4B0TCnb9Ao46OXVeIWszuoNuqRYsWtGjRwolNEaL2++zvY5xLzaVxgBdf3NkNP0/lv1iewcSRi5kciM/gYEI6hxIyadfIj+s6NHRxi4UQdZV1HW1RMV+dhiQgx5H08swEyCn8MLXYqLcQlWFLL69k0G0tppZRyki3xVKkcnkpQbdGC1c9Bn++pKSAd7sH1GpIPg5/vKAcM+gVaFqFNbO13sp63d9cAxd2Kvua9Ybb54BfmOPXFaIWszvofvLJJ4mOjuahhx4qtn/GjBnExcXx+eefO61xQtQmhxIymLFe+cP5xojOtoAbwEuroUuzQLo0C3RR64QQQpTF27pWtyMj3dZRbr9wyLoIKScgP8vxUUJRv+RlFH5oU5k53VC4VndpI92pJ5XK5hpPCI8p/fxud8O6d5UR6CMroO0NsHCiMte7RX8lBb2qGnaAYR/Bsmeg82gY/mnFldmFqMPsLqS2fft2Bg4cWGL/gAED2LFjh1MaJURtYzJbeGlRHCazhSGdwrm+YyNXN0kIIUQl+drW6nZgpNs6st1yQEHarwUS9zuvcaJus1Yu9w0Dz0oWFCivkJo1tbxxl7KX+PL0gx4Tle1Nn8M/b0PCHvAKhFunKyPfzhA7AV46D7d+LQG3qPfs/l+Vk5ODVluy0qCHhwdZWVlOaZQQtc2crWfYc+4yfp4evH5LJ1c3RwghhB18PKsw0n1xr/LYOKZwZFFSzEVl2ZtaDoXp5aUVUisvtbyo3g+DRqcsd7fxU2XfLV9AgJPrPHh4Ovd6QtRSdgfd7dq1Y8WKFSX2r1ixgjZt2jilUULUJhfT83h/pbJ2/eQhUYQHyKe5QghRm/holZHu7Kqkl4fHKIE3QMJeJ7VM1Hn2Vi4H8LcG3RfBfMXPbFmVy0tcIxxibiv8vts90PGWyrdBCGEXu+d0P/bYYzz55JOcO3eOq666CoDNmzezbNkymc8t6qX/W7KfrHwjXZsFMr53c1c3RwghhJ18PAvSy/PtTC/PvQyXzyjb4dGQd1nZlgrmorLsrVwO4NcQVBqwmCDrUmG6uT4HLhZMbahopBuUudtxCyGoBQyRwotCVCe7g+5Bgwbx5Zdf8vXXX/Pnn3/i6elJ+/bt+eGHHwgICKiONgrhtv46cJE/DyTioVYxdVQ0GrUs/yWEELWNj21Ot50j3da52wHNwCe4ML380iEwGZRK0aLqLuwEn1AIqmMfbOemFc7BtifoVmuUkeqMC8q8bmvQHb9bCcT9GxcWWytPWDt4eh/ofJUvIUS1cWjJsGuuuYZrrrkGgKysLJYtW8Z7773HgQMHOHTokDPbJ4Tbyso38n9LDgDw4IBWdGjcwMUtEkII4QhfW/VyO0e6i6aWAwQ2B88GkJ8BSUcgvLMTW1lPJR6Eb69X5hpP2qMEnHXB4eVKZe+sRKXSeEXp4Fdq0KQg6L4AFCzvVXQ+t6qSgwB+snSpEDXB4fKE27dv54UXXqB///7MmjWLq666il9++cWZbRPCrX345xES0vOIDPbhqevauro5QgghHOTj6JJh1jRy61xutVpJMy/6nKiag78ro7eXz8LZLa5uTdVlJ8OC+2HenUrAHdoO7l2mpHjbo7RiapUtoiaEqHF2jXQnJSWxePFiFixYQFZWFjfddBN6vZ4vv/xSiqiJemXvucv8sPk0AG/f2hkvbR355F0IIeohh9PLrVXKi66HHB4DZzZKBXNnObyscPvgb9Diapc1pUosFohbAH9MVtbRVmng6qdg4AuOLadlLaZmXavbYpGgWwg3Vumg+5FHHmH79u1cc801vPzyy/Tv3x+NRsO8efOqs31CuJ2sfCP/WbAXiwVGdm1C/7Zhrm6SEEKIKrAWUsu2p5CaMR+SDivb1tHtotsJMtJdZamniq95fvB3GPJurUsx98hLQT3/LjhasPpPo84wYho0iXX8otaR7oyCke70c8rIudoDmnStUnuFEM5X6aD733//5a677mLcuHG0aNGiGpskhPsymy08PW8PRxOzCPP35JXhHV3dJCGEEFVkHenONdgx0n3pIJiN4B0EARGF+xsXWavbYqn83FpR0pGCILXZVZB0SAkqz26GFv1c2y57HPmDTmsfRmXIArUWBvwH+j0DHrqqXdcWdMcrj9ZR7vBo0HpX7dpCCKer9JzuuXPnkp2dzahRoxg7diyzZ88mNTW1OtsmhNv58K8jrD6UiM5DzTd3dSfUz9PVTRJCCFFF1jnddo10Fy2iVjSwDo0CjQ7y0wuXExOOOVSQWt7pVmh/s7J9YLHr2mOv0xtRL7wPD0MWlsax8PA6uOaFqgfcUCToLkgvPyep5UK4s0oH3V27duWtt95iw4YN3H777SxfvpwBAwZgNpvZuHEjWVlZDjdizpw5DBo0iOjoaMaOHcu+fWWnZC1atIioqKhiX9HR0WUeL4Sz/Lb7Al+tPQHAe6OjiY0McnGLhBBCOIOvI4XUrHO2G8cU3++hg7D2yrakmDsuKwnOFRROaz8MOo1Utg/+DmY75967QvIxmHcnKpOetPD+mO//Exp1ct71ixZSKzafu5fz7iGEcBq7q5f7+PgwZswYfv75Z5YsWcJ9993HjBkz6Nu3L4888ojdDVixYgVTp07l8ccfZ/HixbRv356JEyeSkpJS5jl+fn5s2LDB9vXPP//YfV8h7LHn3GUmL1TePD0ysDW3xkZUcIYQQojawtuRQmoXr1gurChbirkE3Q47uhIsZmjcBQKbQcuB4BUI2UlKoTp3lpUEc8ZA3mUsTXtwqtvLylxrZ/IvWJvbmAeZFwt/1iJ6OPc+QgincHjJMIBWrVoxefJk1q1bx8cff+zQNWbNmsVtt93G6NGjadOmDVOmTMHLy4uFCxeWeY5KpSIsLMz2FRoa6uhLEKJCF9PzeOjHHeiNZgZ3aMh/boxydZOEEEI4ka+nNeiuZHq52QQXCwp8lRZ0h3dRHqWCueOsVcvbD1cePXTQoWDbnVPM9Tnw8x2QdhqCWmC+fS4WTTVMRfPwBJ+C979H/wCTXvne3qXHhBA1wikfu2k0GgYPHszgwYPtOk+v13PgwAEefvhh2z61Wk3fvn3ZvXt3mefl5ORw7bXXYjab6dixI88++yxt29q3TrLJ5JzUJOt1nHW9+qQ29F2ewcSDP27nUmY+bRv68dHYGLCYcXWTa0PfuSvpO8dJ3znO3r6TPq5ZPlo708tTT4IhGzy8IbSU9x9Swbxq8rPgREEWY/thhfs73Qq7Z8OhpXDTB6Bx8uhxVZlNsOhBuLBDKbA3fiH4hgLnq+d+DRpDTrKScg/QrJcU7hPCTbn0t1VaWhomk4mQkJBi+0NCQjh58mSp57Rs2ZJ33nmHqKgoMjMz+e6777jjjjtYvnw54eHhlb53XJxzP3129vXqE3ftO4vFwidb04m7kIe/TsXT3b04fmh/xSfWIHftu9pA+s5x0neOk75zTz6ehenlZrMFtbqCwMWaytuoU+nLV4V3BlSQGQ/ZyQWBl6i0E3+DKR+CWkLDIquEtByoBLPWFPNWA13XxtKsek0Zodfo4I65ENqGav2UvkFTJZvi1Hrle0ktF8JtudlHhBWLjY0lNja22PdDhw5l3rx5PP3005W+TnR0NBpN1dd5NJlMxMXFOe169Ym7992X/5xg47lEPNQqpt/dk94tg13dJBt37zt3Jn3nOOk7x9nbd9bjRc2wFlIDyDOabNXMy2SrXF5GIVdPfwhuBaknlAC99SAntbSeOLxceWw/rPjIrUYLHW6GXT8qKebuFHRv/QY2T1O2R/4Pmvet/ntai6lZCgJ7qVwuhNtyadAdFBSERqMpUTQtJSWl0vO0tVotHTp04OzZs3bdW6PROPVNo7OvV5+4Y9/9eeAiH68+BsAbIzrTt02Yi1tUOnfsu9pC+s5x0neOk75zT15aNR5qFUazhR83n+HhAa1QlZemax3pvrJyeVHh0UrQnSBBt11MBqWIGhTO5y6q061K0H1oCQz90D1SzA+vgJUvKNvXvQbRY2rmvv5NCrdVamjSrWbuK4SwW5UKqVWVTqejU6dObN682bbPbDazefPmYqPZ5TGZTBw9epSwMPcMikTtcyghg2d+2QPAPX2ac2fvSNc2SAghRLVSqVRM7N8SgHf/OMxz8/eSZygjLdhiKTLS3aXsi9oqmEvGgl1Ob4C8dKUoWLNSlr9qMQC8gyEnBU6vr/n2FWU2KfOpF05UKq13uwf6PVtz929QJOhu2Ak8/Wru3kIIu7g06Aa47777mD9/PosXL+bEiRO8/vrr5ObmMmrUKAAmT57MRx99ZDt+2rRpbNiwgXPnznHgwAH+85//EB8fz9ixY131EkQdkpyVzwM/7CBHb6Jfm1BeHd6x4pOEEELUei8Oac/rN3dEo1axaPcFbp++mYvpeSUPzLyoFK9SqaFROX8jwmvRsmH6bPf5cMCWWj609PnyGg/oeIuy7aoq5tkpsOET+KwrzL8bDDnQ+joY9lHNFjJr0LhwW+ZzC+HWXJ6TM3ToUFJTU/n8889JSkqiQ4cOfPvtt7b08oSEBNTqws8GMjIyePXVV0lKSiIgIIBOnToxb9482rRp46qXIOoIvdHMo7N3cuFyLi1CfJh2ZyweGpd/LiWEEKIGqFQq7r26Je0a+fPY3F3sPZ/OzdM2MP2u7nSLDCo80BpEh7YDrXfZF7QG3cnHlKBW51t9ja+qX++DY38q6dq9HnRdO8zmIkF3KanlVh1Hws7vlSrmwz5S5no7ymSAs1tA56MUbvMOKjtwvrALts2A/QuVQm+gHN/tbhgwuWrtcESDpoXbpWUFCCHchsuDboAJEyYwYcKEUp/76aefin3/8ssv8/LLL9dEs0Q9YrFYeOW3OLafTsPfy4Nv7+lJoI/O1c0SQghRw/q2CWXJ4/148McdHEnM5I7pW3jr1s7c1qOZcoAttbyc+dwA/o3ArxFkJULiQWjmpkWuTm9QAm6AP/+rFABr1Mk1bUnYrVR81/kplcrL0qI/+IQUppg7Omf+7BZY9gxcOli4z7MBBDVX1ru2fqk0sPsnuLCz8LjGXaDXQ9B5dPkfvlSnounlUkRNCLfmFkG3EK723cbTzN9xHrUKvhgXS5uGMi9KCCHqq8gQHxY91pdn5+/hzwOJTF6wj0MJGfx3aAc8Lu5VDiqviJpVeDQcT4SLe90z6LZY4O83lW2tj5ImvWAiPPSPawJJ6yh3m8Gg9Sr7OI0HdLgFds5SUsztDbqzU2D1a8qa3wCeAcpId2YC5GcoqfalpdtrdEoht54PKuncrl4T29Mf+j4JhlwIkYxPIdyZBN2i3lt3NIm3lyufcr88tAPXRDV0cYuEEEK4mq+nB/8b353P1xzj09XHmLXxNMcvZfFDZpxSEKeikW7rMcdXF46Ou5vjq+HcFvDwgol/wU+jIOkQ/PUqDPuw5ttzaJnyWF5quVWnW5Wg+9BSGPZx5VK7zWbYM0dZTzs3VdkXexdc/wb4BCvB6+WzkHa6+FdOCrS9QSmU5udmhXtveMvVLRBCVIIE3aJeO5GUxRNzd2G2wNjuEUzs19LVTRJCCOEm1GoVTw9uR/twf56dv5c9x86i9jqtPFnWGt1FuXMFc7MZ/n5D2e71kPJ6bv0fzB4N22coo8fth9Zce5KPQfIRUHtA2+srPr751eAbBtlJcGqdMjpensQDsOxZ5UMGUKp9D/8YIq8qPEbrDWFRypcQQjiRVIkS9VZ6joEHfthBZp6RHs2DeOvWzuWvyyqEEKJeGtK5MbMf6E0Pr/MAXFKHkY5/xSdaR8MvHQSTsRpb6IBDS5SicDp/6PeMsq/NYOjzhLL9++OQkVBz7bGmlrfoD96BFR9vTTGH8quYp55S5qp/3V8JuLW+yujww+uKB9xCCFGNJOgW9ZLRZObxubs4lZxN00Bvvr6rO54epSxNIoQQQgDdIoN45yozAHsMkYybsYXUbH35JwW1VIJaYx4kH62BVlaS2QT/vK1s931CSa22uu415cOC3FRY/LAyIl4TrEF3h0qkllt1ulV5PLQMjEX+LdJOw4ZPYfpA+LwrbJ4GFhN0uBme2KbMg67pSuNCiHpN0stFvfTp6mNsOJ6Mt1bDjLt7EOrn6eomCSGEcHONc48DcErbmoMJGdzxzWbmPHAVYf5l/A1RqyG8M5zdrKSYl7eud03a94vyIYB3MFz1WPHnPDxhzHcwfYCStr3pc+jzpP33MJsg5YQymn4xTilC1u5GaNJN6ZeiMi/C+W3KdpQdKe3N+4JvQ8i+BHvnQl66Muodv7vwGJVaGT3v8wS0u8H+1yGEEE4gQbeody5l5DFj/UkA3h8TQ8cmDVzcIiGEELVCwRrdt9w4hJmrPTmamMXt32xm7gNXER5QRrXt8OiCoHsfdLnd+W0yGeDg73BiDfR8AJp2K/94ox7WTlW2+z0DXqX8DQxtC0PehaWTYM2b0LwfUM70K0OuMmc6YW9h5e/EA2DMLX7cv++DXzhEDYGoYdBygFKl/MgK5fmm3Ysvg1URtQY6jlDmoC99qnC/Sg0t+ikj4e1vdr/iZ0KIekeCblHvfLX2BPlGM90iAxke09jVzRFCCFEbGPMh6TAAjaN6Mb91MOO/3crJpGxum76ZuQ/2JiLIp+R5BfO6c87sIu5kCpl5RjLzDWTkGsnMM5CZZyQjz0h2vpGG/p60aehn+wr00ZXdnpxU2PUDbJsBGReUffsXwogvIXpM2eft+kGp0O0XDr0eLPu4bnfDib/h4O+oFz+Iuvfnhc9lJirzo89tU9a6TtgLZkPJa2h9lDW/G3WGvMtwbDVkXYSd3ytfWl9oc50y7xoqV7X8Sl3HwY6Zynbzq5VAu8PN4CcrkQgh3IcE3aJeSUjPZe62swA8d0OUFE4TQghROZcOgtkI3kEQEEELlYpfHr6KO2ds5WxqDrdP38LkIVEkZ+k5n5bDhbRcLlzOxS8th18A/YW93P7NZsodMb5CqJ+O1mFKAN46zI9AHy2huWdofeonwk8tRmPKA8DkE4YlsAUe8dth4UTlw4FrXi6Zxq3PgX8/ULYH/qf8tbhVKrj5Mzi/E1XqSVrteAPV+ZZwfqsyZ/pKvmHKBwzh0QVfMRDSWhmNtjLmw+n1cHgFHPkDMuOVgm5WjgTdTbvDEzuUNasl0BZCuCkJukW98tU/J9AbzfRqGUzf1iGubo4QQojawrrsV3iMEpACEUE+zH+4D3fO2MLJ5GyemrenxGk6GmHw1BCoyqZPcA45Pk3w99Li7+WBv5cHDby0+Htp8dFpiE/P5filLE5cyiI+PY/kLD3JWansOnWJPuoD3KdZyQDNXtu1D5qbM9N4E0vz+mBM1fCK53zuVy2Bfz/g+IEdHLv6Q5qHN6RlqC/eOg2mrd+gyUok3y+CPzXXkbDuBAnpecRfzkVvMtOvTShDOocXjth7B8HoGVi+H0ZA0jZIKph3jQoadoTI3tCs4Cuoha1fyuThqVRIbzMYhn2kzL0+skJZLzw8GsLaOfZvE9LasfOEEKKGSNAt6o0Ll3OZt10Z5X72+nYyyi2EEKLyEpT53Feuzx0e4MW8h6/itd8OkJCRR0SgN02DvGka6E1EkLKtXtQBLu3n55t9oEO/iu+lzyH31BYyDq9DdW4TQal70ZrzATCjYqu2N/M9hrPV1IFcoxm1wYTZYOaN/Ds4oG7CO9pvaZPyD/rfRvGA/jniCaWZt54l5g8IUsHLqcNZOP9giduuPZLEW8sPEd00gCGdwxnSOZzWzftiGfohGdvn4d+uP+rmfSGiR+WW9SqPSqXMP2/aDQa9UrVrCSGEm5OgW9Qb09Ycx2Cy0Ld1CFe1klFuIYQQdigookbjLiWeaujvxdd3dS/73MYxcGm/MlpuXRLLZIDcy8pc59w0yLoE57fDmU0Qvxtvs4Fiyd8+IdB5DOreD9MnpDV9rrhFvtHE2ZQcTiR1Z/mJXly/7zk6coYlXq/yUP4zDDDsI8gji+OWpuwMuJ5egb40CfCicaA3TQK80Jss/HXgIttPpxJ3IZ24C+l88OcR2jXy44aOVxPeuiPdOnbAz1OHl0GNl8qAt1aDViOrzwohREUk6Bb1wrnUHH7dcQ6AZ653MH1NCCFE3WY2KcFvRjxknFce0wse4/coxxQURrNL4xhlSaut/4M9c5UgW59Z/jn+TaDF1cqyWJF9ISyq3PRtTw8NbRv507aRP3QeDQN7ws93EpoYx0Kfd7CoNGCEVmPfYW3nwaVeY2K/liRn5bPqYCJ/7L/IpuPJHE3M4mhilnLAPxtLnKNRq/DWaujYuAHT7oylYYMyqrgLIUQ9JkG3qBc+//sYRrOF/m1D6dki2NXNEUIIUcNU/75P9NYZqP/xAIsZLBbAUvxRn6UUSyuLb0MIaWP/zZv3VR7z0pWvorwClLnT3kFKpe/mBYF2YPOK50iXJzAS7l8Jix9GdXiZUr6tcRfUHW8p97RQP0/G9YpkXK9I0nMNrDmcyIp9Cew9k4JJpSHPYCLXYMJsUY43mS1k5RvZdjqV+3/Yzi8P9cHXU95eCiFEUfJbUdR5p5KzWbRbWU7lWRnlFkKIekl1biu6vJRKHKgG/8bQoKmyZnSDphBQsN3sKtA48NapcRd44G9lhNsaYHsHKQF30erezubpB7f9BOvegwOLYOiHJSualyPAW8utsRHcEtOYPXv20LVrVzQaDRaLBYPJQq7BRJ7BREJ6HhO/387+Cxk8MXcXM+7ugYeknQshhI0E3aLO++LvY5jMFga1b0hsZJCrmyOEEMIFzHfM48iGJUS1b49GowFUBSPJRR51vuDXyLHAuiIRPZx/zcpQq+Hal5QvJ1GpVOg8VOg81AR4a2nUwItv7+nBuBlb+OdIEq8tOcDbIztLwVIhhCggQbeo045fyuK3Pcoo9zODZZRbCCHqLY2W3IDWSgq3phpHl+up2MggPrsjlkdm72Tu1rM0C/Lh0WtkKS8hhACQ3B9Rp33+9zHMFri+YyOiIwJc3RwhhBCizrqxUzivDusIwHsrD7Nkb3ylzrNYLFgslupsmhBCuJSMdIs662hiJkv3KX/wnx7c1sWtEUIIIeq++/u15FxaDrM2nub5+XsJb+BFr5alFzA9m5LDz9vPsmDneS7n6Anz8yTM35Mwf6+CR08aFjxe1TKEAB9tDb8aIYRwDgm6RZ312epjWCxwU+dwOjWRUW4hhBCiJrwyrCPxl3P580AiD/64g0WP9aV1mB8ABpOZ1QcTmbvtLOuPJRc7Lz49j/j0PCC9xDVD/Tz5+cHeypJoQghRy0jQLeqkQwkZLI9LQKWCp2UutxBCCFFjNGoVn94ey7gZW9hz7jL3ztrGtHHd+OvgRebvOE9SZj6g1K/r3zaMO3s1IzoikKTMfJIy87mUmVfwqHx/MD6DC5dzueObLcx98CqiwiXwFkLULhJ0izrHaDLz6m/7ARge00T+OAshhBA1zFun4dt7ejDqq02cTc1hxJcbbc+F+nlyW48IxvWKpFmwj21/00DvUq+Vlq1nwsytHIjPYNyMLcye2JuOTRpU+2sQQghnkUJqos6Z9s9xdpxJw9/Tg8k3Rrm6OUIIUSds376dRx55hH79+hEVFcXq1asrPGfr1q3ceuutdO7cmeuvv55FixbVQEuFuwj18+T7+3oS7KsDoH/bUP43vhubXxrE5CHtiwXc5Qny1TH3gauIiQggNVvPnd9uYf+FkinoQgjhriToFnXKzjOpfP73MQDeurVzpf+gCyGEKF9OTg5RUVH83//9X6WOP3fuHA8//DC9e/fm999/55577uGVV15h/fr11dxS4U5ahfnx97MD2fLSdfw0sTc3RTdGq7H/7WeAj5afJvamS7NALucYuHPGFvadv+z8BgshRDWQ9HJRZ2TkGXhq3h7MFrg1tikjujZ1dZOEEKLOGDhwIAMHDqz08fPmzSMiIoIXX3wRgNatW7Nz506+//57+vfvX13NFG4oqGCku6oCvLX8NLEX9363jV1nLzP+2638NLE3XZsFOuX6QghRXWSkW9QZr/22n/NpuTQL9uaNEZ1c3RwhhKjX9uzZQ58+fYrt69evH3v27HFNg0Sd0MBLy48Te9OzRRCZeUbu+nYrO8+kubpZQghRLhnpFnXCb7sv8NueeFvFVH8vWctTCCFcKTk5mdDQ0GL7QkNDycrKIi8vDy8vr0pfy2QyVbk91ms441r1jbv1nbeHipl3d+eBH3ey7XQad8/cygdjormufUM8HEhdr07u1ne1ifSd46Tvqsae/qtsH0vQLWq9c6k5vFJQrXzSoLZ0bx7k4hYJIYRwpri4OLe8Vn3jbn33VKyWqdk69ifpeWzuHgI91fSL9GJgc29aBnqgUqlc3UQbd+u72kT6znHSd1XjzP6ToFvUakaTmafm7SYr30iP5kE8fm1rVzdJCCEEyqh2cnJysX3Jycn4+fnZNcoNEB0djUajqVJ7TCYTcXFxTrlWfePOfTcvxsQnfx9j8e54UrP1LDuWw7JjObRt6MfIrk24pUtjmpSxFFlNcOe+c3fSd46Tvqsae/rPemxFJOgWtdoXa46z6+xl/D09+OT2rm6XViaEEPVV165d+ffff4vt27RpE127drX7WhqNxmlvHJ15rfrGHfvOz1vDq8M78eJNHfj3aBKLdl9g1cFEjl3K4oO/jvLhqqP0aRXC0OjGDGrf0GUBuDv2XW0hfec46buqcWb/SdAtaq0dp1P5Yo0sDyaEEDUhOzubs2fP2r4/f/48hw4dIiAggCZNmvDRRx+RmJjI+++/D8Add9zBnDlzeP/99xk9ejRbtmzhjz/+YPr06a56CaIO02rUXNehEdd1aERGnoE/4hJYtOsCW0+lsulECptOpADQsXEDBndoyKAOjYhpGoBa7T4p6EKIukuCblHrWCwWjiZm8fQvsjyYEELUlP3793P33Xfbvp86dSoAt956K++++y5JSUkkJCTYnm/WrBnTp09n6tSp/Pjjj4SHh/PWW2/JcmGi2jXw0nJ7z0hu7xnJ+bQclu5N4O9Diew6m8bBhAwOJmTw+ZrjhPl7MiiqIQOjwvDSqsnMM9q+svINymOeEVRw/9Ut6dw0wNUvTQhRS0nQLWqFhPRcNh5PYePxZDYcTyYpMx9AlgcTQoga0rt3b44cOVLm8++++26p5/z222/V2CohyhcR5MOj17Tm0Wtak5qt55/Dl/j7cCL/HlXeS/yy4xy/7DhX4XX+iLvIJ7d3ZUjn8BpotRCirpGgW7gls9nC2qOXWHckiQ3HkzmRlF3seU8PNb1bhfDfoR1keTAhhBBCVCjYV8fo7hGM7h6B3mhm26lUVh9KZNupVDw0Kvw8PfD38sDPU4u/l3Xbg/XHlA/8H52zkxeGtOfhAa3cqjK6EML9SdAt3I7RZOaFhXEs3HXetk+tguiIQPq1CeHqNqF0iwzCSyuFIYQQQghhP52Hmn5tQ+nXNrTCYyf2a8mUpQf5acsZ3v3jMCeTsnhrZDQ6DyneKoSoHAm6hVvJN5p4et4e/th/EY1axR09mzGgXRhXtQohwFtGtIUQQghRszw0at4c2ZnWYb68sewg83ec52xqDl9P6E6gj87VzRNC1AISdAu3kas38cjsnaw7moROo+aLO2O5sZPMnRJCCCGE6917dUuah/jyxNxdbDmZyq1fbeK7e3vSMtTX1U0TQrg5yYsRbiEzz8A9321j3dEkvLUaZt7bQwJuIYQQQriVa9s3ZOFjfWka6M2p5GxGfrmRzQXLkQkhRFkk6BYul5ajZ/y3W9l2OhV/Tw9+mtiL/m3DXN0sIYQQQogS2oc3YPHjfenaLJD0XAN3f7eVGf+exGy2uLppQgg3JUG3cKm0XBPjZmxj3/l0gn11/PzQVfRoEezqZgkhhBBClKmhvxfzHrqK4TGNMZgsvL3iEONmbOF8Wo6rmyaEcEMSdAuXOZ+Wwyv/pHLsUhaNGngy/+Gr6Nw0wNXNEkIIIYSokJdWwxfjYnl3VDQ+Og1bT6Uy5NP1LNh5HotFRr2FEIUk6BYucSkjj9tnbONitolmQd4seKQvbRr6u7pZQgghhBCVplKpuKNXJH881Z/uzYPIyjfy/K97eWT2TlKy8l3dPCGEm5CgW7jEtH+OczE9jyb+GuY92JtmwT6ubpIQQgghhEOah/gy/+E+/OfGKLQaFX8eSOTGT9ez5vClUo/P0Rs5lZzNlpMpHErIqOHWCiFqmlssGTZnzhxmzpxJUlIS7du359VXXyUmJqbC85YvX86zzz7Lddddx1dffVUDLRXOcCkjj3nbzwHwULcGhAd4ubhFQgghhBBVo1GrePzaNgxsF8Yzv+zh2P+3d+dxUVfrA8c/M8OwI8gmioIICiouuC+Ymbul5pLV1SyvbWZ2W/ypt7KbLS7d261MK0sztay8rrlkpaW5YGbuu4K7IJvKzsDM/P44DIiyDAgM6PN+vebFLN/5zpnDwHee73nOc+LTeGrJXjr5O+B36iDxqdlcSckiPiWb1OzcQs99pnsjJvcNQ6vV2Kj1QojKZPOR7g0bNjBjxgzGjx/PqlWrCAsLY+zYsSQllbz8wsWLF5k1axbt2rWropaKijLv9xgMuSbaBnoQ7mNv6+ZUT8c3wIU/bd0KIYQQQpRRuL87aydE8mRkEAC7LmWzev9ldkYnEZ2Qnh9wO9vrCPRSmX7ztsYwfulesnKMNmu3EKLy2Hyke+HChYwYMYJhw4YBMG3aNLZs2cKKFSt4+umni3yO0Whk4sSJTJgwgb/++ouUFEnLqSkS07L55o9zADzfIxhN+iUbt6gaunoWvvsbOLjBxJOgd7J1i4QQQghRBo56Ha8/0IzeTX359vfDhAbVp667E761HKhTy5E6tRxxdVBfw1ftu8ik5Qf58XAcsdd38cXodvi4Odj4HQghKpJNR7oNBgNHjhyhS5cu+fdptVq6dOnCvn37in3e3Llz8fLy4qGHHqqKZooK9MW2GLJyTLSq7063EG9bN6d6unIEMEN2CkT/auvWCCGEEKKc2jWszWMt3Xi6WxAPRvjTJdibYB/X/IAbYEhEfb4e2xEPZz37L1xjyCc7OHUl1YatFkJUNJuOdF+9ehWj0YiXl1eh+728vIiJiSnyOXv27GH58uWsXr36tl7baKyY9B3Lfipqf3ey5HQDS6IKRrlNJhMgfXczTcLJ/LNhpiNrMDfud8s28rkrP+m78pO+K7+y9p30sRB3l46NvFg5rgt//+pPziZlMPTTnXw2qi1dZYBCiDuCzdPLyyItLY1Jkybx9ttv4+npeVv7OnToUAW1qnL2dydaejiVDIORIA87amde4tChy4D03c0CT+3Gcog1HVvHwYC/Y9bqi9xW+q78pO/KT/qu/KTvhBDFaeTjysrnuvL04j3sOXeVx7/czfQhLRjRvoGtmyaEuE02Dbpr166NTqe7pWhaUlIS3t63ntm7cOECly5dYty4cfn3WUZLmzVrxsaNGwkICLDqtVu0aIFOp7uN1itGo5FDhw5V2P7uVCmZOfz0w1YA/m9AOBHN/aTviqHdn5x/3S43nVa1rkNIr0LbSN+Vn/Rd+UnflV9Z+86yvRDi7uLpYs/XT3Zk0vKD/HDgMpNWHCQ6MY2XejXBUS//d4WoqWwadNvb29O8eXOioqLo1UsFFSaTiaioKEaNGnXL9o0aNWLt2rWF7vvwww9JT0/ntddew8/Pz+rX1ul0FfqlsaL3d6dZvCuGtOxcQuu40S+8XqElMaTvbpIcrX76t4NLe9AdXwuhfYvcVPqu/KTvyk/6rvyk74QQpXHU6/jw4dYEejnz8a+nmbc1hjX7LvNCz8Y81K4+ep3NFx8SQpSRzf9qx4wZw7Jly1i1ahXR0dG8+eabZGZmMnToUAAmTZrE+++/D4CDgwNNmjQpdKlVqxYuLi40adIEe3tZfqo6Ss3KYcF2NUf/+ftCZA3KkmReg/QEdT3yJfXz+How5hb7FCGEEELcWbRaDa/0CWXO3yKo5+5IXEoWr646RJ8PfueHA5cxmcy2bqIQogxsPqd7wIABJCcnM3v2bBISEmjatCnz58/PTy+PjY1Fq7X5uQFxGxZHnSMlK5dGPi4MaFHX1s2p3pJOq5+uftCkHzh5QmYynNsBjbrbtm1CCCGEqFIPtKxHr6Z1+OaP88z97TRnEtN54dt9fLYlmv/rG8q9oT5oNDKYIUR1Z/OgG2DUqFFFppMDLFmypMTnzpw5szKaJCpIenYuC7afAeD5HiHoZJS7ZJag27sx6Owg7H7YtwSO/SBBtxBCCHEXctTrGBsZxMPtG/Dl9jN88XsMR2NTGPPVn7RvWJsHI/zJyDZyLdPA9cwcrmfm5v3M4XqGAZMZnPQ6HO11ONppcbLX4WinUz/1OnqG+dKrWR1bv00h7mjVIugWd65v/jhHcrqBQC9nBrWqZ+vmVH+Jp9RPrxD1s9ngvKB7HfT/N0jWh21lXoP1r0CLhyD01qXchBBCiMri6mDHCz0b81inQD7dGs2inWf58+xV/jx79bb2++3u8zx9TyMm9wuTwREhKokE3aLSZOUY+fx3Nco9/t4Q7KTwR+mSbgq6g7qDgzukxcHF3RDQyXZtE7Dvazi8HC7shiZ9QVL6hBBCVLHaLva8OqApf+8axLzfozmbmI67k15dnO0LrudddFrINJjIyjGSmWMkK++SmWMkJiGd7/68wOe/x3AsNoU5j7bB3bnoZUqFEOUnQbeoNN/uPk9iWjb+Hk4MaeNv6+bUDEl5lcu9G6ufdvZqRPXg93D0Bwm6be3M7+rn9fNwcQ80aG/b9gghhLhr+bk78q+BzW97P5GNvfm//x1k26lEBs3dzhej29GkjlsFtFAIYSFDj6JSpGfnMm+rqlj+XI9gWd7CGiZTQdBtGekGaDpI/Tz2A5ilWqnNGHNUQTuLwyts15aaKjtNfc6FEEJUGw+0rMeKcV3w93DiXFIGQ+bu4KcjcbZulhB3FBnpFhXuekYOT3y1m7iULOq6OzK8bX1bN6lmSLkIuZmg1YNHYMH9IT1B7wLXL8DlveDf1nZtvJtd3geGtILbR1ZB33dBWzVrLptMJgwGQ5W8VnGMRiMAWVlZZV9r+uo5+O5vULsh9JsJHg0qvoHV2M19p9frZb1uIUS10axeLdZOiGT8N3uJiknimSV/8Y+ejflHz8ay1KsQFUCCblGhElKzeWzBHxyPS8XDWc+8x9riYCdfLK1iqVzuGaQql1vonaBxbzi6WqWYS9BtGzFb1c8m/eHcTjXP/nwUNIys9Jc2GAycOXMGk41Hic1mM3Z2dpw7d67sS9RkXYdO09X1s2fBOVV9tu8SRfWdh4cHfn5+styPEKJa8HSxZ/HYDkzfcIyFO87y0eZTHI1N4b8jWuHmKPO8hbgdEnSLCnPpWiaPzf+DmMR0fNwc+HpsR0L9bDQnKOEk7JoL3adArRqyNnhiXtDt1fjWx5oNUkH3sR+g15tV2SphcSYv6G7cC5y9YP/XcHhlpQfdZrOZ2NhYdDodDRo0QGvDCvZms5nMzEycnJzKHigmxYDRHjR2YM5V9zk7g4vvXVGQ7sa+A8jIyCA+Ph6AunVryP8oIcQdT6/T8q+BzWlWtxavrTrML0ev0HnGrwxo4cfwtg1o37C2nCgUohwk6BYV4mxiOiPn/8Gla5n4ezjxzZMdaejtUv4dGnNV8bCQXuBWxrUjzWZY9YxKxdY7Q78Z5W9HVcqvXB5862ON+4DOAZJj4MoR8GlatW272+VkqorloCrK126ogu6ja6D/e4UzEypYbm4uGRkZ1KtXD2dn50p7HWuYzWZMJhOOjo5l+9KVmw2abLDTQJ2mkHYF0hPAkAwYVH/q7uxRlJv7zhJ8x8fH4+vrK6nmQohq5aF2DQjxdeXlZQc4k5jOsj0XWbbnIgGezgxrU5+hbfxp4GnbY5IQNYlUtxK37URcKg/Ni+LStUwaebvwv2c7q4A7+QxkpZRvp7s+gTXPwfcjy1487NQvKuAGOLutfK9vC5b0cu8iRrod3NTcblCj3aVJOAmGjIpr242MuRB3SAVS5ZGeCFfPVmiTKt2FP8CYDW71VJG7oO5qtDsjEc7+XqkvbZkLbG9vX6mvU6myrquf9q4quHavrwJtjVbNk084oYqs3WUsJ1FycnJs3BIhhLhVREBtNr/cne+f7sSIdvVxsddxPjmDDzadpNt7v/HwvCj+t+cCWTlGWzdViGpPgm5xWw5cuMbDn0eRkJpN07q1+P7pTtSL3wYL74fZrWHJg2UPmk0m2LNAXb/4Jxxba/1zzWbYcsPIdtxhyEgu2+tbq6Ln15aUXg4FVcyPlhB052TB2n/A3PbwaReIPVixbQTY9C/4LBLeawTLRsOB70vv48RTsOMjWNAX/h0CH7VSz025XPHtqwyW+dxB96hUaJ2+4PdRRVXMa3Q6nyXodnQvuM+pNniHgp0jmHJUpkfalbuqQn+N/p0KIe4KWq2Gjo28eG94K/58vRcfPNyKyBBvNBr440wy/7f8IAM/3s6pK6ll3nemwcjuM8nkGmVVC3Hnk/RyUaQco4kNh2I5n5SBo16Ho70OJ70OR70WJ726npxhYPLyg6QbjLSt78qSDudw/noSxB8t2NGlv9Rc2Eb3Wv/iMb8VHgndPA1C+1uXfmoZ5bZzAhdvVfH7fBSE3W/965fm0l7YMFGNTLcdA53GgZvf7e0zJ1O1FQovF3aj0H6qsnnCMRXE3uzqWRXIxh7Iu30GFvSGAf+BNo/dXvss0hLgz/nquiFNpVcfXQMaHQR2gdABEDYA3BuodaxPrIfjGwpS5/Np1PNOb4b7Xof2T1VqivZts6zP3ah7wX3hw+Cvheqk0P0fqDXVxa1MuQVV328MugH0juDdRH32M6+qkzA5WeARUOQ87/vuu4/Ro0fzxBNPWPXSf/zxB6NHj+bPP/+kVq1at/lGhBDi7uZsb8eQiPoMiajP5WuZrNp3ia92nuVUfBqD5uxg+tBwhkRYt2LNjtOJ/HPlIc4nZ9CvuR9zR7ZBJ1XSxR2sGn/LFaU6+D84shIGzwVnzwrZpSHXxMq9F5m75TQXkjNL3d6FTN7y3c0owzq0Gy6pO+3doN0TkHoFDi2DXZ+VLeje86X62XoknPxJBbd7F0P7sSU/z2yGrTPV9Q5PqvTqPQvg7PaKCbqzrsOv78DuL4C80bgdH6pU+FaPQJcXik4Nt0ZyjNqno7s6WVAUp9oq6Du9Cc3xteB6X8FjJ3+ClU9D1jVw8oQH/gv7l8Kpn+GH5+HCLhV832616N3zIDcL6rWB+/+jAuoTG9SJlrPb1OWnf6rPgOGGs95aPQR1U0F5aH8VYK17SWUybJyi2vrAh1C/GlZmz7peMF0h6J6C+wO7gGsdNTob/as6KSIACA0NLfHx559/ngkTJqgbWp1aIs/eBa5fhMxkdV8t/1sC7+XLl+fPhbZGREQE27dvx83NRgUdhRDiDlXPw4nxPUJ4uH0DXvxuP9tPJ/LS9wfYfSaZfw1sjqO+6DoV1zIMvLv+GP/762L+fRuPxDF1zWHefTBcMoDEHUuC7poq6zqsfxmyU+Cvr6Dby7e1u+xcI8v2XOSzLdFcuqaCbW9Xe+4L8yXHaCbTYCQr16h+5hjJMhgZnvk9o4xrcErJG8VyraNGfduOAScPNRp7aBmc3KiCSs9GpTck5TKc+FFd7/IC1G0FP06CLTOh5cPg4Fr8c09vUiPrdk7Q5R8qANyz4PbndZvNKoX4p1dVgAXQ4iEVPP7xuQpo9y6GvUtUcN/1H9CgQ9lewzJy7dW45ErOTQepoPvYD9D+PjAZYct02PYf9bh/O3joK7UGctPBsP19+G067PtajYCPWGzd76Eo2Wl5JxyAyBfV0mX+baHnVDV//8SPKgA/t1MF3A7uaqmzsAGqIN6No5zu9eHvP8PeRbDpTYg7CPN7Qru/Q8831Oenuji3E8wm8AxW7bbQ6qD5EPjjM3XyS4LufNu3b8+/vmH518z+YhEbl3+dXxTxxoJwZrMZo9GInYuPypi4dk4VWdPa3ZJB4ulZtpOL9vb2+Pj43MY7EUIIURJvVwcW/b0DH/96io82n+Lb3RfYf+E6n4xsQ9ANBXXNZjPrD8Xy5g9HSEwzoNHA6E6BtKjvwf8tP8DSP87j7erAy72b2PDdCFF5JOiuqfYsVAE3qMJa5Qy6s3KMfLf7PJ9tjSEuJQsAHzcHnu0ezN86BOBkX0xF3d//A79+o657hagAueXDKl3UwrsxhPSG07+o4LT/zNIbtHcJmI0Q0AV8w1SAuOsTlTq96xPoPqno5904l7vDk+DqA4Fd1W3LvO7yZAMkRauTGzFb1G3PYLj/fQjuoW6HD4Pzu9R85RMb4Pg6dQnoAve9Zv1yUvmVy4tJLbcIux/WvYgm7iDOV4+hXTqtYCmrDk9Dn3cL0py1Wrjn/6B+e1g+VhU/m3cvDPm0fCP/exerkXTPYAh7oPBjnkHQ+Tl1yUhWvy+/FiVPCdBqod0Yta9fpsKBb9VJkmNrVcX5FsPL3sbKcON87ps1H6qC7uMb1BSB6r7udK5BzZ+2v42VBayQH+iaTbg5aNGgwad+ENi75Kd8f/7553z00UecPHmSBQsWULduXWbMmMGB/fvIzMigUaA/r7w4gS73Dcjf783p5aGhobzzzjts2bKF7du3U8fXh8lTptCzZy/g1vTylStXMn36dD744AOmT59OXFwcbdq0YcaMGfj6+qouys1l5syZrF69Gp1Ox/Dhw0lMTCQ1NZVPPvmkUvtNCCFqIp1Ww4u9mtAu0JN/fLePY7EpDPx4O7OGteT+lnWJvZ7J1NWH2XRMLZMY4uvKrGEtaBuovpdl5Rh5ffVhZm8+hY+rPY91bmjDdyNE5ZBCajVRbjbs+rTg9uV9cO18mXfz/Z/n6fbeb7y59ihxKVnUdXdk2qDmbJvUg7GRQcUH3Cd+VGnWAH2nw/g/oe3jhQNui07Pqp/7vi69krkxV418ghrxBBVA3jdVXd/xkap8XZRCo9wvqPvc6qj5opjVvO6yyMlSo+ufdFYBt84B7n0Vxu0sCLgtAjrBo9/C+N0QMUqlUp/fCUuGwvVL1r1eUrT66V1K0O3inX8yIWzHBDRntoLeBYYtgAH/LnpecaN74ZnfoX4HyL4O3/0NfvmX6m9rGXMgaq663vUFNcpbHGdP8G9j/RJQrj4w5DN4fJ36faXHw4qxhT/jtlTUfG6L+u3V/HVDqqonUEXMZjMZhtyyXbKyyYg9ri6JF8nIzinT883lKXCWnQaYQINavu8G77//Pq+88gobNmwgNDSUjIwMunfvzleLFrPqmwV069CKZ/8xicvRR4vctcWcOXPo3/MefljwH+5p15yJEydy7dq1YrfPysriyy+/5L333uPrr78mNjaWWbNm5T/+xRdfsHbtWmbMmMHSpUtJS0tj06ZNZX/vQghxl4ls7M2Gf3SjQ5Anadm5jF+6l2eW7KH3f39n07F49DoNL/ZqzPoXIvMDboBRnQJ5sZeanvfGD0dYd7CGFFkVogxkpLsmOvAdpMWp5Yvc68PF3Wp0sPN4q55uNpuZtfEEn21VgZ6/hxPP9QhmeNv6ONiVslZswglY8RRghvZPlv6awT1VIJV4Uo1kdnym+G1P/wIpl9RSTM0GFdzffCjs/Bhi98PW92DAeze/oYJR7vZjwdW34LGGkeq1yzqv+4fn4dD/8t7DfWo+dFHrZ9/IJ1TNr+/xGnw9TM1zjt4MbUaX/no3ppeXptlgOLsNjdmE2asxmoe/VlkBJXH3hyfWwy9vwB+fqrno6QnwoJUjd4eWQ8pFcPGFlo9Y95yyCuoGz+6AX9+GnbPVXG9HD2j9aOW8njXSEiD+iLresNutj2u10PxB9fk8srLw57aSmM1mhn8WxV/nrt7GXuLK/Ix2gbVZ9kynsj0p61reFc0t0yZeeOEFunbtmn/bw8ODsLC8z7HZzIsN/Ni07U9+3fgDo8b631qELc+QfvfyQCeVjvjyk4+wZOVGDu77i3t69Cxy+5ycHKZNm0ZAQAAAI0eOLDSC/fXXX/P000/Tu3dvAN544w1+/71yl4UTQog7RZ1ajix9siP//eUkn2yJ5qcjalpemwAPZg1rSeM6RdfY+EfPxiSlGViy6xwvfb+f2s72dAqqXZVNF6JSyUh3TWMyqoAEVMAbPkxdL2kZqRvkGk383/KD+QH3y72b8NvEexnZMbD0gDvzKnz7qBrVC+wK/axIF9doCgLtP+aVvMzWjQXU7BwK7tdqofe0gm2SYwo/78ZR7q7/KPyYJb27LPO6M5LhyCp1fcg8GLWy9ID7RrXqQdOB6nr0b6VvbzZbn14OED4Mc702JDboh2nsptIDbgs7e5XiP/xLtT7y/m9UMT5r2rfjI3W907iiMxoqip099H4LOj2nbq8Zr1K3rZV4Gr56AD5oAYsfhA3/p+ahR/+minSVdbTWsgZ3nRbFF7iz/A2e2Fhla03XiDIzZnNBdksRdQpatGhR6HZ6ejqzZs2if//+tGvfnoh7BxF9/jKXrySqmgE39q3ZBKmxAIQG+gEacPPD2cMXVxcnki9FF9ssJyen/IAbwNfXl6SkJABSU1NJTEykZcuW+Y/rdDqaN29e1ncvhBB3LTudlkn9wlj4RHs6NfLkrcHNWf5sl2IDblBLKL45qDkDWviRYzTz9OI9HLp0vQpbLUTlkpHumubEBlXN29FdpXRnpcDGyXDhD0iNK3HpqkyDkeeX7mXz8Xh0Wg0zhrZgRLsG1r2uyajmBSdHq3TaEYutTx9u9Shseks99/Qv0KTvrdtcPVeQntv2iVsfb3SvGjWP3qxS24fnBegljXIDBOYF3WWZ131srVrmqE4LVZW8PBr1gK2zVGq6yaROHBQnIylvHWONdcG9syemsZs4t38/tR3KUZU5fJgaWd8yQ1UQr99Ozckuzqmf1TJl9m4Faf+VSaNRc9Mzr8GBpfC/J2DUCjUSXpKja2D1+IKq6dfPq+XnbqR3QesVjJdvb2jduvS2lDSf26Jua6gdpJZoO7mx0ueiazQa/vdsZzJzjNY/KS0BUi+Dzl5lZGRez1uizqx+r7UDS54yADgVU4m2WDkZav44Woo6TXBzFfJZs2axc+dOJk+eTEBAAI6OjrzwwgvkmLWqnckxKtjOzYb442DMBkDv6KpOPNk5Qk4mGjSYDOlgSC+yWXZ2hQ97Go2mfKnzQgghStQjzJceYb6lb5hHp9XwwcOtuZr+J1ExSfx90V9M61aL1pXXRCGqjIx01yRmM2z/UF1v/yQ4uKm0Yf92gFkFi8W4lmFg1II/2Hw8Hgc7LfNGtbU+4AZVYTp6sxpNfmRp8aN+RbF3KVgnurh5unsXAWYVrBYXePZ6E9CoSuKX8pZwOr25+FFuKN+87sMr1M/wIdZtX5T67VQwk5kMcQdK3taSWu7eoOoKcXWbCA06qQB1xZNqznZxLJ+5dk9UXVVxrRYGfQyh96vg6ttHVe2Cohhz4KfX1BrlhlRVxG70Ghg0R30mQgeotH2tHeSko4k7SMOD76vPTWlKms9todEUjHYfXlm291lOGo0GZ3s76y52GpwNiTjrtTjXrouzgz3OHj441wnBWW+Hszkd59RzONtR4n7KvIxLVt4IhZWZEfv27WPIkCH07t2b0NBQvL29uXTpEjjUUv9DzEZVhyAjUX0mtHnBs5ufCrhB/f1Y2plyqcyZDW5ubnh7e3Po0KH8+4xGI0ePljyvXAghRMVwsNPx+ei2NK9Xi+R0A2//fpVdMUlyclTUeBJ01yTndsClPaqoV8dnC+63zCM9VnSKeez1TB7KmwNay9GOb57sSK9mdax/3YP/K0hpf3Au1G1Z8vZF6fC0SmmO+U2NUt3IaFBVy6HkkdS6LaHlCHV9079KH+W2yE8x31F6O9PiC1LRmw8tffvi6PQFI7OlpZjnp5aXIYX9dunsYNgXalmvS3tU0biiXNitisJp9QUp31XZxuFfqrnUhlQ1Tz7hZOFtUmJh0UCImqNud3kBHl+rMiPaPKZS1R/9Fibsgdfi4Pk9mJuqvxftxsklT3e4dl6NXmt0ENC55LaG531WTv9SEGxWFxlJKnNDZ18408OxlvrMaXSQk65O/pR08qWs8oNu55K3yxMYGMgvv/zCsWPHOH78OK+88gomk0kF0Z6N1Ik18r50uXiDb1N1/eaTAZq8kXVDuhptL6NRo0Yxb948Nm3aRExMDO+++y7Xr1+XtWOFEKKKuDnq+WpMBwI8nYnPMDJywZ/0/2gb3+0+T6ahDFleQlQjEnTXJJYRx4iRhQPMvCCCs9tvqe59Oj6VYZ/s5FR8Gn61HFk+rgvtGpZh6azL+1RRMYDIlwtG9MqqdqAacQTYPa/wYyc2qIrVrn5q7euS9HhNBQ9nflejm5f2FD/KbVGWed1H16gU1nptSk65tkajvCrnN6c43yzptPrpbUURtYrkEQCD8uZqb3sfzhTRP5a53K0eVnPVq5reUWVW1G2tgsclQ+DaBfXYmd9hXjeVweBQCx7+Gvq8rYL1ouj04N0YU9+ZGO2c0Vzeq+a1F8cyyu3fVgWoJfFtBt6h6gTS8fUlb5trKPnxkpjN6vNpLZOxYG151zp5AekNHFzV506rh9wsVXQwN7v87bPIzVIXNFaPdE+ZMoVatWrxyCOP8Oyzz9KtW7eCudRaO1XvQGunivm5NygY6S6KZdpFRjGrHZTgqaee4oEHHmDy5Mk88sgjODs7ExkZiYODQ+lPFkIIUSF83Bz49skO9GnkhJNex/G4VKasPETnmZuZ8eMxLl699aSq0WTm5JVUlu25wGurDnH/7G30/u9WPt58ivi8ZXGFsBWZ011TxB1Wo2gaLXR+vvBjnkHg1xLiDqov/G0fByAqOolx3/zFtYwcGvm4sGRsR/w9ypC+nBYP341UX54b94X7Xr+999BpnFrD+sB30PMNsFeBjPavr9TjbUaXPk+8diC0fwp2zVUXKHmUG26Y131IFYNzKqEapiU9uLwnF24UfJ/6eX4XGDLAvpgRv8S8oNuayuUVrfkQlaK/bwmsfBrG7SgYDU04WRBAWpZhswXHWmpO98L+Kihc8iCED4ff31MBaJ1wVWPA2kwBNz8uNxlNg6OfqWkTTQcWnTZvzXxuC0uK+Zbp6jPU+m8Fj5nN6uTViQ1qub0rh1WQHjoAwgZA3YiS5/wbc9TfYGocXE9Xt2sHlvw5tihulPtGeicVeCedVicNEk+qAPd2pjpYRrntXRg67CGGDnso/6GOHTty4sSJW55Sv359Fi9eXOi+kSNHFtzQ2fHrlq2FHi9qP3v27FHv+cpROrZozIl9O8FZ/a8ZOnQoQ4cWzmDp1atXof3Y2dkxdepUpk5VSxWaTCb69+9P//6lnBAUQghRofzcHXmmrTsz/9acFXsvsyjqLBevZjJvawxf/B5D72Z16NPMj1PxaRy4cI1Dl66Tln3rcqjv/3KSDzefoldTX/7WMZBuId5otZK9JKqWBN01hSW9u+mgooOLZoNU0H3sB/70GsjszafYdkqN8rRu4MGXT7TH06WINZyLkhStgoO9i9W8SK/GKhW5lEJLpQrsqoqTXTmk9t3peRzSzqM5+7s6mWDN0loA90xUQWJ2Sumj3FAwrzvxJJyLUoFOUa5fUqnUoILR2+UVrEbkrl+Aczuhca+it7NFevmN+s9So8VJp+GHCWrEWKOBnR8BZjWv2ifUNm2zcPGGx1bBgr6qnVvz0uFbj1TLuRV3QqMYCUFDqH9lM5qkUyq1vv9N6fVms3XzuW8UPlQF3TG/qQD5ymFVef3Ej6qI2Y3ij6rLtv+AW12V4RF6v5qSYOegCiRGb1bPv3QY2r8JmWawy/uScPWcSgsvaQTeZCp5lPtGdg7qbyQpGnIzVaq5ZyM1El4elqDb0aN8z79dWjv1d59yWU1BcKxd8omNG1y6dIkdO3bQvn17DAYD33zzDZcuXmTgPW1Vv3g0KJhDLoQQotK5O+l56p5G/D0yiF+Px/PVzjPsOJ3ET0eu5C9JZuGk19GivjutG3jQuoEHmQYj3+4+z55zV/O3b+DpxCPtAxjRrgE+bpLFJKqGBN22dH5X3qjYo1AvAlBr8F6+nsXRyykcuXyd+NRsunhlcP+h5ar+b+SLRe7K3HQQml/fIff0FsYe3kQKLthpNQxvW583BjbD2b6EX7XJBJf3qlHNExsg4YY5144eak5sMWvkloll+bAfnlfLOHV4Fp9zeSOpjfuqL7PWcPaEe6fAT6+qZdNKGuW2KLRedzFB99HV6mdAZ1Wg7nZpNGpu8b4lKhArKug25qrlkKDq08st7F1g2AKY30tlIvy1EJr0hwPfq8eL+cxVOff6MHo1LByggroB/1Ynasox19as1WPqNxPdN8Ng9+dqP3WaFWyQeBLS4lRwVb+DdTv1bgx+LVRGxQfN1Wirhd4FQu5TgXVAJzVX/sR6lWWQGquWwtvzpSq+V6eZGhk35qWhuzZQAbaDO9TyUtkaWdfUfHOvEPX7K0pGYumj3DfS6cE7BJJi1BzvpGjwbFj2v31jbkHl8NLS8iuTs4+abmM0qDXp3ayrY6HValm5ciWzZs3CbDbTJCiAhf95jWA/VzCk5Z2QCC7ziR4hhBC3R6fV0LtZHXo3q8PJK6ks2nmWI5dTCPNzo3UDD1o18KCxryt2usInWYe1rc+JuFS+3X2eFXsvciE5k3//dIIPfjlJ72Z1GN62Pvc08UGvs+7krBDlIUF3JTp6OYW5W05jNJqp7aLHw9keT2d7fO3SaHfqI/zPLAfAvPsL/vQdxjzdo/x1xci1jMLFjBrbLUJjZ+QvXUtW7bYjMiSWzo28cXfWYzab2XE6idmbk3nH5E8T7SX62O1D3+ZvPHdvMA08S/hiGHcI/pyv1hdOiyu4X6ODhl1VgND8wRKXISuzFg+pImjXL6A5shqvCz+p+9uNKdt+Oo+HJv3UUk3WaBipgpqS5nXnVy2vgNRyi+AeKugurpjatXNqWSU7R6hVv+Jet6zqtVbV4X9+DTb+E6J/Ve0K6AwNrAw6q4J3Y1UUzZhTtgr6RWnUA8IeUCcafpykCrBZAnjLKHeDjmVbl7zlw+rvypSrRpfzR7DvKbwfzyA1Tz4nS30mj69XI+JpcWr5P1CBXdgACL4fst3ViSBHRxXIJhshO1UFxt6Nb00FL8so9420dirj4upZlUmSfEbN/bcmaLfIzhvltnNSI+i2otWq/13Xzqu+cPYqfr7/Der6+fHdws/Uc24sxObooea752aqbIvbyQQQQghxW5rUcePdIS2s3j7Uz403BzVncr8w1h28zNLd59l3/ho/Ho7jx8NxeLnYM7BVPYa1qU+4fy0pnikqnATdleTAhWs8tuAPUrIKRrs0mBiu+51hdkvx1KQBsM8UQoT2NB3i/0eg+RfeznmMjdrOhPi60byeOw0cMnl03xYAPswcwLZd5/l613m0Gmjh7w4aDQcuXAPgZ31HmrCSt5vE4DS0lH9EiadVqm5O3oiUvZsaiQ29X/20Zr5oeegdoe0Y2PYfNBtexi4nDbN7fTQhxaRel6Qs6dilzeu+elYtIaXRQrPBZW9LcYLuBTQQf6ToddQtRdQ8g61Of600nZ5TwXb05oLl57q+aNMmFakisi4s+k6H05tU4Ht0dcG0gpgt6qc187lv1HGcKgjoGaSK8ZX2O9U7QuPe6nL/f9UId/wRFex7N1EnAbKy4MyZgudotOpkU1J0wYi0d+PCAW5ZR7lvpNWp9l87r/5Wrp1Ty3W5+Fj3fEtquVMF/p7Ky8lTrVGem6mC6JIyWExG9X7T4vPXAAeNCtZdfVX/mvIyUwxpeYF3UMV+HoUQQlQqJ3sdD7VrwEPtGnD0cgor9l5kzf5LJKYZ+GrnWb7aeZbGvq4MbVOfByPqUde9ipZyFXc8Cborwd7zV3l8wW5Ss3NpE+DBkAh/NPHH6HZyFoHpas3ms7qGzNI9zW5jEwbXOslzGZ9SJ/sCc+w/xtToMNr731dB5ZZZQDbGOi0Yfc8YgqOT2H46kdPxaRy4qL7cOthp+VvHAB5p+jx8vRKn81sgO634UZjcbFjxd/WFvX57lardsFvVjUq1fxJ2fIjGoE48mCMeR3O788VL41ZHzU1POlX0vG5LAbWG3axLV7eWixfUbQWx+1Ug1+qRwo/nVy4PqbjXLC+tFh78FD7rqtJxfZpC4z62blXlqh2oTixsnQk/va7er52jmoYAanpAWejsoOVDpW9XFK0W6rdVl1K31amR1qRTqsha0mkVpOv05R/lvpFGCx6B6nXSE+H6xbzR+1KyXkxGyEpV1x2qQTCq0aiq+8nR6jPt4l34/5wxR50kyLquMgcsS5JpdOokg4t34eKOWjt1guzqmbxMgBjVT2U9sSGEEMLmmtWrRbN6zfhn/zC2nUpkxd6L/Hz0Cqfi05i18Tjv/XScyBBvRnYMpFdT31vS1kuSlWPkz7PJ+Lo5EurnVonvQtQUEnTfjoQTaBcPJjzXjPZoc/BuwjltPT7emYNTdh3CAoNYOCoc113vw4G56kur3gV6/JOGHZ/l0/wvc70hZ6xanmnb+2hjfoNPOkPkS/DnFwDoIl+kd3M/ejdXX3rjrmex43QiVzMMDGpdD183R1X8qXaQ+kJ46ueCtYNvtvktiD2gRoFGLK76paBq1YVmD8Lh5Zg1OsytR5b6lArRMFIFKUXN6z5iqVp+G2tzFye4hwq6o3+7NehOtBRRs9F87pu51YHhC+Hn16H3NNuPvleFyBdh/1K4fh62fwBh96s50w611FJl1ZUubxmtxJNq3nLSaTXinZFc/lHuG2k0asqD1k5laaTGqf3albDP7DTApF77dqqfVyQHN7DPm4+dGqdORFgCbUumj4XOXgXbzl7FF47UavMyAS5AZnLeFJHcij1ZJ4QQosrY6bT0CPOlR5gv1zNz+PFQLCv3XmL32WS2nUpk26lE/Go58miHAB7p0IA6tYqedmY0mYmKTmLN/ktsPBxHal4l9baBtRnVKYD+4XVx1Jc+yJSda2RndBIHL1znnibeRARUUvapqFISdN+OnEzIvIpDbhZEx0H0ZgKBhRrAEcxXXdF8Yq++mIGaP9pvZtEFw/SOcO9kaDEc1r+iCm9ZKjR7BKog9QZ+7o4Ma3vTHGCNRqVG7/gQjv1QdAB5ejNEzVHXB8+xzdrLAJEvYT7xIwn+vfCqyDnjJWkYqYqE3TyvO/GUSjvX2hWseV6RGvVQwVzMb+rEyI3zhCwj3V7VYKTbIqgbPLO19O3uFHon6PsuLHsMdswuWOs+sKtVc4BtSqcvCLxzs1QRNEtqdHlHuW+k0ajq6ho7SLmIJj0RR1065LqqjAA7RzVyrLVT22ZfU89zdC9XgbtKYRntTjyp/hdb/h9b6J1Vex3d1fuxpt0arZrrrtWpEfSUS2qU382v+rxvIYQQZebupOeRDgE80iGAc0npfLv7Asv2XCAuJYsPNp1k9q+n6NOsDo91CqRzsBcABy5eZ83+S6w7GEtCanb+vnzcHLiabuCvc1f569xV3l53jIfa1Wdkh0ACvArXXErJyuG34/H8fPQKW47Hk24wAvDR5pNMuK8xE+4LKdNIu6h+qvk3ymquXmtMLx7h9M616Ixp/B61iwDzJcIdrlDHGJefPo17AAx4TxVVKo1XsFoa6chKVdAq7Qp0e8X6L//NBqmg++TP6qTAjaNNaQmw6ll1vf2TakTPVvzCMU06w4UDh/CqqtdsWMy8bktqeaMelZMmGtBJFZVKu6KWiarTvOCx/PTyajLSfbdqOlClksdsUSdmoOzzuW3FziEv8D5VMHJ7u6PcN3P1Aa0O87Xz6IyZkJ5Z+HGNTrUjN0vdrm7znO1d1N975lVAo6beWAJtnZVLKd5Mo4Fa/nmZALGqCJ7ZqKrsCyGEqPECvVyY0j+Ml3o35sdDcXy96xx7zl3NL77WyMcFk8nM2aSCopseznrub1GXwa39aRdYm8S0bL7/8wJLd58n9noW87bG8PnvMdzT2IdHOzQgIc3Az0fi2BWTRI7RnL+fOrUcaOTtSlRMEh9tPsW2Uwl89EhEyQWSRbUmQfftcqrNztxQZkVdJyunEd0ae/PF6HZoNEaV5p12BfzblW15GY1GVdBu3EcVSarX2vrn1mtTsDb06c3Q9AF1v9kMa56D9HjwbQZ93inT26wUltGxquLmd+u8brO5cqqW38jOQVWDP71JpZhbgu7sVPVlHWy3RrdQNBro/x582qVgqS9r1+euDvRO6jOUdBrMpooZ5b6ZsyfonchOS8Zea0aTm6XqQxgNKti0VPrW6IpfxsyW3ANU2rjeufjU8bLSaNT/Fa1OzXtPT1Dz3qt7hoQQQgirOdjpeDDCnwcj/DkWm8I3f5xj1d5LxCSoE91Oeh29m9VhcOt6dGvsg71dwfHXt5YjE3o2Zty9wfx6PJ6v/zjP7ycT2Jp3uVGIryt9mtWhT3M/Wvq7o9VqWLP/Eq+vOsze89fo/9E23n6wOUMi5ORuTSTfDG7TtlOJzNh+FYMJ7g314bNRbfPma+jAJ1RdysvBrWwBN6gvgU0Hwq5PVIq5Jej+4zM1z1vnoNZkri7zLavazfO6449C4gk12lXc+t0VoVEPFXTH/AZdnlf3WUa5nb0rr1q8sJ5PKHR8Vk2/cPZWheRqEnsXNeKdk6GCy8pg50iuvQf2zs4FJ8xMJpXSbgnC7V0rJOB/7LHHCAsL47XXXgPgvvvuY/To0TzxxBPFPic0NJS5c+fSq1cRqyFotep/qhVK3E9RXHxUaroxRwJuIYS4gzWtW4t3HmzBlP5N+elwHHo7LT3DfHFxKPl/v51OS5/mfvRp7sfZxHSW7j7P+oOx+NZyoG9zP3o3q0Owz60FkAe39qdNQG1e+n4/e85d5aXvD7DlRAJvPxhOLUd9Ea8kqiv5dnAbohPSePrrvRhMcF+oD58+1hYHu0quwm2NpoNU0H1iI+QaVFD5yxvqsb7vQp1mtm2fLd08r9uSWt64T+WmxAb3UD/P7lBrM+sdVRYDSGp5dXLvFFVwK6h7zSwiZ+9S9aPMWi1onQqdyHv22WfJyclhwYIFt2y+Z88eRo4cyZo1awgLC7P6ZZYvX46TU8WeLPz444/ZtGkTa9asKXT/9u3bcXcv4/8DKwP6mu6bb75hwYIFJCQkEBYWxtSpU2nZsmWR265cuZJ//vOfhe6zt7fn0KFDVdFUIYSoNK4OdrfWVrJSQ28XXh3QlFcHWHdyv4GnM9893Ym5v0Uz+9dTrNl/mb/OXeXDh1vTrqGsnlFT1MBvldVHWlYumM10ru/I3L9FVI+AG9Qav651IPs6nPwRlv9dpYCGDlBzue9mN8/rtqSWW9Znriy+zdTvJDcTLvyh7suvXF6Niqjd7RzcYOBHlVPF/i4yfPhwdu7cSVxc3C2PrVixgvDw8DIF3ACenp4VHnQXx8fHB3v7cs71voNt2LCBGTNmMH78eFatWkVYWBhjx44lKSmp2Oe4urqyffv2/Mtvv/1WhS0WQog7g51Oyz96NWbZM51p4OnExauZjJgXxfile/nteDy5RpOtmyhKIUH3bWjVwIN9U3sxsbNHofkbNqfVqkrpAKvGqaq9rn4waI5U1rXM68YMuz5V8+71ztYVubsdGo1KMQeVYg7Vs3K5EBXg3nvvxdPTk5UrVxa6Pz09nY0bN9KrVy9efvllunXrRqtWrRg4cCDr1q0rcZ/33XcfX331Vf7ts2fPMnLkSFq0aMGAAQPYsWPHLc/597//Td++fWnVqhU9e/bkww8/JCcnB1CjsHPmzOH48eOEhoYSGhqa397Q0FA2bdqUv58TJ04wevRoWrZsSceOHZk6dSrp6QXLjU2ZMoXnnnuOBQsWEBkZSceOHZk2bVr+a90pFi5cyIgRIxg2bBghISFMmzYNR0dHVqxYUexzNBoNPj4++Rdvb+8qbLEQQtxZ2gbWZsML3RgS4Y/JDOsPxjLmqz/pPPNX3l1/lONxKbZuoiiGpJffJmvW27OJZoNgz4K8asYaGDoPXKqsTnj1ZpnXvf1DdbtJ36pJyQ3uAQe/U8XUer2p2gCSXi7KzmwuKFxWVfTWF4O0s7Nj8ODBrFq1inHjxqHJO9m3ceNGTCYTgwYNYuPGjTz11FO4urqyZcsWJk2aREBAQLGpyjcymUxMmDABLy8v/ve//5Gamsr06dNv2c7FxYUZM2bg6+vLyZMnmTp1Ki4uLjz11FMMGDCAU6dOsW3bNhYuVBXr3dxuTRHPyMhg7NixREREsHz5cpKSknj99dd5++23mTlzZv52f/zxBz4+PixatIjz58/z0ksv0bRpU0aMGGF1v1VnBoOBI0eO8Mwzz+Tfp9Vq6dKlC/v27Sv2eRkZGfTo0QOTyUSzZs14+eWXady47P/zjEZjudpd1D4qYl93G+m78pO+Kz/pu6I567X8Z3gLnugcwMp9l1l74DIJqdl8se0MX2w7Q7O6bjzYqi4hemOZ+i49O5fdZ6+yMzqRndHJZOUY+b8+TegXXkVL+1YjZfnsWdvHEnTfqQIjVTGljCTo+g+1HJJQLPO6LesZV1bV8ptZfgexB9Ra0JY53V4SdIsyMJvhy74F0xSqSoNOMOZHqzcfNmwYCxYsYPfu3XTs2BFQo8t9+vTB39+fsWPH5m/72GOPsX37dn788Uergu6dO3cSExPD/PnzqVOnDgAvvfQSTz31VKHtnnvuufzr9evX58yZM6xfv56nnnoKR0dHnJ2d0el0+Pj4FPta69atw2AwMGvWLJyd1YmHN954g2effZaJEyfmj9y6u7vzxhtvoNPpCA4Opnv37kRFRd0xQffVq1cxGo14eRU+eevl5UVMTEyRzwkKCmL69OmEhoaSmprKl19+ySOPPML69evx8yvbl7iKnAcuc8rLT/qu/KTvyk/6rniD6kP/ep7si81my7lM/rqczdHYVI7GpqIBvDdvxs/VjrquOuq62uHnqqOumx11XHRoNXA6OYeD8QYOXsnmZFION6xaBsD4b/fTP8SZx1u6oddZly2bYzSz+WwmVzONdPR3JMjDLv/ke01TkZ89CbrvVDo7VaX88l7o8oKtW1O9WOZ1A9i7QUjvqnldNz/wbQ7xR+Dg96pgl0YHtRtWzeuLO0j1P3gFBwcTERHBihUr6NixI+fOnWPPnj0sXrwYo9HIZ599xsaNG7ly5Qo5OTkYDAYcHR2t2nd0dDR+fn75ATdARETELdtt2LCBxYsXc+HCBTIyMsjNzcXV9dbqsKW9VmhoaH7ADdCmTRtMJhNnzpzJD7pDQkLQ6Qoyn3x8fDh58mSZXutOExERUej3EhERwYABA/juu+948cUXy7SvFi1aFOrf8jAajRw6dKhC9nW3kb4rP+m78pO+s1574GngaoaBdQdjWbn3EgcvpZCQYSIhw8Ch+MLbazRgr9OSnVt4Lri/hyNdQ7zpGuzFkcspfL7tDD+ezuBChh2zH2lNoFfxWW9ms5lfjyfw7o/HOZe3dvnyY+k09nVlSEQ9BrWqR113647ztlaWz55l29JI0H0nC+5RUDVbFLhxve6w+1Ul8aoS3EMF3X/OV7drB4KdFGwSZaDRwN83Vuv0covhw4fzzjvv8MYbb7By5UoCAgLo0KEDX3zxBYsXL+bVV18lNDQUJycnpk+fXqFzoPft28fEiROZMGECkZGRuLm5sX79+vxU8opmZ1f4cKrRaDCbzcVsXfPUrl0bnU53S9G0pKQkq+dp6/V6mjZtyvnz58v8+jqdrsK+dFfkvu420nflJ31XftJ31vN2c+KJro14rFMgW3f9hUvdRly4msXZxHTOJuVdEjNIy84lO9eEh7OeLsFedA3xJjLEmwBP5/xR6cER0DnYm5eW7efw5RQGz93Je8Nb0r9F3Vte93R8Km+tO8bveWuPe7s6EBHgwdaTCZyKT+O9n07y759P0rmRFw9G+NM/3A83Rz1ms5lrGTlcupbJ5WuZ+T9jr2fR2NeNsd2CcC1lObbKVJGfPQm6xd2p7ROw9T3o+HTVvm6jHmod6OS8dExJLRflodFU/dJgoFLby6B///68++67rFu3jtWrV/Poo4+i0WjYu3cvPXv2ZPDgwYCao3327FmCg4Ot2m9wcDBxcXHEx8fj6+sLwP79+wtts2/fPurVq8e4cePy77t8+XKhbfR6PSZTyRVfg4ODWbVqFRkZGfmj3Xv37kWr1RIUFGRVe+8E9vb2NG/enKioqPz1y00mE1FRUYwaNcqqfRiNRk6ePEn37t0rs6lCCCEAd0cdrQNr07FR4aDRbDaTlG7gWkYOQd4u6LTFZ8/1CPNlwwvdmPDtPv46d5Vx3+zliS4N+eeAMBzsdFzPzOGjTadYHHWWXJMZe52Wv0cGMb5HMG6Oeq5n5vDjoVhW7rvE7jPJ7IxOYmd0Em+sOYy/hxOx17PIMBQ3JzqWb/44x6R+YQyN8EdbQjvLIsdo4tSVNIJ9Xap05SkJusXdqcvz6lLVAruAzl4t4QZSuVzc0VxcXBgwYAD//e9/SUtLY8gQtTRfYGAgP/30E3v37sXd3Z2FCxeSmJhoddDdpUsXGjZsyJQpU5g0aRJpaWl88MEHhbYJDAwkNjaW9evX06JFC7Zs2VKoIjmAv78/Fy9e5NixY9SpUwdXV9dblgobOHAgs2fPZsqUKTz//PMkJyfz9ttvM3jw4LuuEveYMWOYPHky4eHhtGzZkkWLFpGZmcnQoWqJvUmTJlGnTh1eeeUVAObMmUPr1q0JDAwkJSWFBQsWcPnyZR566CFbvg0hhLiraTQavF0d8HZ1sGr7eh5OfPd0J/7z8wnmbY3hq51n+evcVQa3rscnW6JJTlffaXs1rcPr9zeloXfBoIC7k55HOgTwSIcALiRn8MOBy6zce5HohHSiEwpWAfF2dcDfw5F6Hk74ezhR28WeZXsucC4pg4n/O8CSqLO8MbAZbQPLty652Wxm34VrrNl3ibUHY0lON/B8jxAm9g0t1/7KQ4JuIaqSvTMEdIIzv6vb3hJ0izvb8OHDWb58Od27d8+fgz1u3DguXLjA2LFjcXJyYsSIEfTq1YvU1FSr9qnVapkzZw6vvfYaw4cPx9/fn9dff50nn3wyf5uePXvy+OOP89Zbb2EwGLj33nsZN24cc+bMyd+mb9++/PLLL4wePZqUlBRmzJiRH0BaODk5sWDBAt59912GDx+Ok5MTffr0YcqUKRXQOzXLgAEDSE5OZvbs2SQkJNC0aVPmz5+ff/IhNjYWrbZg+cyUlBSmTp1KQkIC7u7uNG/enO+++46QEPm/J4QQNYlep+Wf/ZvSMciTl5cd4NCl6xy6dB2AEF9X3nigGfc0Kb4oKUADT2fG9wjhuXuDORqbwrWMHOp5OFHX3bHI1aCe7BbEwh1nmfPraQ5cvM6wT6MY3Loek/uFUc/Dyap2xySksXr/Zdbsv5Q/zxzA29WeDkHlC+DLS2O+kyadWcFoNLJ//35at25dITn6Fb2/u8ld23fb/gubp6nrj6+DoG5l3sVd23cVoKb1XVZWFmfOnCEoKMjqQmOVxWw256dZ19RKpLZSVN+V9LutaZ/TylKR/SB9Wn7Sd+UnfVd+0nflV5l9d/laJi99v5+TV1J5oWdjRnUKRK/Tlv7EckpIzeY/P51g2V8XMJvBUa/l2e7B9GnmR1aukawcy8WU//NqhoGfj8Rx4OL1/P046XX0C/djcOt6RIZ4Y1dCm8vSf9ZuKyPdQlS14B4FQbeklwshhBBCiBqinocT3z/TGZPJXGHzrEvi4+bArOEteaxzIG+tPcrus8l8uOkUH246VepzdVoN3Rp782Brf3o3q4OLDYuyVYug+5tvvmHBggUkJCQQFhbG1KlTi12r9eeff+azzz7j/Pnz5ObmEhgYyJgxY3jwwQerttFClJdfKwgfDnYOqpK6EEIIIYQQNUhVBNw3Cvd35/tnOrHhUByzN58iOcOAo16Lk16Ho16Ho50Oh7zbTvY6WjfwYGCrelbPXa9sNg+6N2zYwIwZM5g2bRqtWrVi0aJFjB07lo0bN+Ll5XXL9u7u7owbN45GjRqh1+v57bffePXVV/Hy8qJbt7Kn6QpR5bRaGL7A1q0QQgghhBCixtBoNNzfsi73t7x12bLqrvIS8K20cOFCRowYwbBhwwgJCWHatGk4OjqyYsWKIrfv2LEjvXv3Jjg4mICAAB5//HFCQ0P566+/qrjlQgghhBBCCCFEyWw60m0wGDhy5AjPPPNM/n1arZYuXbqwb9++Up9vNpvZtWsXZ86cYeLEiWV6baOxuDXhysayn4ra391E+q78pO/Kr6b1ndFoxGw2519syfL6tm5HTVRU31l+p0aj8ZbPY035fAohhBCidDYNuq9evYrRaLwljdzLy4uYmJhin5eamso999yDwWBAq9Xyr3/9i65du5bptQ8dOlSuNlfV/u4m0nflJ31XfjWp73Q6HZmZmZhMJls3BYDMzExbN6HGurHvsrKyMBgMHD9+3IYtEkIIIURls/mc7vJwcXFh9erVZGRkEBUVxcyZM2nQoAEdO3a0eh8tWrSosCXDDh06VGH7u5tI35Wf9F351bS+y8nJISYmBp1Oh7Ozs03bYjabyczMxMnJSZYMK6Oi+i4zMxN7e3tCQkJu+SxaPqdCCCGEqPlsGnTXrl0bnU5HUlJSofuTkpLw9vYu9nlarZbAwEAAmjZtSnR0NJ9//nmZgm6dTlehX7gren93E+m78pO+K7+a0ndarRYXFxcSEhLQ6/VotbYrxWE2m8nOzkar1UrQXUY39h1ARkYGCQkJ1K5dG3t7exu3TgghhBCVyaZBt729Pc2bNycqKopevXoBYDKZiIqKYtSoUVbvx2QyYTAYKquZQghhMxqNhrp163LmzBnOnTtn07aYzWZycnLQ6/USdJdRUX3n4eGBn58sGyiEEELc6WyeXj5mzBgmT55MeHg4LVu2ZNGiRWRmZjJ06FAAJk2aRJ06dXjllVcAmDdvHuHh4QQEBGAwGNi6dSs//PADb775pg3fhRBCVB57e3saN25s85OLRqOR48ePF5kOLUp2c9/p9XrpQyGEEOIuYfOge8CAASQnJzN79mwSEhJo2rQp8+fPz08vj42NLZROmZGRwbRp04iLi8PR0ZFGjRrx73//mwEDBtjqLQghRKXTarU4OjratA2WitqOjo4SMJaR9J0QQghx97J50A0watSoYtPJlyxZUuj2Sy+9xEsvvVQVzRJCCCGEEEIIIW6L7SryCCGEEEIIIYQQdzgJuoUQQgghhBBCiEpSLdLLq5LZbAYK5tfdLst+Kmp/dxPpu/KTvis/6bvyk74rv7L2nWU7yzHrblWRx2z5/Jaf9F35Sd+Vn/Rd+Unf3Z6y9J+1x2uN+S47ohsMBg4dOmTrZgghhBClatGixV29jrccs4UQQtQEpR2v77qg22QykZubi1arlXVmhRBCVEtmsxmTyYSdnV2hFTzuNnLMFkIIUZ1Ze7y+64JuIYQQQgghhBCiqty9p8+FEEIIIYQQQohKJkG3EEIIIYQQQghRSSToFkIIIYQQQgghKokE3UIIIYQQQgghRCWRoFsIIYQQQgghhKgkEnQLIYQQQgghhBCVRIJuIYQQQgghhBCikkjQfRu++eYb7rvvPlq0aMFDDz3EwYMHbd2kaufPP//k2WefJTIyktDQUDZt2lTocbPZzEcffURkZCQtW7bkiSee4OzZs7ZpbDUzb948hg0bRkREBJ07d+a5554jJiam0DbZ2dlMmzaNjh07EhERwYQJE0hMTLRRi6uPpUuXMnDgQNq0aUObNm14+OGH2bp1a/7j0m/W+/zzzwkNDeXdd9/Nv0/6r2gff/wxoaGhhS79+vXLf1z6zbbkmF06OWaXnxyzy0+O2RVDjtdlU9XHbAm6y2nDhg3MmDGD8ePHs2rVKsLCwhg7dixJSUm2blq1kpGRQWhoKP/617+KfPyLL75gyZIlvPnmmyxbtgwnJyfGjh1LdnZ2Fbe0+tm9ezcjR45k2bJlLFy4kNzcXMaOHUtGRkb+NtOnT+e3337jww8/ZMmSJcTHx/P888/bsNXVg5+fHxMnTmTlypWsWLGCTp06MX78eE6dOgVIv1nr4MGDfPfdd4SGhha6X/qveI0bN2b79u35l6VLl+Y/Jv1mO3LMto4cs8tPjtnlJ8fs2yfH6/Kp0mO2WZTL8OHDzdOmTcu/bTQazZGRkeZ58+bZsFXVW5MmTcy//PJL/m2TyWTu2rWref78+fn3paSkmMPDw83r1q2zRROrtaSkJHOTJk3Mu3fvNpvNqq+aN29u/vHHH/O3OX36tLlJkybmffv22aiV1Vf79u3Ny5Ytk36zUlpamrlPnz7mHTt2mEeNGmV+5513zGazfO5KMnv2bPOgQYOKfEz6zbbkmF12csy+PXLMvj1yzLaeHK/Lp6qP2TLSXQ4Gg4EjR47QpUuX/Pu0Wi1dunRh3759NmxZzXLx4kUSEhIK9aObmxutWrWSfixCamoqAO7u7gAcPnyYnJycQv0XHBxMvXr12L9/vy2aWC0ZjUbWr19PRkYGERER0m9Weuutt+jevXuhfgL53JXm3LlzREZG0rNnT1555RUuX74MSL/ZkhyzK4Ycs8tGjtnlI8fsspPjdflV5THbriIafLe5evUqRqMRLy+vQvd7eXndMn9HFC8hIQGgyH6U+SaFmUwmpk+fTps2bWjSpAkAiYmJ6PV6atWqVWhbLy+v/L69m504cYJHHnmE7OxsnJ2dmTt3LiEhIRw7dkz6rRTr16/n6NGjLF++/JbH5HNXvJYtWzJjxgyCgoJISEhg7ty5jBw5krVr10q/2ZAcsyuGHLOtJ8fsspNjdvnI8br8qvqYLUG3EDXAtGnTOHXqVKG5JqJkQUFBrF69mtTUVH766ScmT57M119/betmVXuxsbG8++67fPnllzg4ONi6OTVK9+7d86+HhYXRqlUrevTowY8//oijo6MNWyaEqEpyzC47OWaXnRyvb09VH7MlvbwcateujU6nu6UAS1JSEt7e3jZqVc3j4+MDIP1YirfeeostW7awaNEi/Pz88u/39vYmJyeHlJSUQtsnJSXl9+3dzN7ensDAQMLDw3nllVcICwtj8eLF0m+lOHLkCElJSQwdOpRmzZrRrFkzdu/ezZIlS2jWrJn0XxnUqlWLhg0bcv78eek3G5JjdsWQY7Z15JhdPnLMLjs5Xlesyj5mS9BdDvb29jRv3pyoqKj8+0wmE1FRUURERNiwZTVL/fr18fHxKdSPaWlpHDhwQPoRtTTLW2+9xS+//MKiRYto0KBBocfDw8PR6/WF+i8mJobLly/TunXrKm5t9WcymTAYDNJvpejUqRNr165l9erV+Zfw8HAGDhyYf136zzrp6elcuHABHx8f6TcbkmN2xZBjdsnkmF2x5JhdOjleV6zKPmZLenk5jRkzhsmTJxMeHk7Lli1ZtGgRmZmZDB061NZNq1bS09M5f/58/u2LFy9y7Ngx3N3dqVevHqNHj+bTTz8lMDCQ+vXr89FHH+Hr60uvXr1s2OrqYdq0aaxbt45PPvkEFxeX/Dkkbm5uODo64ubmxrBhw5g5cybu7u64urryzjvvEBERcdf/M33//fe55557qFu3Lunp6axbt47du3ezYMEC6bdSuLq65s9BtHB2dsbDwyP/fum/os2aNYsePXpQr1494uPj+fjjj9FqtTzwwAPyubMxOWZbR47Z5SfH7PKTY3b5yPH69lT1MVuC7nIaMGAAycnJzJ49m4SEBJo2bcr8+fMlxeomhw8fZvTo0fm3Z8yYAcCQIUOYOXMmTz31FJmZmbzxxhukpKTQtm1b5s+fL3NTgG+//RaAxx57rND9M2bMyP+i+Oqrr6LVannhhRcwGAxERkYWu77q3SQpKYnJkycTHx+Pm5sboaGhLFiwgK5duwLSb7dL+q9ocXFxvPzyy1y7dg1PT0/atm3LsmXL8PT0BKTfbEmO2daRY3b5yTG7/OSYXXmk74pX1cdsjdlsNldU44UQQgghhBBCCFFA5nQLIYQQQgghhBCVRIJuIYQQQgghhBCikkjQLYQQQgghhBBCVBIJuoUQQgghhBBCiEoiQbcQQgghhBBCCFFJJOgWQgghhBBCCCEqiQTdQgghhBBCCCFEJZGgWwghhBBCCCGEqCQSdAshbCY0NJRNmzbZuhlCCCGEKIEcr4W4PXa2boAQwjamTJnCqlWrbrk/MjKSBQsW2KBFQgghhLiZHK+FqPkk6BbiLtatWzdmzJhR6D57e3sbtUYIIYQQRZHjtRA1m6SXC3EXs7e3x8fHp9DF3d0dUKlkS5cu5cknn6Rly5b07NmTjRs3Fnr+iRMnGD16NC1btqRjx45MnTqV9PT0QtssX76c+++/n/DwcCIjI3nrrbcKPX716lXGjx9Pq1at6NOnD5s3b67cNy2EEELUMHK8FqJmk6BbCFGsjz76iL59+7JmzRoGDhzIyy+/THR0NAAZGRmMHTsWd3d3li9fzocffsjOnTt5++2385+/dOlS3nrrLUaMGMHatWv55JNPCAgIKPQac+bMoX///vzwww/cc889TJw4kWvXrlXl2xRCCCFqNDleC1G9SdAtxF1sy5YtREREFLp89tln+Y/369ePhx56iKCgIF588UXCw8NZsmQJAOvWrcNgMDBr1iyaNGlC586deeONN1izZg2JiYkAfPrpp4wZM4bHH3+coKAgWrZsyRNPPFGoDUOGDOGBBx4gMDCQl19+mYyMDA4ePFhlfSCEEEJUd3K8FqJmkzndQtzFOnbsyJtvvlnoPku6GkBEREShx1q3bs2xY8cAiI6OJjQ0FGdn5/zH27Rpg8lk4syZM2g0GuLj4+ncuXOJbQgNDc2/7uzsjKurK8nJyeV9S0IIIcQdR47XQtRsEnQLcRdzcnIiMDCwUvbt4OBg1XZ6vb7QbY1Gg8lkqowmCSGEEDWSHK+FqNkkvVwIUaz9+/cXun3gwAGCg4MBCA4O5sSJE2RkZOQ/vnfvXrRaLUFBQbi6uuLv709UVFRVNlkIIYS468jxWojqTYJuIe5iBoOBhISEQpcbU8U2btzI8uXLOXPmDLNnz+bgwYOMGjUKgIEDB2Jvb8+UKVM4efIku3bt4u2332bw4MF4e3sDMGHCBBYuXMjixYs5e/YsR44cyZ9jJoQQQgjryPFaiJpN0suFuItt27aNyMjIQvcFBQXlLzUyYcIENmzYwLRp0/Dx8eH9998nJCQEUKluCxYs4N1332X48OE4OTnRp08fpkyZkr+vIUOGkJ2dzVdffcV7772Hh4cH/fr1q7o3KIQQQtwB5HgtRM2mMZvNZls3QghR/YSGhjJ37lx69epl66YIIYQQohhyvBai+pP0ciGEEEIIIYQQopJI0C2EEEIIIYQQQlQSSS8XQgghhBBCCCEqiYx0CyGEEEIIIYQQlUSCbiGEEEIIIYQQopJI0C2EEEIIIYQQQlQSCbqFEEIIIYQQQohKIkG3EEIIIYQQQghRSSToFkIIIYQQQgghKokE3UIIIYQQQgghRCWRoFsIIYQQQgghhKgkEnQLIYQQQgghhBCV5P8BsN7Fjj2DVXAAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "These two lines work together to set up a mechanism to save the best performing model weights during training. The model's weights are saved to the file specified by top_model_weights_path only when the validation accuracy (val_acc) improves. This ensures that you retain the model's optimal state throughout the training process."
      ],
      "metadata": {
        "id": "dT-H0aUcW5BV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "top_model_weights_path = location+'/top_model_weights.weights.h5'  # Changed the file extension to .weights.h5\n",
        "checkpoint = ModelCheckpoint(top_model_weights_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=True,mode='auto')"
      ],
      "metadata": {
        "id": "5OWDQh7wNNoj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function is designed to fine-tune a pre-trained model called VGG16 for a specific image classification task, likely the one classifying the location of damage in images (front, rear, or behind)."
      ],
      "metadata": {
        "id": "FGixY8NkXDlU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def finetune_categorical_model():\n",
        "    input_tensor = Input(shape=(256,256,3))\n",
        "    base_model = VGG16(weights='imagenet',include_top= False,input_tensor=input_tensor)\n",
        "    print(\"Model loaded.\")\n",
        "    top_model = Sequential()\n",
        "    top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "    top_model.add(Dense(256, activation='relu'))\n",
        "    top_model.add(Dropout(0.5))\n",
        "    top_model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # Load the weights here, within the function's scope\n",
        "    top_model.load_weights(top_model_weights_path)\n",
        "\n",
        "    model = Model(inputs=base_model.input, outputs=top_model(base_model.output))\n",
        "\n",
        "    for layer in model.layers[:25]:\n",
        "        layer.trainable = False\n",
        "\n",
        "    model.compile(loss='categorical_crossentropy', optimizer=optimizers.SGD(lr=0.00001, momentum=0.9), metrics=['accuracy'])\n",
        "\n",
        "    train_datagen = ImageDataGenerator(rescale=1./255, shear_range=0.2, zoom_range=0.2, horizontal_flip=True)\n",
        "\n",
        "    test_datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    train_generator = train_datagen.flow_from_directory(train_data_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode='categorical')\n",
        "\n",
        "    validation_generator = test_datagen.flow_from_directory(validation_data_dir, target_size=(img_height, img_width), batch_size=batch_size, class_mode='categorical')\n",
        "\n",
        "    checkpoint = ModelCheckpoint(fine_tuned_model_path, monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto')\n",
        "\n",
        "    fit = model.fit_generator(train_generator, steps_per_epoch=nb_train_samples//batch_size, epochs=epochs, validation_data=validation_generator, validation_steps=nb_validation_samples//batch_size, verbose=1, callbacks=[checkpoint])\n",
        "\n",
        "    with open(location+'/ft_history.txt', 'w') as f:\n",
        "        json.dump(fit.history, f)\n",
        "\n",
        "    return model, fit.history"
      ],
      "metadata": {
        "id": "7Q1TZlFpN8-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function is designed to fine-tune a pre-trained VGG16 model for a specific image classification task (likely front, rear, behind damage classification). Let's go through it step by step:\n",
        "\n",
        "input_tensor = Input(shape=(256,256,3)): Defines the input shape for the model. This means the model expects images of size 256x256 pixels with 3 color channels (RGB).\n",
        "\n",
        "base_model = VGG16(weights='imagenet',include_top= False,input_tensor=input_tensor): Loads the pre-trained VGG16 model with weights trained on the ImageNet dataset. include_top=False means we are excluding the fully connected layers at the top of the VGG16"
      ],
      "metadata": {
        "id": "7PK64E9OXKIe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import h5py\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from IPython.display import Image, display, clear_output\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, load_model, Model\n",
        "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, Activation, Dense, Dropout, Flatten\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.callbacks import ModelCheckpoint, History\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.regularizers import l2, l1\n",
        "from tensorflow.keras.layers import Input # Import Input\n",
        "\n",
        "# ... (Your other functions and code) ...\n",
        "\n",
        "def finetune_categorical_model():\n",
        "    input_tensor = Input(shape=(256,256,3))\n",
        "    base_model = VGG16(weights='imagenet',include_top= False,input_tensor=input_tensor)\n",
        "    print(\"Model loaded.\")\n",
        "    top_model = Sequential()\n",
        "    top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "    top_model.add(Dense(256, activation='relu'))\n",
        "    top_model.add(Dropout(0.5))\n",
        "    top_model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # Load the weights using the correct path and extension (.h5)\n",
        "    top_model.load_weights(location + '/top_model_weights.h5')\n",
        "\n",
        "    # ... (rest of your finetune_categorical_model function) ..."
      ],
      "metadata": {
        "id": "tQRbOKuBSt8g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "input_tensor = Input(shape=(256,256,3)): Defines the input shape for the model, expecting 256x256 RGB images.\n",
        "base_model = VGG16(weights='imagenet',include_top= False,input_tensor=input_tensor): Loads the pre-trained VGG16 model with weights learned from the ImageNet dataset. include_top=False excludes VGG16's classification layers.\n",
        "print(\"Model loaded.\"): A simple message indicating the model is loaded.\n",
        "top_model = Sequential(): Creates a new sequential model, which will be stacked on top of VGG16.\n",
        "top_model.add(...): Adds layers to the top_model:\n",
        "Flatten: Flattens the output from VGG16.\n",
        "Dense(256, activation='relu'): A fully connected layer with 256 neurons and ReLU activation.\n",
        "Dropout(0.5): A dropout layer to prevent overfitting.\n",
        "Dense(3, activation='softmax'): The output layer with 3 neurons (for 3 damage categories) and softmax activation for probability distribution.\n",
        "Checking for Weights:\n",
        "`weights_path = location + '/top_model_"
      ],
      "metadata": {
        "id": "rqx-bVVrYJY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib.request\n",
        "import h5py\n",
        "import json\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "from IPython.display import Image, display, clear_output\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_style('whitegrid')\n",
        "\n",
        "from keras import optimizers\n",
        "from keras.models import Sequential, load_model, Model\n",
        "from keras.layers import Convolution2D, MaxPooling2D, ZeroPadding2D, Activation, Dense, Dropout, Flatten\n",
        "from keras.applications.vgg16 import VGG16\n",
        "from keras.callbacks import ModelCheckpoint, History\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
        "from keras.regularizers import l2, l1\n",
        "from tensorflow.keras.layers import Input # Import Input\n",
        "\n",
        "# ... (Your other functions and code) ...\n",
        "\n",
        "def finetune_categorical_model():\n",
        "    input_tensor = Input(shape=(256,256,3))\n",
        "    base_model = VGG16(weights='imagenet',include_top= False,input_tensor=input_tensor)\n",
        "    print(\"Model loaded.\")\n",
        "    top_model = Sequential()\n",
        "    top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "    top_model.add(Dense(256, activation='relu'))\n",
        "    top_model.add(Dropout(0.5))\n",
        "    top_model.add(Dense(3, activation='softmax'))\n",
        "\n",
        "    # Check if the weights file exists before loading\n",
        "    weights_path = location + '/top_model_weights.h5'\n",
        "    if os.path.exists(weights_path):\n",
        "        # Load the weights using the correct path and extension (.h5)\n",
        "        top_model.load_weights(weights_path)\n",
        "    else:\n",
        "        print(f\"Error: Weights file not found at {weights_path}\")\n",
        "        # You might want to handle this error differently, like returning or raising an exception\n",
        "        return None, None  # Returning None for model and history\n",
        "\n",
        "\n",
        "    # ... (rest of your finetune_categorical_model function) ..."
      ],
      "metadata": {
        "id": "Sraa83iLYAL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code trains an initial model, saves its learned weights, and then uses those weights to fine-tune a more powerful, pre-trained model (VGG16). This approach leverages the benefits of transfer learning to improve the accuracy and efficiency of the image classification task."
      ],
      "metadata": {
        "id": "7LZM5-JrXbS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def finetune_categorical_model():\n",
        "    input_tensor = Input(shape=(256,256,3))\n",
        "    base_model = VGG16(weights='imagenet',include_top= False,input_tensor=input_tensor)\n",
        "    print(\"Model loaded.\")\n",
        "    top_model = Sequential()\n",
        "    top_model.add(Flatten(input_shape=base_model.output_shape[1:]))\n",
        "    top_model.add(Dense(256, activation='relu'))\n",
        "    top_model.add(Dropout(0.5))\n",
        "    top_model.add(Dense(3, activation='softmax'))  # Make sure this layer has the correct output shape\n",
        "\n",
        "    # Check if the weights file exists and load if it does\n",
        "    weights_path = location + '/top_model_weights.h5'\n",
        "    if os.path.exists(weights_path):\n",
        "        # Load weights, handling potential shape mismatches\n",
        "        try:\n",
        "            top_model.load_weights(weights_path)\n",
        "        except ValueError as e:\n",
        "            print(f\"Error loading weights: {e}\")\n",
        "            print(\"Possible shape mismatch. Check model architecture or weights file.\")\n",
        "            # Consider using 'by_name=True' and 'skip_mismatch=True' for partial loading\n",
        "            # top_model.load_weights(weights_path, by_name=True, skip_mismatch=True)\n",
        "            return None, None  # Or handle the error differently\n",
        "    else:\n",
        "        print(f\"Error: Weights file not found at {weights_path}\")\n",
        "        return None, None\n",
        "\n",
        "    # ... (rest of your finetune_categorical_model function) ..."
      ],
      "metadata": {
        "id": "PTX1IJkCMOpO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %% [markdown]\n",
        "# # Train the initial model first\n",
        "# This will create the 'top_model_weights.weights.h5' file\n",
        "# %%\n",
        "d3_model, d3_history = train_categorical_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGejRbauOR-U",
        "outputId": "474e461a-3172-45a5-b833-7e994dc47730"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 181ms/step - accuracy: 0.3438 - loss: 8.3459 - val_accuracy: 0.3326 - val_loss: 1.0988\n",
            "Epoch 2/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/callbacks/model_checkpoint.py:206: UserWarning: Can save best model only with val_acc available, skipping.\n",
            "  self._save_model(epoch=epoch, batch=None, logs=logs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 169ms/step - accuracy: 0.3503 - loss: 1.3294 - val_accuracy: 0.3371 - val_loss: 1.0986\n",
            "Epoch 3/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 261ms/step - accuracy: 0.3646 - loss: 1.1504 - val_accuracy: 0.3348 - val_loss: 1.1563\n",
            "Epoch 4/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 182ms/step - accuracy: 0.3425 - loss: 1.1834 - val_accuracy: 0.3304 - val_loss: 1.1130\n",
            "Epoch 5/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 163ms/step - accuracy: 0.3470 - loss: 1.1861 - val_accuracy: 0.3482 - val_loss: 1.1195\n",
            "Epoch 6/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 169ms/step - accuracy: 0.3578 - loss: 1.1314 - val_accuracy: 0.3170 - val_loss: 1.1089\n",
            "Epoch 7/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 163ms/step - accuracy: 0.3923 - loss: 1.1096 - val_accuracy: 0.3103 - val_loss: 1.1005\n",
            "Epoch 8/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 164ms/step - accuracy: 0.3841 - loss: 1.1199 - val_accuracy: 0.3348 - val_loss: 1.1020\n",
            "Epoch 9/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 167ms/step - accuracy: 0.4079 - loss: 1.0927 - val_accuracy: 0.3460 - val_loss: 1.1829\n",
            "Epoch 10/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 173ms/step - accuracy: 0.4060 - loss: 1.1248 - val_accuracy: 0.3259 - val_loss: 1.2835\n",
            "Epoch 11/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 163ms/step - accuracy: 0.4297 - loss: 1.0673 - val_accuracy: 0.3259 - val_loss: 1.1485\n",
            "Epoch 12/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.4481 - loss: 1.0565 - val_accuracy: 0.3438 - val_loss: 1.1199\n",
            "Epoch 13/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 164ms/step - accuracy: 0.4722 - loss: 1.0178 - val_accuracy: 0.3393 - val_loss: 1.2508\n",
            "Epoch 14/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 169ms/step - accuracy: 0.4640 - loss: 1.0781 - val_accuracy: 0.3237 - val_loss: 1.1560\n",
            "Epoch 15/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 163ms/step - accuracy: 0.5215 - loss: 0.9986 - val_accuracy: 0.3237 - val_loss: 1.2303\n",
            "Epoch 16/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.5500 - loss: 0.9519 - val_accuracy: 0.2924 - val_loss: 1.3314\n",
            "Epoch 17/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 169ms/step - accuracy: 0.5434 - loss: 0.9318 - val_accuracy: 0.3348 - val_loss: 1.6373\n",
            "Epoch 18/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 164ms/step - accuracy: 0.5772 - loss: 0.8737 - val_accuracy: 0.3348 - val_loss: 1.2223\n",
            "Epoch 19/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 162ms/step - accuracy: 0.6259 - loss: 0.7939 - val_accuracy: 0.3728 - val_loss: 1.3170\n",
            "Epoch 20/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 175ms/step - accuracy: 0.5892 - loss: 0.8918 - val_accuracy: 0.3482 - val_loss: 1.3372\n",
            "Epoch 21/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 163ms/step - accuracy: 0.6220 - loss: 0.8220 - val_accuracy: 0.3304 - val_loss: 1.2403\n",
            "Epoch 22/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 170ms/step - accuracy: 0.6515 - loss: 0.7425 - val_accuracy: 0.3281 - val_loss: 1.6341\n",
            "Epoch 23/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 251ms/step - accuracy: 0.6442 - loss: 0.7376 - val_accuracy: 0.3817 - val_loss: 1.3010\n",
            "Epoch 24/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 158ms/step - accuracy: 0.6626 - loss: 0.6904 - val_accuracy: 0.3616 - val_loss: 1.5440\n",
            "Epoch 25/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 160ms/step - accuracy: 0.6905 - loss: 0.6651 - val_accuracy: 0.3482 - val_loss: 1.3951\n",
            "Epoch 26/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 163ms/step - accuracy: 0.6977 - loss: 0.6622 - val_accuracy: 0.3594 - val_loss: 1.3095\n",
            "Epoch 27/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 159ms/step - accuracy: 0.6879 - loss: 0.6343 - val_accuracy: 0.3772 - val_loss: 1.4006\n",
            "Epoch 28/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 162ms/step - accuracy: 0.7233 - loss: 0.6694 - val_accuracy: 0.3594 - val_loss: 1.3369\n",
            "Epoch 29/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 160ms/step - accuracy: 0.7154 - loss: 0.6331 - val_accuracy: 0.3549 - val_loss: 1.6530\n",
            "Epoch 30/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 170ms/step - accuracy: 0.7293 - loss: 0.6002 - val_accuracy: 0.3549 - val_loss: 1.9506\n",
            "Epoch 31/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 160ms/step - accuracy: 0.6994 - loss: 0.6898 - val_accuracy: 0.3504 - val_loss: 2.0754\n",
            "Epoch 32/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m34s\u001b[0m 279ms/step - accuracy: 0.7231 - loss: 0.5960 - val_accuracy: 0.3504 - val_loss: 1.5483\n",
            "Epoch 33/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 167ms/step - accuracy: 0.7270 - loss: 0.5842 - val_accuracy: 0.3527 - val_loss: 1.4816\n",
            "Epoch 34/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 162ms/step - accuracy: 0.7106 - loss: 0.6188 - val_accuracy: 0.3549 - val_loss: 1.6090\n",
            "Epoch 35/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 161ms/step - accuracy: 0.7714 - loss: 0.5315 - val_accuracy: 0.3683 - val_loss: 2.3041\n",
            "Epoch 36/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 167ms/step - accuracy: 0.7426 - loss: 0.5360 - val_accuracy: 0.3906 - val_loss: 1.7480\n",
            "Epoch 37/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 164ms/step - accuracy: 0.7581 - loss: 0.5552 - val_accuracy: 0.3705 - val_loss: 1.7218\n",
            "Epoch 38/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 160ms/step - accuracy: 0.7574 - loss: 0.5616 - val_accuracy: 0.3750 - val_loss: 2.0737\n",
            "Epoch 39/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 168ms/step - accuracy: 0.7887 - loss: 0.5080 - val_accuracy: 0.3705 - val_loss: 1.7646\n",
            "Epoch 40/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 160ms/step - accuracy: 0.7875 - loss: 0.5320 - val_accuracy: 0.3594 - val_loss: 1.6245\n",
            "Epoch 41/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 163ms/step - accuracy: 0.7666 - loss: 0.5347 - val_accuracy: 0.3504 - val_loss: 1.8630\n",
            "Epoch 42/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 172ms/step - accuracy: 0.7696 - loss: 0.4815 - val_accuracy: 0.3839 - val_loss: 1.9240\n",
            "Epoch 43/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 163ms/step - accuracy: 0.7795 - loss: 0.4660 - val_accuracy: 0.3750 - val_loss: 1.6236\n",
            "Epoch 44/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 162ms/step - accuracy: 0.7764 - loss: 0.4917 - val_accuracy: 0.3728 - val_loss: 1.9380\n",
            "Epoch 45/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m22s\u001b[0m 172ms/step - accuracy: 0.7920 - loss: 0.4972 - val_accuracy: 0.3415 - val_loss: 2.2527\n",
            "Epoch 46/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 162ms/step - accuracy: 0.8139 - loss: 0.4415 - val_accuracy: 0.3862 - val_loss: 1.9464\n",
            "Epoch 47/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 162ms/step - accuracy: 0.8045 - loss: 0.4352 - val_accuracy: 0.3527 - val_loss: 1.9030\n",
            "Epoch 48/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m19s\u001b[0m 165ms/step - accuracy: 0.7933 - loss: 0.4610 - val_accuracy: 0.3549 - val_loss: 2.0240\n",
            "Epoch 49/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 165ms/step - accuracy: 0.8062 - loss: 0.4412 - val_accuracy: 0.3460 - val_loss: 2.0810\n",
            "Epoch 50/50\n",
            "\u001b[1m114/114\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 162ms/step - accuracy: 0.7934 - loss: 0.4803 - val_accuracy: 0.3638 - val_loss: 2.2704\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ft_model, ft_history = finetune_categorical_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mk1Bfs_jMeaz",
        "outputId": "19017c79-d9ab-46c9-d952-846abf8ae8bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded.\n",
            "Error loading weights: The shape of the target variable and the shape of the target value in `variable.assign(value)` must match. variable.shape=(256, 3), Received: value.shape=(256, 1). Target variable: <KerasVariable shape=(256, 3), dtype=float32, path=sequential_4/dense_9/kernel>\n",
            "Possible shape mismatch. Check model architecture or weights file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This section of code is designed to load a pre-trained, fine-tuned model if it exists. This is a common practice in machine learning to save time and resources, as training a model can be computationally expensive."
      ],
      "metadata": {
        "id": "FRIWnZZFdaPJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to load the fine-tuned model:\n",
        "# Check if the fine-tuned model file exists\n",
        "if os.path.exists(fine_tuned_model_path):\n",
        "    ft_model = load_model(fine_tuned_model_path)  # Assuming fine_tuned_model_path is defined correctly\n",
        "else:\n",
        "    print(f\"Error: Fine-tuned model file not found at {fine_tuned_model_path}\")\n",
        "    # You might want to handle this error differently, like retraining the model\n",
        "    # or loading a different model."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8BiMIedgdVtZ",
        "outputId": "2c7141da-ef9f-4530-b5c4-63a90d0c3a32"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Fine-tuned model file not found at /content/drive/MyDrive/data2/ft_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is designed to load a pre-trained model if it exists, avoiding the need for retraining. This is important because training a machine learning model can be time-consuming and computationally expensive."
      ],
      "metadata": {
        "id": "AxPaoYjTo5td"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# If you want to load the fine-tuned model:\n",
        "# Check if the fine-tuned model file exists\n",
        "if os.path.exists(fine_tuned_model_path):\n",
        "    ft_model = load_model(fine_tuned_model_path)  # Assuming fine_tuned_model_path is defined correctly\n",
        "else:\n",
        "    print(f\"Error: Fine-tuned model file not found at {fine_tuned_model_path}\")\n",
        "    # You might want to handle this error differently, like retraining the model\n",
        "    # or loading a different model."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d5nC1ICAeV8s",
        "outputId": "2e70e16f-d7bb-442a-cadf-6e2039ce9718"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Fine-tuned model file not found at /content/drive/MyDrive/data2/ft_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Loading a pre-trained model saves a lot of time and computing power. Instead of retraining the model from scratch, which can take hours or even days, you can simply load the existing one and use it immediately for making predictions or further fine-tuning."
      ],
      "metadata": {
        "id": "-tWQEx0fpHG3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# ... other imports ...\n",
        "\n",
        "# Assuming 'location' is defined as '/content/drive/MyDrive/data2'\n",
        "fine_tuned_model_path = os.path.join(location, 'ft_model.h5')\n",
        "\n",
        "# Check if the file exists before trying to load it\n",
        "if os.path.exists(fine_tuned_model_path):\n",
        "    ft_model = load_model(fine_tuned_model_path)\n",
        "else:\n",
        "    print(f\"Error: Fine-tuned model file not found at {fine_tuned_model_path}\")\n",
        "    # Either retrain the model or handle the error appropriately\n",
        "    # For example, you could train a new model:\n",
        "    # ft_model, ft_history = finetune_categorical_model()\n",
        "    # or raise an exception to stop execution\n",
        "    # raise FileNotFoundError(f\"Model file not found: {fine_tuned_model_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "By_5lO-0iJWO",
        "outputId": "343f831d-ee3a-41a1-e380-dede60563a7f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: Fine-tuned model file not found at /content/drive/MyDrive/data2/ft_model.h5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is creating an array called validation_labels which will likely be used to evaluate the performance of the image classification model"
      ],
      "metadata": {
        "id": "BLTF7PmUpPn-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "validation_labels = np.array([0] * validation_samples[0] +\n",
        "                             [1] * validation_samples[1])"
      ],
      "metadata": {
        "id": "fht4ZivTiSpN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line of code is using a validation dataset to test the performance of a fine-tuned model and saving the results for analysis. The confusion matrix (cm) that is generated will provide insights into the types of errors the model makes, helping the user understand where it might need improvement."
      ],
      "metadata": {
        "id": "3ggOQPtHpU_S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def evaluate_categorical_model(model, directory, labels):\n",
        "    datagen = ImageDataGenerator(rescale=1./255)\n",
        "\n",
        "    generator = datagen.flow_from_directory(directory, target_size=(img_height,img_width), batch_size=batch_size, class_mode='categorical', shuffle=False)\n",
        "\n",
        "    # Use predict with steps argument instead of predict_generator\n",
        "    predictions = model.predict(generator, steps=len(labels) // batch_size + (len(labels) % batch_size > 0))\n",
        "\n",
        "    pred_labels = [0 if i<0.5 else 1 for i in predictions]\n",
        "\n",
        "    print('')\n",
        "    print(classification_report(validation_labels, pred_labels))\n",
        "    print('')\n",
        "    cm = confusion_matrix(validation_labels, pred_labels)\n",
        "    return cm"
      ],
      "metadata": {
        "id": "zJLAtlwyihcF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Path 3"
      ],
      "metadata": {
        "id": "2os46SpnimYJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function is designed to take an image path and a trained model as input, and then predict the location of damage in the image (Front, Rear, or Side)."
      ],
      "metadata": {
        "id": "il4JTP5NpeF7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def path3(image_path, model):\n",
        "    urllib.request.urlretrieve(image_path, 'save.jpg')\n",
        "    img = load_img('save.jpg', target_size=(256,256))\n",
        "    x = img_to_array(img)\n",
        "    x = x.reshape((1,)+x.shape)/255\n",
        "    pred = model.predict(x)\n",
        "    pred_labels = np.argmax(pred, axis=1)\n",
        "    d = {0:'Front', 1:'Rear', 2:'Side'}\n",
        "    for key in d.keys():\n",
        "        if pred_labels[0] == key:\n",
        "            print(\"Validating location of damage....Result:\",d[key])\n",
        "    print(\"Severity assessment complete.\")"
      ],
      "metadata": {
        "id": "cgz-yBJMirRj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This line of code is using the Image function (likely from the IPython.display library, which was imported earlier in the notebook) to display an image within the Jupyter notebook."
      ],
      "metadata": {
        "id": "KKgxzbF0UUZl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Image('http://www.copartdirect.com/content/2007-kia-rio-front-end-damage.jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "id": "ZlLYxsqHjLkc",
        "outputId": "352202fc-5684-498d-fee4-05ada785fc0a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "image/jpeg": "PGh0bWw+DQo8aGVhZD4NCjxNRVRBIE5BTUU9InJvYm90cyIgQ09OVEVOVD0ibm9pbmRleCxub2ZvbGxvdyI+DQo8c2NyaXB0IHNyYz0iL19JbmNhcHN1bGFfUmVzb3VyY2U/U1dKSVlMV0E9NTA3NGE3NDRlMmUzZDg5MTgxNGU5YTJkYWNlMjBiZDQsNzE5ZDM0ZDMxYzhlM2E2ZTZmZmZkNDI1ZjdlMDMyZjMiPg0KPC9zY3JpcHQ+DQo8Ym9keT4NCjwvYm9keT48L2h0bWw+DQo=\n",
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " This code is primarily about displaying an image from a URL within a Jupyter notebook. It has a built-in mechanism to handle a common issue where websites might block direct image access, ensuring the image is displayed correctly"
      ],
      "metadata": {
        "id": "XWx4h4XLUOJ3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "\n",
        "from IPython.display import Image, display\n",
        "import requests\n",
        "import io\n",
        "\n",
        "def display_image_from_url(url):\n",
        "    \"\"\"\n",
        "    Displays an image from a URL using IPython.display.Image.\n",
        "\n",
        "    Handles HTTPError 403 by using the 'requests' library\n",
        "    to fetch the image with a user agent header.\n",
        "\n",
        "    Args:\n",
        "        url: The URL of the image to display.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try loading the image directly using IPython.display.Image\n",
        "        display(Image(url=url))\n",
        "    except urllib.error.HTTPError as e:\n",
        "        if e.code == 403:\n",
        "            # If HTTPError 403 (Forbidden) is encountered:\n",
        "            print(\"Using requests to bypass 403 error...\")\n",
        "            # Use requests to fetch the image with a user agent header\n",
        "            headers = {'User-Agent': 'Mozilla/5.0'}\n",
        "            response = requests.get(url, headers=headers)\n",
        "            response.raise_for_status()  # Raise an exception for bad responses\n",
        "            image_data = io.BytesIO(response.content)\n",
        "            # Display the image using IPython.display.Image\n",
        "            display(Image(data=image_data.getvalue()))\n",
        "        else:\n",
        "            # If it's a different HTTP error, re-raise it\n",
        "            raise e\n",
        "\n",
        "# Replace the original Image call with:\n",
        "display_image_from_url('http://drndata.com/wp-content/uploads/2016/03/car.jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "id": "YfW0kty6jscn",
        "outputId": "e097a8d1-1c3a-4088-e526-6cab41c713a7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<img src=\"http://drndata.com/wp-content/uploads/2016/03/car.jpg\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function Definition:\n",
        "\n",
        "def path3(image_path, model): defines a function named path3 that takes two arguments:\n",
        "image_path: A string containing the URL or local file path of the image.\n",
        "model: The trained machine learning model that will be used to predict the damage location.\n",
        "Downloading and Preparing the Image:\n",
        "\n",
        "response = requests.get(...): Fetches the image from the provided image_path using the requests library. The stream=True argument is for efficient handling of large images, and headers is used to mimic a web browser.\n",
        "response.raise_for_status(): Checks if the download was successful. If there was an HTTP error (like a 404 Not Found), it will raise an exception.\n",
        "image = Image.open(...): Opens the downloaded image data using Pillow's Image.open.\n",
        "image = image.resize((256, 256)): Resizes the image to 256x256 pixels. This is likely required because the trained model expects images of this specific size.\n",
        "x = img_to_array(image): Converts the Pillow image object into a NumPy array suitable for use with the machine learning model.\n",
        "x = x.reshape((1,) + x.shape) / 255: Reshapes the array and normalizes its pixel values to be between 0 and 1. This is a standard preprocessing step for many image classification models.\n",
        "Damage Location Prediction:\n",
        "\n",
        "pred = model.predict(x): This is where the magic happens! The pre-trained model is used to make a prediction on the prepared image data (x). The output pred will contain the model's prediction probabilities for each damage location category.\n",
        "pred_labels = np.argmax(pred, axis=1): Extracts the predicted damage location label (0, 1, or 2) by finding the index with the highest probability in the pred array."
      ],
      "metadata": {
        "id": "pxXciUolUBmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "\n",
        "import requests\n",
        "import io\n",
        "from PIL import Image\n",
        "\n",
        "def path3(image_path, model):\n",
        "    \"\"\"\n",
        "    Predicts the damage location in an image.\n",
        "\n",
        "    Args:\n",
        "        image_path: The URL or local path of the image.\n",
        "        model: The trained model to use for prediction.\n",
        "\n",
        "    Prints:\n",
        "        The predicted damage location.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try loading the image directly\n",
        "        response = requests.get(image_path, stream=True, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "        image = Image.open(io.BytesIO(response.content)).convert(\"RGB\")\n",
        "        image = image.resize((256, 256))  # Resize to match model input\n",
        "        x = img_to_array(image)\n",
        "        x = x.reshape((1,) + x.shape) / 255  # Reshape and normalize\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching image: {e}\")\n",
        "        return\n",
        "\n",
        "    pred = model.predict(x)\n",
        "    pred_labels = np.argmax(pred, axis=1)\n",
        "    d = {0: 'Front', 1: 'Rear', 2: 'Side'}\n",
        "    for key in d.keys():\n",
        "        if pred_labels[0] == key:\n",
        "            print(\"Validating location of damage....Result:\", d[key])\n",
        "    print(\"Severity assessment complete.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZG5AoECOjzOy",
        "outputId": "fbe746dc-3b77-4ae2-af45-b7a2dffe5ae1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This code is telling the notebook to fetch an image from a website (the provided URL) and show it directly in the notebook's output."
      ],
      "metadata": {
        "id": "8gokaZ2DTsIT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from IPython.display import Image  # Import the Image class\n",
        "\n",
        "# Now you can use the Image class to display the image:\n",
        "Image(url='http://repairablecars-forsale.com/photos/Exotic_Wrecked_Cars_F430_Spider_Red_Ferrari.jpg')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 502
        },
        "id": "VY4XM6qbj36C",
        "outputId": "6f5b22e2-58af-4cfc-f31b-2bdab0b6ec56"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<img src=\"http://repairablecars-forsale.com/photos/Exotic_Wrecked_Cars_F430_Spider_Red_Ferrari.jpg\"/>"
            ],
            "text/plain": [
              "<IPython.core.display.Image object>"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        " It defines a function called path3 that aims to predict the location of damage in a car image (front, rear, or side) using a trained machine learning model."
      ],
      "metadata": {
        "id": "VmIgYT30TlVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests\n",
        "\n",
        "import requests\n",
        "import io\n",
        "from PIL import Image  # Make sure to import Image from PIL\n",
        "from tensorflow.keras.preprocessing.image import img_to_array\n",
        "import numpy as np  # Make sure numpy is imported\n",
        "\n",
        "def path3(image_path, model):\n",
        "    \"\"\"\n",
        "    Predicts the damage location in an image.\n",
        "\n",
        "    Args:\n",
        "        image_path: The URL or local path of the image.\n",
        "        model: The trained model to use for prediction.\n",
        "\n",
        "    Prints:\n",
        "        The predicted damage location.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Try loading the image directly\n",
        "        response = requests.get(image_path, stream=True, headers={'User-Agent': 'Mozilla/5.0'})\n",
        "        response.raise_for_status()  # Raise HTTPError for bad responses (4xx or 5xx)\n",
        "\n",
        "        image = Image.open(io.BytesIO(response.content)).convert(\"RGB\")  # Using Image from PIL\n",
        "        image = image.resize((256, 256))  # Resize to match model input\n",
        "        x = img_to_array(image)\n",
        "        x = x.reshape((1,) + x.shape) / 255  # Reshape and normalize\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching image: {e}\")\n",
        "        return\n",
        "\n",
        "    # Check if the model has been loaded/trained\n",
        "    if model is not None:\n",
        "        pred = model.predict(x)\n",
        "        pred_labels = np.argmax(pred, axis=1)\n",
        "        d = {0: 'Front', 1: 'Rear', 2: 'Side'}\n",
        "        for key in d.keys():\n",
        "            if pred_labels[0] == key:\n",
        "                print(\"Validating location of damage....Result:\", d[key])\n",
        "        print(\"Severity assessment complete.\")\n",
        "    else:\n",
        "        print(\"Error: Model not loaded or trained.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbQeonvXS-99",
        "outputId": "9bb6e8f5-71e9-483a-e1f1-f44222dcc761"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests) (3.4.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests) (2024.8.30)\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}